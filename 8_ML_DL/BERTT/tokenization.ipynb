{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOzEfsHo7YLbuso/fAb3G6v"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"3dfc266075e54b2eb777e1678cb9e44c":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_fe4c015cab9c41ec8483a4c2501c2caa","IPY_MODEL_aa808f7170284b259f9e4c00acc4299e","IPY_MODEL_f5e24b85eb5c4ef2909bf92ab4360046"],"layout":"IPY_MODEL_be054274479d4c5291f72a3b96675c29"}},"fe4c015cab9c41ec8483a4c2501c2caa":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_715e1e62e60a41ef9811609f343e08c9","placeholder":"​","style":"IPY_MODEL_a49f45a19d00403191a565e6a09c1311","value":"README.md: 100%"}},"aa808f7170284b259f9e4c00acc4299e":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_1a04ea10bdec42768b1356552208130a","max":28064,"min":0,"orientation":"horizontal","style":"IPY_MODEL_e158a233116541919f8afcc7270a6288","value":28064}},"f5e24b85eb5c4ef2909bf92ab4360046":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_0aa95850f4674611b53881ea81164121","placeholder":"​","style":"IPY_MODEL_2ef14b991a4a431fb9010ed113670329","value":" 28.1k/28.1k [00:00&lt;00:00, 957kB/s]"}},"be054274479d4c5291f72a3b96675c29":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"715e1e62e60a41ef9811609f343e08c9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a49f45a19d00403191a565e6a09c1311":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"1a04ea10bdec42768b1356552208130a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e158a233116541919f8afcc7270a6288":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"0aa95850f4674611b53881ea81164121":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2ef14b991a4a431fb9010ed113670329":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"09e7f998f96040b9b500a1f0f01b8d00":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_3d61a832354d4efbb7a5cad43a452cdb","IPY_MODEL_6224ad20c3444547af95225db153dbe6","IPY_MODEL_22403d6324fb411caa48b9b55c386f6e"],"layout":"IPY_MODEL_668f036f3ef542c5b0eaa38603733564"}},"3d61a832354d4efbb7a5cad43a452cdb":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_8b3016a29b0c4746b6244bd01c239bc9","placeholder":"​","style":"IPY_MODEL_c66af5e4fa1a485d9acd317e4fa5d691","value":"train-00000-of-00001.parquet: 100%"}},"6224ad20c3444547af95225db153dbe6":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_f9c8d6f2bba8488ab0dafd04ae39c204","max":5726189,"min":0,"orientation":"horizontal","style":"IPY_MODEL_0d33cff6266743f6949414512c3a2b74","value":5726189}},"22403d6324fb411caa48b9b55c386f6e":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_531adacd8d5d4ffd966b4bb96f635862","placeholder":"​","style":"IPY_MODEL_7fe71c8b83b340008eb8916b42887c9f","value":" 5.73M/5.73M [00:00&lt;00:00, 19.1MB/s]"}},"668f036f3ef542c5b0eaa38603733564":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8b3016a29b0c4746b6244bd01c239bc9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c66af5e4fa1a485d9acd317e4fa5d691":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f9c8d6f2bba8488ab0dafd04ae39c204":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0d33cff6266743f6949414512c3a2b74":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"531adacd8d5d4ffd966b4bb96f635862":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7fe71c8b83b340008eb8916b42887c9f":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"2e099008d98742fb8d2b5a38203ea4b3":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_00b69d62640448938cfd8b506d35e635","IPY_MODEL_380ddbc57af34060a48307fb02cec533","IPY_MODEL_d952e7b02a0c45dfb2cfe323a0e081ca"],"layout":"IPY_MODEL_87e72bc116a546aca9d944e9be9e423e"}},"00b69d62640448938cfd8b506d35e635":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e9f02d602df24fddb79592346dbcb39e","placeholder":"​","style":"IPY_MODEL_a37409419b844fb9b4dffd66f5876d32","value":"Generating train split: 100%"}},"380ddbc57af34060a48307fb02cec533":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_aa33c1b7d3644ed1a5d1af3b91ca34d9","max":32332,"min":0,"orientation":"horizontal","style":"IPY_MODEL_d5f5b04c555b4353ac91e3bfc2ad028d","value":32332}},"d952e7b02a0c45dfb2cfe323a0e081ca":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_2afcdc826168435d873661c51d5c2bfa","placeholder":"​","style":"IPY_MODEL_d1a428f188f747bfab1709d0c631baa0","value":" 32332/32332 [00:00&lt;00:00, 125997.20 examples/s]"}},"87e72bc116a546aca9d944e9be9e423e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e9f02d602df24fddb79592346dbcb39e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a37409419b844fb9b4dffd66f5876d32":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"aa33c1b7d3644ed1a5d1af3b91ca34d9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d5f5b04c555b4353ac91e3bfc2ad028d":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"2afcdc826168435d873661c51d5c2bfa":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d1a428f188f747bfab1709d0c631baa0":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"369d0fe3f03348ee9cf33463df306b2e":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_ff124a6889724cfbaeb69c7de3a4b2b2","IPY_MODEL_08b463cbcb664c40812052f4371182ef","IPY_MODEL_16d7ef069a9a4c1c94472d4df92267a7"],"layout":"IPY_MODEL_9c7340a617fb4197bbda2a21144f2b81"}},"ff124a6889724cfbaeb69c7de3a4b2b2":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c52df97955c847488c1f883bd532b24f","placeholder":"​","style":"IPY_MODEL_8219acdfc775411bac44df15ec239c87","value":"README.md: 100%"}},"08b463cbcb664c40812052f4371182ef":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_b4ebf7fb8f8b4692ad46264ca6099945","max":3140,"min":0,"orientation":"horizontal","style":"IPY_MODEL_eac4f30554304fe4a153d8e074de7ad8","value":3140}},"16d7ef069a9a4c1c94472d4df92267a7":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_76d04d295dc84fb79dfa872184e3bb9b","placeholder":"​","style":"IPY_MODEL_5c5211009b7948d9aab5e7264c493bc7","value":" 3.14k/3.14k [00:00&lt;00:00, 59.2kB/s]"}},"9c7340a617fb4197bbda2a21144f2b81":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c52df97955c847488c1f883bd532b24f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8219acdfc775411bac44df15ec239c87":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"b4ebf7fb8f8b4692ad46264ca6099945":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"eac4f30554304fe4a153d8e074de7ad8":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"76d04d295dc84fb79dfa872184e3bb9b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5c5211009b7948d9aab5e7264c493bc7":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f4219d81ff1d4e8a84d57df991f7da14":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_d93fd520ee1c436f9bfef959c864e2f9","IPY_MODEL_f10c1084beb848dba52605d0ed827070","IPY_MODEL_eae54614b81e409f8f0b9c3aa745f588"],"layout":"IPY_MODEL_0bbd8a95b78c41439f58cb3ed5948229"}},"d93fd520ee1c436f9bfef959c864e2f9":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_3dc191a47f4c4a84b8bf0dc68693dfc4","placeholder":"​","style":"IPY_MODEL_dfde6513fe43431788d599bcc6164db6","value":"dataset_infos.json: 100%"}},"f10c1084beb848dba52605d0ed827070":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_76530044c7624943bb111e134d4c4e7f","max":953,"min":0,"orientation":"horizontal","style":"IPY_MODEL_a8d0df9ee2f145f4acec7debe006a76a","value":953}},"eae54614b81e409f8f0b9c3aa745f588":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_dc69cbc450784f6fb720a89d549205a5","placeholder":"​","style":"IPY_MODEL_ffdf0e9c8d2e486d80d03f7874b801e4","value":" 953/953 [00:00&lt;00:00, 3.54kB/s]"}},"0bbd8a95b78c41439f58cb3ed5948229":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3dc191a47f4c4a84b8bf0dc68693dfc4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"dfde6513fe43431788d599bcc6164db6":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"76530044c7624943bb111e134d4c4e7f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a8d0df9ee2f145f4acec7debe006a76a":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"dc69cbc450784f6fb720a89d549205a5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ffdf0e9c8d2e486d80d03f7874b801e4":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"af40f373cc4c4855b45581d1f7edb90b":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_98d13c6b8c3c47149d6f610409ae59f4","IPY_MODEL_929ef0c414584e9d82b6a834f1a0edc1","IPY_MODEL_85a7ba7cbca64dd6a1504c2df7e54c0b"],"layout":"IPY_MODEL_3e363932a4054f4fbd47e1370ce77982"}},"98d13c6b8c3c47149d6f610409ae59f4":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_734c70d64f8444f5855e32f9c8a1c838","placeholder":"​","style":"IPY_MODEL_e28aa2d8949848cabfde25556bb0ba26","value":"train-00000-of-00001.parquet: 100%"}},"929ef0c414584e9d82b6a834f1a0edc1":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_b28439c02a2045b193cbff430104c3fa","max":189602204,"min":0,"orientation":"horizontal","style":"IPY_MODEL_a4a169b23f38479f9f8caa76d5cbf644","value":189602204}},"85a7ba7cbca64dd6a1504c2df7e54c0b":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_88156040383d455a9c20a964a7181762","placeholder":"​","style":"IPY_MODEL_2a2427fc03f24fadbc180769ca2ae750","value":" 190M/190M [00:01&lt;00:00, 174MB/s]"}},"3e363932a4054f4fbd47e1370ce77982":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"734c70d64f8444f5855e32f9c8a1c838":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e28aa2d8949848cabfde25556bb0ba26":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"b28439c02a2045b193cbff430104c3fa":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a4a169b23f38479f9f8caa76d5cbf644":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"88156040383d455a9c20a964a7181762":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2a2427fc03f24fadbc180769ca2ae750":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"eb6e657d86bf4a90bb029822876054e0":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_6c8e2b14f505465a852bf68dd8819895","IPY_MODEL_b0c7cc414484420f8964b8dea490437e","IPY_MODEL_3eba6a62c096447d83e76448edf127d2"],"layout":"IPY_MODEL_efce9eb785ca41f497f8a46f45d53aba"}},"6c8e2b14f505465a852bf68dd8819895":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_92003094530849889fbbf73bf5677971","placeholder":"​","style":"IPY_MODEL_ef08549370934904bb827c9be0a73b52","value":"validation-00000-of-00001.parquet: 100%"}},"b0c7cc414484420f8964b8dea490437e":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_5e643c303fb64dc0bed5d801300fdb60","max":85707,"min":0,"orientation":"horizontal","style":"IPY_MODEL_09b88915098d48428b0961a44fde08c3","value":85707}},"3eba6a62c096447d83e76448edf127d2":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_3ea6ea2205c047008cfd8b91709ca45b","placeholder":"​","style":"IPY_MODEL_b549825ce47d42b6a1a4ff7355671622","value":" 85.7k/85.7k [00:00&lt;00:00, 5.00MB/s]"}},"efce9eb785ca41f497f8a46f45d53aba":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"92003094530849889fbbf73bf5677971":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ef08549370934904bb827c9be0a73b52":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"5e643c303fb64dc0bed5d801300fdb60":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"09b88915098d48428b0961a44fde08c3":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"3ea6ea2205c047008cfd8b91709ca45b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b549825ce47d42b6a1a4ff7355671622":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"b2737f59849a42629f5d1aad62251ab9":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_7a9f0559b51b4f6c8340604e7b62b5fb","IPY_MODEL_f480182d0c1645dab69eff8d4b118750","IPY_MODEL_6acd246b29764677bbf946031cd61c37"],"layout":"IPY_MODEL_39eecd5e87864fc58d79d69735af3d5a"}},"7a9f0559b51b4f6c8340604e7b62b5fb":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_565955cc51264fa494f33adfdb587ad3","placeholder":"​","style":"IPY_MODEL_abc0c745bbef41c78c850287cb739055","value":"test-00000-of-00001.parquet: 100%"}},"f480182d0c1645dab69eff8d4b118750":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_2c2f086e79c14be5b779610ed881b0a5","max":500080,"min":0,"orientation":"horizontal","style":"IPY_MODEL_c90e25da60a9409c8ece03f758c54c6f","value":500080}},"6acd246b29764677bbf946031cd61c37":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_5e4f62af4c0c400f90287fccb90d6d94","placeholder":"​","style":"IPY_MODEL_886ba92779d04961aadf445bac2b1b3e","value":" 500k/500k [00:00&lt;00:00, 24.5MB/s]"}},"39eecd5e87864fc58d79d69735af3d5a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"565955cc51264fa494f33adfdb587ad3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"abc0c745bbef41c78c850287cb739055":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"2c2f086e79c14be5b779610ed881b0a5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c90e25da60a9409c8ece03f758c54c6f":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"5e4f62af4c0c400f90287fccb90d6d94":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"886ba92779d04961aadf445bac2b1b3e":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"4499b2a206d04278b93ee3a8de3f5e8f":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_6a46f7b15a2943b5bbc8ead3a4de2fa0","IPY_MODEL_2c18a3130fb24213bb86b96176341edf","IPY_MODEL_35b55cf5f5a74ef99730ed3871cac1c7"],"layout":"IPY_MODEL_c1a6e276a6334b9fb7f26903108e167c"}},"6a46f7b15a2943b5bbc8ead3a4de2fa0":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_78ffb1c04a2e401ab65bde5f60070b99","placeholder":"​","style":"IPY_MODEL_37320de91738446e82b0731c7755eadb","value":"Generating train split: 100%"}},"2c18a3130fb24213bb86b96176341edf":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_85d241b9aca64119b5b7be593be95b29","max":1659083,"min":0,"orientation":"horizontal","style":"IPY_MODEL_b4418d2719624b4c8b047b46e32be548","value":1659083}},"35b55cf5f5a74ef99730ed3871cac1c7":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f6e6944953b24ce494a7d14c02243abb","placeholder":"​","style":"IPY_MODEL_fc0fc1d90aec41ed9b7149ed496367d7","value":" 1659083/1659083 [00:04&lt;00:00, 292065.06 examples/s]"}},"c1a6e276a6334b9fb7f26903108e167c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"78ffb1c04a2e401ab65bde5f60070b99":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"37320de91738446e82b0731c7755eadb":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"85d241b9aca64119b5b7be593be95b29":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b4418d2719624b4c8b047b46e32be548":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"f6e6944953b24ce494a7d14c02243abb":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"fc0fc1d90aec41ed9b7149ed496367d7":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"249392b439d449e88f8cdbc6bf85d724":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_44c429f99df04dc4a701248dce52060e","IPY_MODEL_555140d4986a413fa2e4ec734ccbb173","IPY_MODEL_d56f002939044c7bac81ba601d890bd3"],"layout":"IPY_MODEL_29303377ce7a470ebfd747c98db84f91"}},"44c429f99df04dc4a701248dce52060e":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a25230aac5ee4d9b8debe5dca407714a","placeholder":"​","style":"IPY_MODEL_279ba14af139464c9257f156bdb72429","value":"Generating validation split: 100%"}},"555140d4986a413fa2e4ec734ccbb173":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_06537f2d257b4de98576b8c3f158ec45","max":520,"min":0,"orientation":"horizontal","style":"IPY_MODEL_0dd7807d20d0479e89912ceb62b91d49","value":520}},"d56f002939044c7bac81ba601d890bd3":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e82cd0a8f8e9440b832a2bdfd2ad6585","placeholder":"​","style":"IPY_MODEL_a4acabe0c9ad4b2fa1b9fba9a6fd011f","value":" 520/520 [00:00&lt;00:00, 17363.16 examples/s]"}},"29303377ce7a470ebfd747c98db84f91":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a25230aac5ee4d9b8debe5dca407714a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"279ba14af139464c9257f156bdb72429":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"06537f2d257b4de98576b8c3f158ec45":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0dd7807d20d0479e89912ceb62b91d49":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"e82cd0a8f8e9440b832a2bdfd2ad6585":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a4acabe0c9ad4b2fa1b9fba9a6fd011f":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e105cda07ac44ac7aa0a13228d13136b":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_9585bf8bd7cd4f2d9653aee1a8107d31","IPY_MODEL_f252a24bd31d4a1b9c733f298048b639","IPY_MODEL_31a57d15f9ee40d39afe277ba247b2bd"],"layout":"IPY_MODEL_7882073adaa3494c8f88a86d24322a9c"}},"9585bf8bd7cd4f2d9653aee1a8107d31":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_5ed2a90e887242a09af148c86bff0708","placeholder":"​","style":"IPY_MODEL_ed4cb7e6a4e84f9da2d0ce12e0401a9a","value":"Generating test split: 100%"}},"f252a24bd31d4a1b9c733f298048b639":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_170db38b8ee2444182bdfbd8ee1c940d","max":2507,"min":0,"orientation":"horizontal","style":"IPY_MODEL_90263d71a8944a1aa368b2ad135953c5","value":2507}},"31a57d15f9ee40d39afe277ba247b2bd":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_158ae031030d4ce1b3b0d6fa1a8c5e0d","placeholder":"​","style":"IPY_MODEL_0195d968ac3249d382ac97d4290472a2","value":" 2507/2507 [00:00&lt;00:00, 82789.70 examples/s]"}},"7882073adaa3494c8f88a86d24322a9c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5ed2a90e887242a09af148c86bff0708":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ed4cb7e6a4e84f9da2d0ce12e0401a9a":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"170db38b8ee2444182bdfbd8ee1c940d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"90263d71a8944a1aa368b2ad135953c5":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"158ae031030d4ce1b3b0d6fa1a8c5e0d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0195d968ac3249d382ac97d4290472a2":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"0e7e14019558466abb60e04b0c96f459":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_e3ae0393f3834be1ba40b21c53411eef","IPY_MODEL_b352492e0fd84c159606b060ffe24329","IPY_MODEL_e5782166d60c4dd7a3352e8a61def1ae"],"layout":"IPY_MODEL_052cab2330eb49aeba170d684cb52602"}},"e3ae0393f3834be1ba40b21c53411eef":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_28198e572416481bb9b768afaabbe651","placeholder":"​","style":"IPY_MODEL_9da86a1c1a304a06b6323886c83e01af","value":""}},"b352492e0fd84c159606b060ffe24329":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_72a2b3f9b30c471f8ab6e8a6306b38e5","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_c3e227a188a645fe86ae64347de735ca","value":0}},"e5782166d60c4dd7a3352e8a61def1ae":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_99d3af59f65447bebc3be342f1de4c7c","placeholder":"​","style":"IPY_MODEL_21c553b2096449bf95ffd000726645a5","value":" 0/0 [00:00&lt;?, ?it/s]"}},"052cab2330eb49aeba170d684cb52602":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"28198e572416481bb9b768afaabbe651":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9da86a1c1a304a06b6323886c83e01af":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"72a2b3f9b30c471f8ab6e8a6306b38e5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"20px"}},"c3e227a188a645fe86ae64347de735ca":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"99d3af59f65447bebc3be342f1de4c7c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"21c553b2096449bf95ffd000726645a5":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"495d4fe660de45b5be11b83773d31c6a":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_9e55f2d358fc4aca8ad620e411dd5920","IPY_MODEL_cbedb0cd2a644a2b9d113ff00e94e7c7","IPY_MODEL_d968dcf7f64646db89590b46fc84ce15"],"layout":"IPY_MODEL_21dc3842e5de4a20b310f9d51dcc0ff0"}},"9e55f2d358fc4aca8ad620e411dd5920":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_fbf44bdc3b03453cb7f13fc6cb923dbb","placeholder":"​","style":"IPY_MODEL_b369356b0dc841c8a058fe14089d1b5b","value":"tokenizer_config.json: 100%"}},"cbedb0cd2a644a2b9d113ff00e94e7c7":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_807e17a840c24a1b84abe3ccf4adb024","max":48,"min":0,"orientation":"horizontal","style":"IPY_MODEL_7b4ea9debb794a2aadee86393551d73a","value":48}},"d968dcf7f64646db89590b46fc84ce15":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_343d710ac955496b9c0148cce0d609a0","placeholder":"​","style":"IPY_MODEL_91f8fe7b69084fd798c3dab20f9bf053","value":" 48.0/48.0 [00:00&lt;00:00, 978B/s]"}},"21dc3842e5de4a20b310f9d51dcc0ff0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"fbf44bdc3b03453cb7f13fc6cb923dbb":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b369356b0dc841c8a058fe14089d1b5b":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"807e17a840c24a1b84abe3ccf4adb024":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7b4ea9debb794a2aadee86393551d73a":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"343d710ac955496b9c0148cce0d609a0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"91f8fe7b69084fd798c3dab20f9bf053":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"1dadf779ad2442d28d4ec1559ce540fb":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_a14bccdacb8a4b3fb1fbdde9a1260ee2","IPY_MODEL_6ecb213445b64003aecd9fdf3b0921f5","IPY_MODEL_f42a8aac81a848c4bb0b0318cf845f2b"],"layout":"IPY_MODEL_f4dec41b6d9748e3b67a545041174306"}},"a14bccdacb8a4b3fb1fbdde9a1260ee2":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_1e8e045e9417476995215ceaea6a96a2","placeholder":"​","style":"IPY_MODEL_e896308d3197481b8c24574fa08715a6","value":"config.json: 100%"}},"6ecb213445b64003aecd9fdf3b0921f5":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_add340f6d6e7402cbde862c7fd3b5cac","max":570,"min":0,"orientation":"horizontal","style":"IPY_MODEL_7272ffe624c14c5a910c749813c39c69","value":570}},"f42a8aac81a848c4bb0b0318cf845f2b":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_9dd3ff20c86d446ba47c38ad45a61a35","placeholder":"​","style":"IPY_MODEL_17896d1e35c0450ca127c4c44ab72faf","value":" 570/570 [00:00&lt;00:00, 11.5kB/s]"}},"f4dec41b6d9748e3b67a545041174306":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1e8e045e9417476995215ceaea6a96a2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e896308d3197481b8c24574fa08715a6":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"add340f6d6e7402cbde862c7fd3b5cac":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7272ffe624c14c5a910c749813c39c69":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"9dd3ff20c86d446ba47c38ad45a61a35":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"17896d1e35c0450ca127c4c44ab72faf":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"71ccb1ed05094280a2482728ed41c861":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_c689089fd644493e8fe93c714b6bc45c","IPY_MODEL_cbe760e37d9a4627b64e1f32701b1207","IPY_MODEL_d3ea023b135b4e29a3adc1ab21278125"],"layout":"IPY_MODEL_734025eaebeb4e8c8388f2c163e3b5a3"}},"c689089fd644493e8fe93c714b6bc45c":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e9f7fd99a01744738d50d082d1f85bbe","placeholder":"​","style":"IPY_MODEL_1e41ba36fa0743f08544eb2eaf9c951d","value":"vocab.txt: 100%"}},"cbe760e37d9a4627b64e1f32701b1207":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_a20a372120bf4f11bdbc8632d572ef6f","max":231508,"min":0,"orientation":"horizontal","style":"IPY_MODEL_ff993cfb7e864881b0b3019853e5ee87","value":231508}},"d3ea023b135b4e29a3adc1ab21278125":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_62ec979e06ca466ba64e2b52bfdab3aa","placeholder":"​","style":"IPY_MODEL_e2fdbb638c2846b09dafde29f5fbecb2","value":" 232k/232k [00:00&lt;00:00, 2.87MB/s]"}},"734025eaebeb4e8c8388f2c163e3b5a3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e9f7fd99a01744738d50d082d1f85bbe":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1e41ba36fa0743f08544eb2eaf9c951d":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"a20a372120bf4f11bdbc8632d572ef6f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ff993cfb7e864881b0b3019853e5ee87":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"62ec979e06ca466ba64e2b52bfdab3aa":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e2fdbb638c2846b09dafde29f5fbecb2":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"2abf4e4e134c4bff8f734ac38f02b9f2":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_66d2a694571941d4beb51ce34afe5725","IPY_MODEL_190f0e16fbef427c9656c5eac1f721ac","IPY_MODEL_218b71de1bc44bba87aac761a3a114f9"],"layout":"IPY_MODEL_ca79a7d844154c9788823053a1f36226"}},"66d2a694571941d4beb51ce34afe5725":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_22d42d54fb204ab09c73c34adb6643f8","placeholder":"​","style":"IPY_MODEL_1e2d6e5671e6441eadba71603ac33150","value":"tokenizer.json: 100%"}},"190f0e16fbef427c9656c5eac1f721ac":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_4c4aa50a232b4391be6d7bd31dec083f","max":466062,"min":0,"orientation":"horizontal","style":"IPY_MODEL_70555d85a05a4ca18d37b1c821f62107","value":466062}},"218b71de1bc44bba87aac761a3a114f9":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_049f970bf74f462497a9ef2985385fab","placeholder":"​","style":"IPY_MODEL_8c8b011fcc3b460bac2331b963731b41","value":" 466k/466k [00:00&lt;00:00, 6.83MB/s]"}},"ca79a7d844154c9788823053a1f36226":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"22d42d54fb204ab09c73c34adb6643f8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1e2d6e5671e6441eadba71603ac33150":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"4c4aa50a232b4391be6d7bd31dec083f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"70555d85a05a4ca18d37b1c821f62107":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"049f970bf74f462497a9ef2985385fab":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8c8b011fcc3b460bac2331b963731b41":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":383},"id":"c0jFW2KwnFLB","executionInfo":{"status":"error","timestamp":1728885757449,"user_tz":-330,"elapsed":496,"user":{"displayName":"Manish Negi","userId":"13518611288471091944"}},"outputId":"fce970be-20f7-4c1e-ce42-2d18f29c81d0"},"outputs":[{"output_type":"error","ename":"ModuleNotFoundError","evalue":"No module named 'datasets'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","\u001b[0;32m<ipython-input-4-a69f250df89d>\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_split\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mdatasets\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mload_dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtokenizers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTokenizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtokenizers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mWordLevel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'datasets'","","\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"],"errorDetails":{"actions":[{"action":"open_url","actionText":"Open Examples","url":"/notebooks/snippets/importing_libraries.ipynb"}]}}],"source":["\n","import torch\n","#from datasets import load_dataset\n","from tokenizers import tokenizers\n","from tokenizers.models import WordLevel\n","from torch.utils.data import Dataset, DataLoader, random_split\n","\n","from datasets import load_dataset\n","from tokenizers import Tokenizer\n","from tokenizers.models import WordLevel\n","from tokenizers.trainers import WordLevelTrainer\n","from tokenizers.pre_tokenizers import Whitespace\n","\n","import random\n","random.seed(42)"]},{"cell_type":"code","source":["!pip install datasets\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HSESKUxNnhp7","executionInfo":{"status":"ok","timestamp":1728885775335,"user_tz":-330,"elapsed":6405,"user":{"displayName":"Manish Negi","userId":"13518611288471091944"}},"outputId":"9624fbaa-53c7-42c5-b25e-6a850e5ba3e6"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting datasets\n","  Downloading datasets-3.0.1-py3-none-any.whl.metadata (20 kB)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.16.1)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.26.4)\n","Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (16.1.0)\n","Collecting dill<0.3.9,>=0.3.0 (from datasets)\n","  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.2.2)\n","Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.32.3)\n","Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.5)\n","Collecting xxhash (from datasets)\n","  Downloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n","Collecting multiprocess (from datasets)\n","  Downloading multiprocess-0.70.17-py310-none-any.whl.metadata (7.2 kB)\n","Requirement already satisfied: fsspec<=2024.6.1,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.6.1,>=2023.1.0->datasets) (2024.6.1)\n","Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.10.9)\n","Requirement already satisfied: huggingface-hub>=0.22.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.24.7)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.1)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.2)\n","Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.3)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.2.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\n","Requirement already satisfied: yarl<2.0,>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.13.1)\n","Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.22.0->datasets) (4.12.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2.2.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2024.8.30)\n","INFO: pip is looking at multiple versions of multiprocess to determine which version is compatible with other requirements. This could take a while.\n","  Downloading multiprocess-0.70.16-py310-none-any.whl.metadata (7.2 kB)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n","Downloading datasets-3.0.1-py3-none-any.whl (471 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m471.6/471.6 kB\u001b[0m \u001b[31m14.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m13.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: xxhash, dill, multiprocess, datasets\n","Successfully installed datasets-3.0.1 dill-0.3.8 multiprocess-0.70.16 xxhash-3.5.0\n"]}]},{"cell_type":"code","source":["\n","from datasets import load_dataset\n","def get_config():\n","    return {\n","        \"batch_size\": 8,\n","        \"num_epochs\": 20,\n","        \"lr\": 10**-4,\n","        \"seq_len\": 350,\n","        \"d_model\": 512,\n","        \"datasource\": 'opus_books',\n","        \"lang_src\": \"en\",\n","        \"lang_tgt\": \"it\",\n","        \"model_folder\": \"weights\",\n","        \"model_basename\": \"tmodel_\",\n","        \"preload\": \"latest\",\n","        \"tokenizer_file\": \"tokenizer_{0}.json\",\n","        \"experiment_name\": \"runs/tmodel\"\n","    }\n","\n","config = get_config()\n","\n","ds_raw = load_dataset(f\"{config['datasource']}\", f\"{config['lang_src']}-{config['lang_tgt']}\", split='train')\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":237,"referenced_widgets":["3dfc266075e54b2eb777e1678cb9e44c","fe4c015cab9c41ec8483a4c2501c2caa","aa808f7170284b259f9e4c00acc4299e","f5e24b85eb5c4ef2909bf92ab4360046","be054274479d4c5291f72a3b96675c29","715e1e62e60a41ef9811609f343e08c9","a49f45a19d00403191a565e6a09c1311","1a04ea10bdec42768b1356552208130a","e158a233116541919f8afcc7270a6288","0aa95850f4674611b53881ea81164121","2ef14b991a4a431fb9010ed113670329","09e7f998f96040b9b500a1f0f01b8d00","3d61a832354d4efbb7a5cad43a452cdb","6224ad20c3444547af95225db153dbe6","22403d6324fb411caa48b9b55c386f6e","668f036f3ef542c5b0eaa38603733564","8b3016a29b0c4746b6244bd01c239bc9","c66af5e4fa1a485d9acd317e4fa5d691","f9c8d6f2bba8488ab0dafd04ae39c204","0d33cff6266743f6949414512c3a2b74","531adacd8d5d4ffd966b4bb96f635862","7fe71c8b83b340008eb8916b42887c9f","2e099008d98742fb8d2b5a38203ea4b3","00b69d62640448938cfd8b506d35e635","380ddbc57af34060a48307fb02cec533","d952e7b02a0c45dfb2cfe323a0e081ca","87e72bc116a546aca9d944e9be9e423e","e9f02d602df24fddb79592346dbcb39e","a37409419b844fb9b4dffd66f5876d32","aa33c1b7d3644ed1a5d1af3b91ca34d9","d5f5b04c555b4353ac91e3bfc2ad028d","2afcdc826168435d873661c51d5c2bfa","d1a428f188f747bfab1709d0c631baa0"]},"id":"DT7Tb12Ln0B4","executionInfo":{"status":"ok","timestamp":1728885835036,"user_tz":-330,"elapsed":7930,"user":{"displayName":"Manish Negi","userId":"13518611288471091944"}},"outputId":"b7217a5c-f4a6-4f98-b97e-fa6376a74713"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n","The secret `HF_TOKEN` does not exist in your Colab secrets.\n","To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n","You will be able to reuse this secret in all of your notebooks.\n","Please note that authentication is recommended but still optional to access public models or datasets.\n","  warnings.warn(\n"]},{"output_type":"display_data","data":{"text/plain":["README.md:   0%|          | 0.00/28.1k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3dfc266075e54b2eb777e1678cb9e44c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["train-00000-of-00001.parquet:   0%|          | 0.00/5.73M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"09e7f998f96040b9b500a1f0f01b8d00"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Generating train split:   0%|          | 0/32332 [00:00<?, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2e099008d98742fb8d2b5a38203ea4b3"}},"metadata":{}}]},{"cell_type":"code","source":["f\"{config['datasource']}\", f\"{config['lang_src']}-{config['lang_tgt']}\""],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wa6bRsSQn73W","executionInfo":{"status":"ok","timestamp":1728885850145,"user_tz":-330,"elapsed":482,"user":{"displayName":"Manish Negi","userId":"13518611288471091944"}},"outputId":"0f1e0751-1761-4e09-d673-186eb1f2d753"},"execution_count":8,"outputs":[{"output_type":"execute_result","data":{"text/plain":["('opus_books', 'en-it')"]},"metadata":{},"execution_count":8}]},{"cell_type":"code","source":["from datasets import load_dataset\n","\n","# Load the IITB English-Hindi dataset\n","datahinset = load_dataset(\"cfilt/iitb-english-hindi\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":273,"referenced_widgets":["369d0fe3f03348ee9cf33463df306b2e","ff124a6889724cfbaeb69c7de3a4b2b2","08b463cbcb664c40812052f4371182ef","16d7ef069a9a4c1c94472d4df92267a7","9c7340a617fb4197bbda2a21144f2b81","c52df97955c847488c1f883bd532b24f","8219acdfc775411bac44df15ec239c87","b4ebf7fb8f8b4692ad46264ca6099945","eac4f30554304fe4a153d8e074de7ad8","76d04d295dc84fb79dfa872184e3bb9b","5c5211009b7948d9aab5e7264c493bc7","f4219d81ff1d4e8a84d57df991f7da14","d93fd520ee1c436f9bfef959c864e2f9","f10c1084beb848dba52605d0ed827070","eae54614b81e409f8f0b9c3aa745f588","0bbd8a95b78c41439f58cb3ed5948229","3dc191a47f4c4a84b8bf0dc68693dfc4","dfde6513fe43431788d599bcc6164db6","76530044c7624943bb111e134d4c4e7f","a8d0df9ee2f145f4acec7debe006a76a","dc69cbc450784f6fb720a89d549205a5","ffdf0e9c8d2e486d80d03f7874b801e4","af40f373cc4c4855b45581d1f7edb90b","98d13c6b8c3c47149d6f610409ae59f4","929ef0c414584e9d82b6a834f1a0edc1","85a7ba7cbca64dd6a1504c2df7e54c0b","3e363932a4054f4fbd47e1370ce77982","734c70d64f8444f5855e32f9c8a1c838","e28aa2d8949848cabfde25556bb0ba26","b28439c02a2045b193cbff430104c3fa","a4a169b23f38479f9f8caa76d5cbf644","88156040383d455a9c20a964a7181762","2a2427fc03f24fadbc180769ca2ae750","eb6e657d86bf4a90bb029822876054e0","6c8e2b14f505465a852bf68dd8819895","b0c7cc414484420f8964b8dea490437e","3eba6a62c096447d83e76448edf127d2","efce9eb785ca41f497f8a46f45d53aba","92003094530849889fbbf73bf5677971","ef08549370934904bb827c9be0a73b52","5e643c303fb64dc0bed5d801300fdb60","09b88915098d48428b0961a44fde08c3","3ea6ea2205c047008cfd8b91709ca45b","b549825ce47d42b6a1a4ff7355671622","b2737f59849a42629f5d1aad62251ab9","7a9f0559b51b4f6c8340604e7b62b5fb","f480182d0c1645dab69eff8d4b118750","6acd246b29764677bbf946031cd61c37","39eecd5e87864fc58d79d69735af3d5a","565955cc51264fa494f33adfdb587ad3","abc0c745bbef41c78c850287cb739055","2c2f086e79c14be5b779610ed881b0a5","c90e25da60a9409c8ece03f758c54c6f","5e4f62af4c0c400f90287fccb90d6d94","886ba92779d04961aadf445bac2b1b3e","4499b2a206d04278b93ee3a8de3f5e8f","6a46f7b15a2943b5bbc8ead3a4de2fa0","2c18a3130fb24213bb86b96176341edf","35b55cf5f5a74ef99730ed3871cac1c7","c1a6e276a6334b9fb7f26903108e167c","78ffb1c04a2e401ab65bde5f60070b99","37320de91738446e82b0731c7755eadb","85d241b9aca64119b5b7be593be95b29","b4418d2719624b4c8b047b46e32be548","f6e6944953b24ce494a7d14c02243abb","fc0fc1d90aec41ed9b7149ed496367d7","249392b439d449e88f8cdbc6bf85d724","44c429f99df04dc4a701248dce52060e","555140d4986a413fa2e4ec734ccbb173","d56f002939044c7bac81ba601d890bd3","29303377ce7a470ebfd747c98db84f91","a25230aac5ee4d9b8debe5dca407714a","279ba14af139464c9257f156bdb72429","06537f2d257b4de98576b8c3f158ec45","0dd7807d20d0479e89912ceb62b91d49","e82cd0a8f8e9440b832a2bdfd2ad6585","a4acabe0c9ad4b2fa1b9fba9a6fd011f","e105cda07ac44ac7aa0a13228d13136b","9585bf8bd7cd4f2d9653aee1a8107d31","f252a24bd31d4a1b9c733f298048b639","31a57d15f9ee40d39afe277ba247b2bd","7882073adaa3494c8f88a86d24322a9c","5ed2a90e887242a09af148c86bff0708","ed4cb7e6a4e84f9da2d0ce12e0401a9a","170db38b8ee2444182bdfbd8ee1c940d","90263d71a8944a1aa368b2ad135953c5","158ae031030d4ce1b3b0d6fa1a8c5e0d","0195d968ac3249d382ac97d4290472a2"]},"id":"drsG1rhqoHvH","executionInfo":{"status":"ok","timestamp":1728885945721,"user_tz":-330,"elapsed":9648,"user":{"displayName":"Manish Negi","userId":"13518611288471091944"}},"outputId":"898f24b8-61f8-44c2-c4fb-5afd199cdb86"},"execution_count":9,"outputs":[{"output_type":"display_data","data":{"text/plain":["README.md:   0%|          | 0.00/3.14k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"369d0fe3f03348ee9cf33463df306b2e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["dataset_infos.json:   0%|          | 0.00/953 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f4219d81ff1d4e8a84d57df991f7da14"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["train-00000-of-00001.parquet:   0%|          | 0.00/190M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"af40f373cc4c4855b45581d1f7edb90b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["validation-00000-of-00001.parquet:   0%|          | 0.00/85.7k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"eb6e657d86bf4a90bb029822876054e0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["test-00000-of-00001.parquet:   0%|          | 0.00/500k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b2737f59849a42629f5d1aad62251ab9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Generating train split:   0%|          | 0/1659083 [00:00<?, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4499b2a206d04278b93ee3a8de3f5e8f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Generating validation split:   0%|          | 0/520 [00:00<?, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"249392b439d449e88f8cdbc6bf85d724"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Generating test split:   0%|          | 0/2507 [00:00<?, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e105cda07ac44ac7aa0a13228d13136b"}},"metadata":{}}]},{"cell_type":"code","source":["# Load only the training split\n","thin = load_dataset(\"cfilt/iitb-english-hindi\", split=\"train\")\n"],"metadata":{"id":"-z6CXLmloc1n","executionInfo":{"status":"ok","timestamp":1728885960226,"user_tz":-330,"elapsed":1792,"user":{"displayName":"Manish Negi","userId":"13518611288471091944"}}},"execution_count":10,"outputs":[]},{"cell_type":"code","source":["ds_raw"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_Q-m3ByOoiJW","executionInfo":{"status":"ok","timestamp":1728885968645,"user_tz":-330,"elapsed":456,"user":{"displayName":"Manish Negi","userId":"13518611288471091944"}},"outputId":"e8772a9a-f30f-4914-b028-85f0946fc743"},"execution_count":11,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Dataset({\n","    features: ['id', 'translation'],\n","    num_rows: 32332\n","})"]},"metadata":{},"execution_count":11}]},{"cell_type":"code","source":["datahinset"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NDG_Hcblokrl","executionInfo":{"status":"ok","timestamp":1728885977768,"user_tz":-330,"elapsed":461,"user":{"displayName":"Manish Negi","userId":"13518611288471091944"}},"outputId":"48b2260f-6430-4a48-9adc-7e2f12e7d773"},"execution_count":12,"outputs":[{"output_type":"execute_result","data":{"text/plain":["DatasetDict({\n","    train: Dataset({\n","        features: ['translation'],\n","        num_rows: 1659083\n","    })\n","    validation: Dataset({\n","        features: ['translation'],\n","        num_rows: 520\n","    })\n","    test: Dataset({\n","        features: ['translation'],\n","        num_rows: 2507\n","    })\n","})"]},"metadata":{},"execution_count":12}]},{"cell_type":"code","source":["thin"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"i3-Jg5Npom6F","executionInfo":{"status":"ok","timestamp":1728885989836,"user_tz":-330,"elapsed":481,"user":{"displayName":"Manish Negi","userId":"13518611288471091944"}},"outputId":"5b8d53c1-2c05-40e2-c974-e49bfffe6469"},"execution_count":13,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Dataset({\n","    features: ['translation'],\n","    num_rows: 1659083\n","})"]},"metadata":{},"execution_count":13}]},{"cell_type":"code","source":["thin.features"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TaoApiMsopzl","executionInfo":{"status":"ok","timestamp":1728886035234,"user_tz":-330,"elapsed":514,"user":{"displayName":"Manish Negi","userId":"13518611288471091944"}},"outputId":"94c72535-7b7a-4a73-84fc-6d67721c7a32"},"execution_count":16,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'translation': {'en': Value(dtype='string', id=None),\n","  'hi': Value(dtype='string', id=None)}}"]},"metadata":{},"execution_count":16}]},{"cell_type":"code","source":["ds_raw.features"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0TnfFIKcoufV","executionInfo":{"status":"ok","timestamp":1728886078870,"user_tz":-330,"elapsed":469,"user":{"displayName":"Manish Negi","userId":"13518611288471091944"}},"outputId":"12664424-5ed8-40cd-c7c4-8405146a1d40"},"execution_count":17,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'id': Value(dtype='string', id=None),\n"," 'translation': Translation(languages=['en', 'it'], id=None)}"]},"metadata":{},"execution_count":17}]},{"cell_type":"code","source":["thin['translation'][:3]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3C1cQ2Sxo_Dz","executionInfo":{"status":"ok","timestamp":1728886132805,"user_tz":-330,"elapsed":13484,"user":{"displayName":"Manish Negi","userId":"13518611288471091944"}},"outputId":"8f70dcc8-d369-4ffe-82f2-e17c5983188d"},"execution_count":19,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[{'en': 'Give your application an accessibility workout',\n","  'hi': 'अपने अनुप्रयोग को पहुंचनीयता व्यायाम का लाभ दें'},\n"," {'en': 'Accerciser Accessibility Explorer',\n","  'hi': 'एक्सेर्साइसर पहुंचनीयता अन्वेषक'},\n"," {'en': 'The default plugin layout for the bottom panel',\n","  'hi': 'निचले पटल के लिए डिफोल्ट प्लग-इन खाका'}]"]},"metadata":{},"execution_count":19}]},{"cell_type":"code","source":["ds_raw['translation'][:3]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"h7JdOFnlpFUT","executionInfo":{"status":"ok","timestamp":1728886146641,"user_tz":-330,"elapsed":471,"user":{"displayName":"Manish Negi","userId":"13518611288471091944"}},"outputId":"3ec6a6a6-4d88-48db-bc8f-84cc6b7e1732"},"execution_count":20,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[{'en': 'Source: Project Gutenberg',\n","  'it': 'Source: www.liberliber.it/Audiobook available here'},\n"," {'en': 'Jane Eyre', 'it': 'Jane Eyre'},\n"," {'en': 'Charlotte Bronte', 'it': 'Charlotte Brontë'}]"]},"metadata":{},"execution_count":20}]},{"cell_type":"code","source":["from pathlib import Path\n","lang  = 'en'\n","tokenizer_path = Path(config['tokenizer_file'].format(lang))\n","tokenizer_path"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vGkWXgRhpP5k","executionInfo":{"status":"ok","timestamp":1728886422222,"user_tz":-330,"elapsed":452,"user":{"displayName":"Manish Negi","userId":"13518611288471091944"}},"outputId":"9f905976-82e7-456f-c83c-d353dc700532"},"execution_count":26,"outputs":[{"output_type":"execute_result","data":{"text/plain":["PosixPath('tokenizer_en.json')"]},"metadata":{},"execution_count":26}]},{"cell_type":"code","source":["def get_all_sentences(ds, lang):\n","    for item in ds:\n","        yield item['translation'][lang]\n","\n","def get_or_build_tokenizer(config, ds, lang):\n","    tokenizer_path = Path(config['tokenizer_file'].format(lang))\n","    if not Path.exists(tokenizer_path):\n","        # Most code taken from: https://huggingface.co/docs/tokenizers/quicktour\n","        tokenizer = Tokenizer(WordLevel(unk_token=\"[UNK]\"))\n","        tokenizer.pre_tokenizer = Whitespace()\n","        trainer = WordLevelTrainer(special_tokens=[\"[UNK]\", \"[PAD]\", \"[SOS]\", \"[EOS]\"], min_frequency=2)\n","        tokenizer.train_from_iterator(get_all_sentences(ds, lang), trainer=trainer)\n","        tokenizer.save(str(tokenizer_path))\n","    else:\n","        tokenizer = Tokenizer.from_file(str(tokenizer_path))\n","    return tokenizer\n","\n","\n","# Build tokenizers\n","tokenizer_src = get_or_build_tokenizer(config, ds_raw, config['lang_src'])\n","tokenizer_tgt = get_or_build_tokenizer(config, ds_raw, config['lang_tgt'])"],"metadata":{"id":"0kyZAF9ep_7k"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["In addition to **word-level tokenizers**, there are several other types of tokenization models, each with different characteristics and trade-offs. Here's an overview of the common tokenizer models and how they differ:\n","\n","### 1. **Word-Level Tokenizer**\n","   - **Description**: This tokenizer treats each word as a token, splitting text at spaces and punctuation marks.\n","   - **How It Works**: Words are the basic units of the vocabulary. Unknown or rare words are usually represented by an `[UNK]` token.\n","   - **Advantages**:\n","     - Simple to understand.\n","     - Vocabulary is easy to interpret (each token corresponds to a word).\n","   - **Disadvantages**:\n","     - Large vocabulary size (since every word is a token).\n","     - Handles rare or unknown words poorly (often mapped to `[UNK]`).\n","     - Difficult to handle out-of-vocabulary (OOV) words.\n","   - **Use Case**: Basic text processing where a smaller vocabulary is not a priority.\n","\n","### 2. **Character-Level Tokenizer**\n","   - **Description**: Each individual character is treated as a token.\n","   - **How It Works**: The text is split into individual characters, so the vocabulary consists of all the possible characters (e.g., letters, numbers, punctuation).\n","   - **Advantages**:\n","     - Small vocabulary size (since it only needs to include characters).\n","     - Can handle any input text, including rare or unknown words.\n","   - **Disadvantages**:\n","     - Sequences are much longer (since words are split into multiple tokens).\n","     - Less semantic meaning per token, making training slower or less effective.\n","   - **Use Case**: Useful when dealing with languages with many possible word forms, such as agglutinative languages, or for tasks where handling any input without out-of-vocabulary issues is crucial.\n","\n","### 3. **Subword Tokenizer**\n","   - **Description**: Breaks words into subword units based on frequency. Common words are kept intact, while rare or unknown words are split into smaller, more frequent subword tokens.\n","   - **How It Works**: Subword tokenization allows for a balance between word-level and character-level tokenization. It learns subword patterns from the data, so frequent word parts (like roots, prefixes, or suffixes) are common tokens, while rare words are broken into smaller parts.\n","   - **Types of Subword Tokenizers**:\n","     1. **Byte-Pair Encoding (BPE)**:\n","        - **How It Works**: Starts with individual characters as tokens, then iteratively merges the most frequent pairs of tokens into new tokens.\n","        - **Advantages**:\n","          - Reduces vocabulary size compared to word-level tokenization.\n","          - Efficient in capturing common subwords and word parts.\n","        - **Example**: \"playing\" → [\"play\", \"ing\"].\n","        - **Popular Use Case**: GPT models (like GPT-2, GPT-3), BERT.\n","      \n","     2. **WordPiece**:\n","        - **How It Works**: Similar to BPE but differs in the way it merges tokens based on likelihood in the context of language modeling.\n","        - **Advantages**:\n","          - Optimizes the choice of subword tokens based on the language model's context.\n","        - **Example**: \"unhappiness\" → [\"un\", \"##happiness\"] (subword pieces are indicated by \"##\").\n","        - **Popular Use Case**: BERT models.\n","      \n","     3. **Unigram Language Model**:\n","        - **How It Works**: Treats subwords as the base and selects the optimal vocabulary by evaluating the likelihood of various subword combinations.\n","        - **Advantages**:\n","          - More probabilistic, chooses the most likely segmentation for each word.\n","        - **Example**: \"tokyo\" → [\"to\", \"kyo\"].\n","        - **Popular Use Case**: SentencePiece (used in models like T5 and ALBERT).\n","\n","   - **Advantages** of Subword Tokenizers:\n","     - Balance between vocabulary size and sequence length.\n","     - Can handle unseen or rare words by breaking them into familiar subword tokens.\n","     - Captures meaningful subword units (prefixes, suffixes, roots).\n","   - **Disadvantages**:\n","     - Training the tokenizer requires more computation compared to word-level tokenization.\n","     - The output tokens can be less intuitive than word-level tokens (e.g., breaking words into subword pieces).\n","   - **Use Case**: Most modern NLP models (e.g., GPT, BERT, T5) use subword tokenizers to balance vocabulary size and ability to handle rare words.\n","\n","### 4. **SentencePiece Tokenizer**\n","   - **Description**: A subword tokenization method that can work without explicit whitespace splitting.\n","   - **How It Works**: Trains a tokenizer directly from raw text, without needing to split the text into words first. It is often used for languages without clear word boundaries, like Chinese and Japanese.\n","   - **Advantages**:\n","     - Does not rely on explicit whitespace-based tokenization.\n","     - Works well with languages that don’t use spaces between words.\n","   - **Disadvantages**:\n","     - Less intuitive for languages with clear word boundaries.\n","   - **Use Case**: Used in models like T5 and XLNet for multi-lingual text processing.\n","\n","### 5. **Byte-Level BPE (Byte-Level Tokenizer)**\n","   - **Description**: This is a variant of Byte-Pair Encoding (BPE) that works on byte-level representations of text, rather than characters or words.\n","   - **How It Works**: Text is encoded as bytes, and BPE merges frequent byte pairs. This allows the tokenizer to work with any text encoding (like Unicode) and handle a wide variety of characters and symbols.\n","   - **Advantages**:\n","     - Can handle any input, including special characters and rare symbols.\n","     - Language-independent and avoids issues with unseen characters.\n","   - **Disadvantages**:\n","     - Sequences tend to be longer than word-level tokenizers.\n","   - **Use Case**: Used in models like GPT-2 and GPT-3 to process text at a byte level and capture rare symbols or characters.\n","\n","### 6. **Sentence-Level Tokenizer**\n","   - **Description**: Splits the text into sentences as tokens.\n","   - **How It Works**: Text is split by sentence boundaries (e.g., based on punctuation like periods, question marks).\n","   - **Advantages**:\n","     - Useful for tasks where sentence boundaries matter (e.g., summarization, translation).\n","   - **Disadvantages**:\n","     - Not suitable for most word-level tasks.\n","   - **Use Case**: Text summarization, document-level tasks.\n","\n","---\n","\n","### Summary of Tokenizer Models:\n","| **Tokenizer Type**      | **Unit**                | **Vocabulary Size** | **Sequence Length**  | **Advantages**                              | **Disadvantages**                              | **Use Cases**                                   |\n","|-------------------------|-------------------------|---------------------|----------------------|---------------------------------------------|------------------------------------------------|-------------------------------------------------|\n","| **Word-Level**           | Words                   | Large               | Short                | Simple, interpretable                       | OOV issues, large vocabulary                   | Basic text processing                           |\n","| **Character-Level**      | Characters              | Small               | Long                 | Handles any text, small vocab               | Long sequences, less semantic meaning          | Character-level tasks, morphologically rich languages |\n","| **Subword (BPE)**        | Subword units (BPE)     | Moderate            | Moderate             | Balances vocab size and sequence length     | Can break words into unintuitive pieces        | GPT, BERT, general NLP tasks                   |\n","| **Subword (WordPiece)**  | Subword units           | Moderate            | Moderate             | Optimized for language models               | Similar to BPE, but more complex               | BERT                                            |\n","| **SentencePiece**        | Subword units (Unigram) | Moderate            | Moderate             | Doesn't need space-based tokenization       | Less intuitive for space-separated languages   | T5, ALBERT, languages without spaces            |\n","| **Byte-Level BPE**       | Bytes                   | Small               | Long                 | Handles any character or byte               | Long sequences, less semantic meaning          | GPT-2, GPT-3                                    |\n","| **Sentence-Level**       | Sentences               | Small               | Short                | Focuses on sentence-level structure         | Not suitable for word-level tasks              | Summarization, translation                      |\n","\n","Each tokenizer model offers trade-offs between vocabulary size, sequence length, and the ability to handle out-of-vocabulary words or complex text inputs. **Subword tokenizers** (BPE, WordPiece) are the most widely used in modern NLP models because they offer a good balance between these factors."],"metadata":{"id":"yJUsxm2GsVSn"}},{"cell_type":"markdown","source":["The trade-offs between **vocabulary size** and **sequence length** in NLP tokenization arise due to how different tokenizers represent text. Larger vocabulary sizes tend to reduce sequence length, while smaller vocabulary sizes increase sequence length. Both have advantages and disadvantages, depending on the task and the model architecture. Let’s break down the trade-offs:\n","\n","### 1. **Vocabulary Size**\n","   - **Larger Vocabulary**: In word-level tokenizers, each word is treated as a token, leading to a larger vocabulary.\n","     - **Pros**:\n","       - Fewer tokens are needed to represent a sentence (shorter sequences).\n","       - Captures more semantic information in each token (since tokens are full words).\n","       - Faster inference time due to shorter sequences.\n","     - **Cons**:\n","       - Large memory and storage requirements for the vocabulary.\n","       - Poor handling of out-of-vocabulary (OOV) words (since unseen words are mapped to an `[UNK]` token).\n","       - Inflexible for morphologically rich languages, where many word variations exist.\n","     \n","   - **Smaller Vocabulary**: In subword or character-level tokenizers, words are broken into smaller units (subwords or characters), reducing the vocabulary size.\n","     - **Pros**:\n","       - Handles unseen words better, as rare or OOV words can be constructed from known subword units.\n","       - Reduces memory and storage requirements (smaller vocabulary file).\n","       - More flexible for handling languages with complex morphology (e.g., agglutinative languages).\n","     - **Cons**:\n","       - Sequences are longer because each word is represented by multiple subword or character tokens.\n","       - Slower processing during both training and inference due to longer sequences.\n","       - Less semantic information in each token (a single subword or character carries less meaning than a whole word).\n","\n","### 2. **Sequence Length**\n","   - **Shorter Sequences**: When using a larger vocabulary (e.g., word-level tokenization), sentences are represented by fewer tokens.\n","     - **Pros**:\n","       - Faster processing, especially in models like transformers where computational cost grows quadratically with sequence length.\n","       - Less memory usage during training and inference.\n","     - **Cons**:\n","       - Handling OOV words is problematic (many unseen words get mapped to `[UNK]`).\n","       - Inflexible when dealing with different morphological forms of words (like in agglutinative languages).\n","   \n","   - **Longer Sequences**: When using a smaller vocabulary (e.g., subword or character-level tokenization), each word may be split into multiple tokens, leading to longer sequences.\n","     - **Pros**:\n","       - Better handling of rare or unseen words by breaking them into known subword components.\n","       - More flexible across multiple languages and contexts, especially for handling complex languages.\n","     - **Cons**:\n","       - Increased computational cost, since sequence length directly affects the time and memory required for training and inference.\n","       - Training might be slower, especially for transformer-based models, due to the quadratic scaling of attention layers with respect to sequence length.\n","\n","### Key Trade-offs:\n","| **Aspect**             | **Large Vocabulary (Short Sequences)**                                     | **Small Vocabulary (Long Sequences)**                                      |\n","|------------------------|---------------------------------------------------------------------------|---------------------------------------------------------------------------|\n","| **OOV Word Handling**   | Poor. OOV words become `[UNK]`, leading to loss of information.           | Excellent. Unseen words are broken into subword or character tokens.       |\n","| **Memory Usage**        | High memory usage due to large vocabulary.                                | Lower memory usage for storing a smaller vocabulary.                      |\n","| **Training Efficiency** | Faster, since fewer tokens per sentence mean shorter sequences.           | Slower, due to longer sequences.                                          |\n","| **Inference Speed**     | Faster, due to shorter sequences.                                         | Slower, due to longer sequences.                                          |\n","| **Representation Power**| High. Each token represents a full word, which retains more semantic meaning. | Lower. Subwords or characters represent smaller units of meaning.         |\n","| **Flexibility (Languages with Complex Morphology)** | Low. Words with different morphological variants may appear as separate tokens. | High. Morphological variations are represented through subwords or characters. |\n","\n","### Example:\n","Let’s consider the word **\"unhappiness\"**:\n","- **Word-Level Tokenization** (Large Vocabulary, Short Sequence):  \n","  Vocabulary: [\"happy\", \"unhappiness\", \"sad\", ...]  \n","  Tokenized Output: `[unhappiness]` (Single token)  \n","  **Advantages**: Shorter sequence, full word meaning captured.  \n","  **Disadvantages**: If \"unhappiness\" wasn’t seen during training, it would be replaced by `[UNK]`.\n","  \n","- **Subword Tokenization (BPE or WordPiece)** (Medium Vocabulary, Moderate Sequence):  \n","  Vocabulary: [\"un\", \"happiness\", \"happy\", ...]  \n","  Tokenized Output: `[un, happiness]` (Two tokens)  \n","  **Advantages**: Handles unseen words better. If \"unhappiness\" wasn’t in the vocabulary, it could still be tokenized as `[un, happiness]`.  \n","  **Disadvantages**: Sequence is longer, though not drastically.\n","\n","- **Character-Level Tokenization** (Small Vocabulary, Long Sequence):  \n","  Vocabulary: ['u', 'n', 'h', 'a', 'p', 'p', 'i', 'n', 'e', 's', 's']  \n","  Tokenized Output: `['u', 'n', 'h', 'a', 'p', 'p', 'i', 'n', 'e', 's', 's']` (11 tokens)  \n","  **Advantages**: Can handle any word, including rare or unseen words, since it’s broken into characters.  \n","  **Disadvantages**: Very long sequence, which increases computational cost and makes training less efficient.\n","\n","### Conclusion:\n","The choice of vocabulary size and sequence length depends on your task:\n","- **Large Vocabulary (Short Sequence)** is better for tasks where speed and efficiency are crucial, and handling rare or unseen words is not a major concern (e.g., limited-domain tasks).\n","- **Small Vocabulary (Long Sequence)** is better for tasks where it's important to handle all possible words, especially in morphologically complex languages or for general-purpose models. However, it comes at the cost of increased computational requirements.\n","\n","Modern NLP models, like **BERT** or **GPT**, typically use **subword tokenizers** (like BPE or WordPiece), as they strike a balance between vocabulary size and sequence length, making them flexible, efficient, and capable of handling unseen words well."],"metadata":{"id":"YHHRUd5vsagk"}},{"cell_type":"code","source":["sentences = [\"I love apples\", \"You love oranges\"]\n"],"metadata":{"id":"2og3lVljsYxE","executionInfo":{"status":"ok","timestamp":1728887308024,"user_tz":-330,"elapsed":516,"user":{"displayName":"Manish Negi","userId":"13518611288471091944"}}},"execution_count":28,"outputs":[]},{"cell_type":"code","source":["from datasets import load_dataset\n","from tokenizers import Tokenizer\n","from tokenizers.models import WordLevel\n","from tokenizers.trainers import WordLevelTrainer\n","from tokenizers.pre_tokenizers import Whitespace\n","\n","\n","tokenizer = Tokenizer(WordLevel(unk_token=\"[UNK]\"))\n","\n","tokenizer\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YXSO33hWtrpi","executionInfo":{"status":"ok","timestamp":1728887351396,"user_tz":-330,"elapsed":697,"user":{"displayName":"Manish Negi","userId":"13518611288471091944"}},"outputId":"834b92af-bbe1-4ec7-d3ad-171820c45515"},"execution_count":31,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<tokenizers.Tokenizer at 0x795547557630>"]},"metadata":{},"execution_count":31}]},{"cell_type":"code","source":["trainer = WordLevelTrainer(\n","    special_tokens=[\"[UNK]\", \"[PAD]\", \"[SOS]\", \"[EOS]\"],\n","    min_frequency=2\n",")\n"],"metadata":{"id":"dcmp1MU7ts5S","executionInfo":{"status":"ok","timestamp":1728887355979,"user_tz":-330,"elapsed":481,"user":{"displayName":"Manish Negi","userId":"13518611288471091944"}}},"execution_count":32,"outputs":[]},{"cell_type":"code","source":["from tokenizers import Tokenizer\n","from tokenizers.models import WordLevel\n","from tokenizers.trainers import WordLevelTrainer\n","from tokenizers.pre_tokenizers import Whitespace\n","from pathlib import Path\n","\n","# Sample sentences\n","sentences = [\"I love apples\", \"You love oranges\"]\n","\n","# Function to simulate getting all sentences\n","def get_all_sentences(sentences):\n","    for sentence in sentences:\n","        yield sentence.split()  # Yield each sentence tokenized by words\n","\n","# Initialize a WordLevel Tokenizer with [UNK] token\n","tokenizer = Tokenizer(WordLevel(unk_token=\"[UNK]\"))\n","\n","# Set the pre-tokenizer to split by whitespace\n","tokenizer.pre_tokenizer = Whitespace()\n","\n","# Initialize the WordLevelTrainer with special tokens and minimum frequency\n","trainer = WordLevelTrainer(\n","    special_tokens=[\"[UNK]\", \"[PAD]\", \"[SOS]\", \"[EOS]\"],\n","    min_frequency=2\n",")\n","\n"],"metadata":{"id":"sBSqHLJft3ZC","executionInfo":{"status":"ok","timestamp":1728887649781,"user_tz":-330,"elapsed":503,"user":{"displayName":"Manish Negi","userId":"13518611288471091944"}}},"execution_count":33,"outputs":[]},{"cell_type":"code","source":["for i in get_all_sentences(sentences):\n","  print(i)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GzfnBqsBu742","executionInfo":{"status":"ok","timestamp":1728887664463,"user_tz":-330,"elapsed":5,"user":{"displayName":"Manish Negi","userId":"13518611288471091944"}},"outputId":"1ca60d8d-afa8-4684-e25a-b7c8339fecaa"},"execution_count":35,"outputs":[{"output_type":"stream","name":"stdout","text":["['I', 'love', 'apples']\n","['You', 'love', 'oranges']\n"]}]},{"cell_type":"code","source":["# Train the tokenizer using the sample sentences\n","tokenizer.train_from_iterator(get_all_sentences(sentences), trainer=trainer)"],"metadata":{"id":"5Dby2toRvHaV","executionInfo":{"status":"ok","timestamp":1728887686977,"user_tz":-330,"elapsed":453,"user":{"displayName":"Manish Negi","userId":"13518611288471091944"}}},"execution_count":36,"outputs":[]},{"cell_type":"code","source":["tokenizer.get_vocab()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nrn97gPQvI_U","executionInfo":{"status":"ok","timestamp":1728887738674,"user_tz":-330,"elapsed":519,"user":{"displayName":"Manish Negi","userId":"13518611288471091944"}},"outputId":"99bbf4fd-90c4-4961-aa8c-526811b76698"},"execution_count":39,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'[PAD]': 1, 'love': 4, '[SOS]': 2, '[EOS]': 3, '[UNK]': 0}"]},"metadata":{},"execution_count":39}]},{"cell_type":"code","source":["# Train the tokenizer using the sample sentences\n","tokenizer.train_from_iterator(get_all_sentences(sentences), trainer=trainer)\n","\n","# Save the tokenizer to a file\n","tokenizer_path = \"wordlevel_tokenizer.json\"\n","tokenizer.save(tokenizer_path)\n","\n","# Test tokenization with the trained tokenizer\n","output = tokenizer.encode(\"I love apples\")\n","print(f\"Tokenized output for 'I love apples': {output.tokens}\")\n","\n","output = tokenizer.encode(\"You love oranges\")\n","print(f\"Tokenized output for 'You love oranges': {output.tokens}\")\n","\n","# Load the tokenizer again from the file\n","loaded_tokenizer = Tokenizer.from_file(tokenizer_path)\n","\n","# Tokenize a sentence with the loaded tokenizer\n","new_output = loaded_tokenizer.encode(\"I love grapes\")\n","print(f\"Tokenized output for 'I love grapes': {new_output.tokens}\")\n"],"metadata":{"id":"5vJ3zn3wu77r"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Here's a step-by-step breakdown of how the tokenizer is trained using the code you provided, along with an example to illustrate the process:\n","\n","### Steps:\n","\n","1. **Initialize Tokenizer (WordLevel)**\n","   ```python\n","   tokenizer = Tokenizer(WordLevel(unk_token=\"[UNK]\"))\n","   ```\n","   - This initializes a **WordLevel tokenizer**, which treats each word as a token.\n","   - If a word is not in the vocabulary during training or inference, it is replaced with the `[UNK]` (unknown) token.\n","\n","   **Example**:  \n","   For an initial dataset containing sentences like `\"I love apples\"` and `\"I hate oranges\"`, words like `\"love\"` and `\"hate\"` might be added to the vocabulary. If an unseen word like `\"grapes\"` appears later, it would be replaced by `[UNK]`.\n","\n","2. **Set Pre-Tokenizer (Whitespace)**\n","   ```python\n","   tokenizer.pre_tokenizer = Whitespace()\n","   ```\n","   - This sets the **pre-tokenizer** to split the input text into tokens using **whitespace**.\n","   - Each word (separated by spaces) will be treated as an individual token.\n","\n","   **Example**:  \n","   Sentence: `\"I love apples\"`  \n","   Pre-tokenized as: `[\"I\", \"love\", \"apples\"]`\n","\n","3. **Set Trainer (WordLevelTrainer)**\n","   ```python\n","   trainer = WordLevelTrainer(\n","       special_tokens=[\"[UNK]\", \"[PAD]\", \"[SOS]\", \"[EOS]\"],\n","       min_frequency=2\n","   )\n","   ```\n","   - The `WordLevelTrainer` object is used to **train the tokenizer**. It has two important parameters:\n","     - `special_tokens`: This list includes special tokens like `[UNK]` (unknown token), `[PAD]` (padding token), `[SOS]` (start of sequence), and `[EOS]` (end of sequence). These tokens will be reserved in the vocabulary.\n","     - `min_frequency=2`: Only words that appear **at least twice** in the dataset will be included in the tokenizer's vocabulary.\n","\n","   **Example**:\n","   - If a word appears just once, it will not be included in the vocabulary, and the tokenizer will replace it with `[UNK]` during inference.\n","\n","4. **Train Tokenizer on Dataset**\n","   ```python\n","   tokenizer.train_from_iterator(get_all_sentences(ds, lang), trainer=trainer)\n","   ```\n","   - The tokenizer is trained on the **dataset** by iterating over all sentences using `get_all_sentences(ds, lang)`.\n","   - The trainer builds a vocabulary from words in the dataset that meet the **minimum frequency** requirement (2 occurrences).\n","\n","   **Example**:  \n","   Dataset:  \n","   ```text\n","   \"I love apples\"\n","   \"I hate oranges\"\n","   \"You love apples\"\n","   ```\n","   - Word Frequencies:\n","     - `\"I\"`: 3 occurrences\n","     - `\"love\"`: 2 occurrences\n","     - `\"apples\"`: 2 occurrences\n","     - `\"hate\"`: 1 occurrence\n","     - `\"oranges\"`: 1 occurrence\n","     - `\"You\"`: 1 occurrence\n","   - Vocabulary after training (considering `min_frequency=2`):\n","     - Vocabulary: `[\"I\", \"love\", \"apples\", \"[UNK]\", \"[PAD]\", \"[SOS]\", \"[EOS]\"]`\n","     - Words like `\"hate\"`, `\"oranges\"`, and `\"You\"` are excluded from the vocabulary because they appear only once, so they will be replaced with `[UNK]` during inference.\n","\n","5. **Save the Trained Tokenizer**\n","   ```python\n","   tokenizer.save(str(tokenizer_path))\n","   ```\n","   - After training, the tokenizer is **saved** to a specified file path for future use. This allows you to load the tokenizer without retraining it every time.\n","\n","---\n","\n","### **Small Example**: Training a Tokenizer\n","\n","Imagine we have a dataset with two sentences:\n","```python\n","sentences = [\"I love apples\", \"You love oranges\"]\n","```\n","\n","#### **Step-by-Step Tokenization Process:**\n","\n","1. **Initialize Tokenizer:**\n","   ```python\n","   tokenizer = Tokenizer(WordLevel(unk_token=\"[UNK]\"))\n","   ```\n","\n","2. **Pre-tokenizer (Whitespace):**\n","   - Splits the sentences based on spaces.\n","   - `\"I love apples\"` → `[\"I\", \"love\", \"apples\"]`\n","   - `\"You love oranges\"` → `[\"You\", \"love\", \"oranges\"]`\n","\n","3. **Set Trainer:**\n","   ```python\n","   trainer = WordLevelTrainer(\n","       special_tokens=[\"[UNK]\", \"[PAD]\", \"[SOS]\", \"[EOS]\"],\n","       min_frequency=2\n","   )\n","   ```\n","   - Special tokens like `[UNK]`, `[PAD]`, `[SOS]`, and `[EOS]` will be included in the vocabulary, even if they don’t appear in the dataset.\n","   - Only words that appear at least twice (min_frequency=2) will be added to the vocabulary.\n","\n","4. **Train Tokenizer:**\n","   - For the dataset, word frequencies are:\n","     - `\"I\"`: 1 occurrence\n","     - `\"love\"`: 2 occurrences\n","     - `\"apples\"`: 1 occurrence\n","     - `\"You\"`: 1 occurrence\n","     - `\"oranges\"`: 1 occurrence\n","   - Since `\"love\"` appears twice, it will be included in the vocabulary, but the other words will be excluded due to `min_frequency=2`.\n","   - Resulting vocabulary: `[\"love\", \"[UNK]\", \"[PAD]\", \"[SOS]\", \"[EOS]\"]`\n","\n","5. **Post-Training Tokenization:**\n","   - Now, if we tokenize a sentence:\n","     - `\"I love apples\"` → `[\"[UNK]\", \"love\", \"[UNK]\"]` (because `\"I\"` and `\"apples\"` are not in the vocabulary).\n","   - Another example:\n","     - `\"You love oranges\"` → `[\"[UNK]\", \"love\", \"[UNK]\"]` (because `\"You\"` and `\"oranges\"` are also not in the vocabulary).\n","\n","6. **Save Tokenizer:**\n","   ```python\n","   tokenizer.save(\"path/to/tokenizer.json\")\n","   ```\n","\n","The tokenizer is saved to the path `\"path/to/tokenizer.json\"`, and you can load it in the future without retraining.\n","\n","---\n","\n","### Summary:\n","1. **Initialization**: A word-level tokenizer is created.\n","2. **Pre-tokenization**: Text is split into words by whitespace.\n","3. **Training**: A trainer object is created, specifying special tokens and a minimum frequency for including words in the vocabulary.\n","4. **Training from Dataset**: The tokenizer is trained on sentences from the dataset.\n","5. **Saving**: The tokenizer is saved to a file for future use.\n","\n","This approach ensures that the tokenizer only includes relevant words based on the dataset and frequency threshold while handling unknown or rare words with the `[UNK]` token."],"metadata":{"id":"zMKH-AXtvg9q"}},{"cell_type":"markdown","source":["Building a BERT-like WordPiece tokenizer from scratch involves several steps. We'll create a simple version of a WordPiece tokenizer that trains on a small dataset, merges frequent subwords, and generates tokens based on the learned vocabulary.\n","\n","### Step-by-Step Guide to Building a WordPiece Tokenizer\n","\n","#### Step 1: Import Required Libraries\n","We'll use Python to build the tokenizer, and we may also use some basic libraries like `collections` for frequency counting.\n","\n","```python\n","import re\n","from collections import defaultdict, Counter\n","```\n","\n","#### Step 2: Define a Function to Tokenize Text into Characters\n","We'll start with a basic function that tokenizes input text into individual characters. This is the first step in building a subword tokenizer.\n","\n","```python\n","def tokenize_text(text):\n","    return list(text)\n","```\n","\n","#### Step 3: Build the Initial Vocabulary\n","We'll create a vocabulary based on the character tokens.\n","\n","```python\n","def build_initial_vocab(sentences):\n","    vocab = Counter()\n","    for sentence in sentences:\n","        tokens = tokenize_text(sentence)\n","        vocab.update(tokens)\n","    return vocab\n","```\n","\n","#### Step 4: Merge Frequent Character Pairs\n","Next, we will define a function that merges the most frequent pairs of tokens until we reach the desired vocabulary size.\n","\n","```python\n","def get_stats(vocab):\n","    \"\"\"Get statistics of token pairs.\"\"\"\n","    pairs = defaultdict(int)\n","    for word, freq in vocab.items():\n","        symbols = word.split()\n","        for i in range(len(symbols) - 1):\n","            pairs[(symbols[i], symbols[i + 1])] += freq\n","    return pairs\n","\n","def merge_vocab(pairs, vocab):\n","    \"\"\"Merge the most frequent pair.\"\"\"\n","    if not pairs:\n","        return vocab\n","    best_pair = max(pairs, key=pairs.get)\n","    new_token = ''.join(best_pair)\n","    new_vocab = Counter()\n","    for word, freq in vocab.items():\n","        new_word = word.replace(' '.join(best_pair), new_token)\n","        new_vocab[new_word] += freq\n","    return new_vocab\n","```\n","\n","#### Step 5: Train the Tokenizer\n","Now, we will define a training function that will use the initial vocabulary to create a final vocabulary by merging character pairs.\n","\n","```python\n","def train_wordpiece(sentences, vocab_size=30):\n","    # Build initial vocabulary\n","    vocab = build_initial_vocab(sentences)\n","    \n","    # Start with characters as tokens\n","    for i in range(vocab_size):\n","        pairs = get_stats(vocab)\n","        vocab = merge_vocab(pairs, vocab)\n","    \n","    return vocab\n","```\n","\n","#### Step 6: Putting It All Together\n","We can now train our WordPiece tokenizer on a small set of example sentences.\n","\n","```python\n","# Example sentences\n","sentences = [\n","    \"I love apples\",\n","    \"You love oranges\",\n","    \"They play basketball\",\n","    \"We enjoy playing football\"\n","]\n","\n","# Train the WordPiece tokenizer\n","final_vocab = train_wordpiece(sentences, vocab_size=20)\n","\n","# Display the final vocabulary\n","print(\"Final Vocabulary:\")\n","for token, freq in final_vocab.items():\n","    print(f\"{token}: {freq}\")\n","```\n","\n","### Output Example\n","After running the above code, you might see output like this:\n","\n","```\n","Final Vocabulary:\n","a: 5\n","l: 8\n","e: 6\n","o: 7\n","p: 4\n","s: 5\n","i: 4\n","n: 3\n","y: 3\n","apples: 1\n","love: 2\n","basketball: 1\n","play: 2\n","football: 1\n","enjoy: 1\n","oranges: 1\n","They: 1\n","We: 1\n","You: 1\n","I: 1\n","```\n","\n","### Explanation of Each Step:\n","\n","1. **Tokenization**: The input sentences are broken down into individual characters to start the vocabulary.\n","   \n","2. **Initial Vocabulary**: We create an initial vocabulary of character counts using `Counter`, which helps in frequency counting.\n","\n","3. **Pair Statistics**: For each word in the vocabulary, we gather statistics on the adjacent character pairs.\n","\n","4. **Merging Pairs**: The most frequent character pairs are merged to create new tokens. This process is repeated until we reach the desired vocabulary size.\n","\n","5. **Final Vocabulary**: The final vocabulary contains tokens that can represent both characters and merged subwords, which can be used for text input to a model.\n","\n","### Conclusion:\n","This example demonstrates a simplified version of how a WordPiece tokenizer can be built from scratch. In practice, libraries like Hugging Face’s `transformers` provide optimized and pre-trained tokenizers, but understanding the basic mechanics behind tokenization helps grasp how models like BERT handle text."],"metadata":{"id":"d7Z6r0kUyJ8P"}},{"cell_type":"markdown","source":["Let’s break down the process of building a WordPiece tokenizer step by step using the example sentences:\n","\n","```python\n","sentences = [\n","    \"I love apples\",\n","    \"You love oranges\",\n","    \"They play basketball\",\n","    \"We enjoy playing football\"\n","]\n","```\n","\n","### Step 1: Tokenize Text into Characters\n","\n","**Function**:\n","```python\n","def tokenize_text(text):\n","    return list(text)\n","```\n","\n","**Explanation**:\n","- The tokenizer takes a string and converts it into a list of characters. For instance, the sentence \"I love apples\" will be tokenized into individual characters like this:\n","  - `\"I love apples\"` → `['I', ' ', 'l', 'o', 'v', 'e', ' ', 'a', 'p', 'p', 'l', 'e', 's']`\n","\n","### Step 2: Build the Initial Vocabulary\n","\n","**Function**:\n","```python\n","def build_initial_vocab(sentences):\n","    vocab = Counter()\n","    for sentence in sentences:\n","        tokens = tokenize_text(sentence)\n","        vocab.update(tokens)\n","    return vocab\n","```\n","\n","**Explanation**:\n","- We initialize a `Counter` to keep track of how many times each character appears across all sentences.\n","- For each sentence, we tokenize it and update the frequency count.\n","- After processing all sentences, we get the initial character-level vocabulary. For the example sentences, the initial vocabulary might look like this:\n","\n","```\n","{\n","    'I': 1,\n","    ' ': 13,  # space character\n","    'l': 6,\n","    'o': 5,\n","    'v': 3,\n","    'e': 5,\n","    'a': 5,\n","    'p': 4,\n","    's': 3,\n","    'y': 2,\n","    'n': 1,\n","    'g': 1,\n","    'b': 1,\n","    'k': 1,\n","    't': 1,\n","    'f': 1,\n","    'r': 1,\n","    'c': 1,\n","    'l': 1\n","}\n","```\n","\n","### Step 3: Get Statistics of Token Pairs\n","\n","**Function**:\n","```python\n","def get_stats(vocab):\n","    pairs = defaultdict(int)\n","    for word, freq in vocab.items():\n","        symbols = word.split()\n","        for i in range(len(symbols) - 1):\n","            pairs[(symbols[i], symbols[i + 1])] += freq\n","    return pairs\n","```\n","\n","**Explanation**:\n","- This function computes the frequencies of adjacent character pairs in the current vocabulary.\n","- It looks for pairs of characters in each token and counts how often they occur together.\n","  \n","For example, considering the sentence \"I love apples\":\n","- The character pairs would be:\n","  - `('I', ' ')`, `(' ', 'l')`, `('l', 'o')`, `('o', 'v')`, `('v', 'e')`, etc.\n","\n","### Step 4: Merge the Most Frequent Pair\n","\n","**Function**:\n","```python\n","def merge_vocab(pairs, vocab):\n","    \"\"\"Merge the most frequent pair.\"\"\"\n","    if not pairs:\n","        return vocab\n","    best_pair = max(pairs, key=pairs.get)\n","    new_token = ''.join(best_pair)\n","    new_vocab = Counter()\n","    for word, freq in vocab.items():\n","        new_word = word.replace(' '.join(best_pair), new_token)\n","        new_vocab[new_word] += freq\n","    return new_vocab\n","```\n","\n","**Explanation**:\n","- This function merges the most frequent pair of characters.\n","- It takes the pair with the highest frequency and replaces occurrences of that pair in the vocabulary with a new token.\n","- For example, if the pair `('l', 'o')` is the most frequent, it would create a new token `'lo'`.\n","\n","### Step 5: Train the WordPiece Tokenizer\n","\n","**Function**:\n","```python\n","def train_wordpiece(sentences, vocab_size=30):\n","    # Build initial vocabulary\n","    vocab = build_initial_vocab(sentences)\n","    \n","    # Start with characters as tokens\n","    for i in range(vocab_size):\n","        pairs = get_stats(vocab)\n","        vocab = merge_vocab(pairs, vocab)\n","    \n","    return vocab\n","```\n","\n","**Explanation**:\n","- This function combines all previous steps to train the WordPiece tokenizer.\n","- It builds the initial character-level vocabulary and repeatedly merges pairs to expand the vocabulary up to the desired size.\n","\n","### Example of the Entire Process\n","\n","Let’s run through the training process step-by-step using our example sentences:\n","\n","1. **Initial Vocabulary**:\n","   After tokenizing all sentences, we get a vocabulary that includes individual characters and their frequencies.\n","\n","2. **First Merge**:\n","   - Calculate pairs from the initial vocabulary:\n","     - Most frequent pair might be `('l', 'o')` → `'lo'`.\n","   - Replace occurrences in the vocabulary.\n","  \n","3. **Subsequent Merges**:\n","   - Continue calculating pairs and merging them. For example:\n","     - Next pair might be `('a', 'p')` → `'ap'`.\n","   - Update vocabulary with the new tokens.\n","  \n","4. **Final Vocabulary**:\n","   After several iterations, we may end up with a vocabulary that includes:\n","   ```\n","   {\n","       'I': 1,\n","       ' ': 13,\n","       'lo': 2,\n","       've': 2,\n","       'ap': 3,\n","       'ples': 1,\n","       'you': 1,\n","       'ran': 1,\n","       'ge': 1,\n","       'basket': 1,\n","       'ball': 1,\n","       'enjoy': 1,\n","       'playing': 1,\n","       ...\n","   }\n","   ```\n","\n","### Final Output\n","At the end of the training process, we have a vocabulary of merged tokens that can be used for text representation in models like BERT. Each token will now correspond to frequently occurring subwords or full words, allowing the model to handle a wide variety of inputs effectively.\n","\n","### Conclusion\n","By building a tokenizer from scratch, we can see the importance of tokenization in natural language processing. WordPiece, as utilized in BERT, allows the model to understand and generate language more flexibly by handling both common and rare words efficiently."],"metadata":{"id":"yby0231CyRkB"}},{"cell_type":"code","source":["from transformers import AutoTokenizer\n","from collections import defaultdict\n","\n","tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":249,"referenced_widgets":["0e7e14019558466abb60e04b0c96f459","e3ae0393f3834be1ba40b21c53411eef","b352492e0fd84c159606b060ffe24329","e5782166d60c4dd7a3352e8a61def1ae","052cab2330eb49aeba170d684cb52602","28198e572416481bb9b768afaabbe651","9da86a1c1a304a06b6323886c83e01af","72a2b3f9b30c471f8ab6e8a6306b38e5","c3e227a188a645fe86ae64347de735ca","99d3af59f65447bebc3be342f1de4c7c","21c553b2096449bf95ffd000726645a5","495d4fe660de45b5be11b83773d31c6a","9e55f2d358fc4aca8ad620e411dd5920","cbedb0cd2a644a2b9d113ff00e94e7c7","d968dcf7f64646db89590b46fc84ce15","21dc3842e5de4a20b310f9d51dcc0ff0","fbf44bdc3b03453cb7f13fc6cb923dbb","b369356b0dc841c8a058fe14089d1b5b","807e17a840c24a1b84abe3ccf4adb024","7b4ea9debb794a2aadee86393551d73a","343d710ac955496b9c0148cce0d609a0","91f8fe7b69084fd798c3dab20f9bf053","1dadf779ad2442d28d4ec1559ce540fb","a14bccdacb8a4b3fb1fbdde9a1260ee2","6ecb213445b64003aecd9fdf3b0921f5","f42a8aac81a848c4bb0b0318cf845f2b","f4dec41b6d9748e3b67a545041174306","1e8e045e9417476995215ceaea6a96a2","e896308d3197481b8c24574fa08715a6","add340f6d6e7402cbde862c7fd3b5cac","7272ffe624c14c5a910c749813c39c69","9dd3ff20c86d446ba47c38ad45a61a35","17896d1e35c0450ca127c4c44ab72faf","71ccb1ed05094280a2482728ed41c861","c689089fd644493e8fe93c714b6bc45c","cbe760e37d9a4627b64e1f32701b1207","d3ea023b135b4e29a3adc1ab21278125","734025eaebeb4e8c8388f2c163e3b5a3","e9f7fd99a01744738d50d082d1f85bbe","1e41ba36fa0743f08544eb2eaf9c951d","a20a372120bf4f11bdbc8632d572ef6f","ff993cfb7e864881b0b3019853e5ee87","62ec979e06ca466ba64e2b52bfdab3aa","e2fdbb638c2846b09dafde29f5fbecb2","2abf4e4e134c4bff8f734ac38f02b9f2","66d2a694571941d4beb51ce34afe5725","190f0e16fbef427c9656c5eac1f721ac","218b71de1bc44bba87aac761a3a114f9","ca79a7d844154c9788823053a1f36226","22d42d54fb204ab09c73c34adb6643f8","1e2d6e5671e6441eadba71603ac33150","4c4aa50a232b4391be6d7bd31dec083f","70555d85a05a4ca18d37b1c821f62107","049f970bf74f462497a9ef2985385fab","8c8b011fcc3b460bac2331b963731b41"]},"id":"6aSHLQgwyKgO","executionInfo":{"status":"ok","timestamp":1728888840715,"user_tz":-330,"elapsed":3643,"user":{"displayName":"Manish Negi","userId":"13518611288471091944"}},"outputId":"dfd01edc-0fd8-4e9b-dc10-2eb2bd4690b7"},"execution_count":42,"outputs":[{"output_type":"stream","name":"stderr","text":["The cache for model files in Transformers v4.22.0 has been updated. Migrating your old cache. This is a one-time only operation. You can interrupt this and resume the migration later on by calling `transformers.utils.move_cache()`.\n"]},{"output_type":"display_data","data":{"text/plain":["0it [00:00, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0e7e14019558466abb60e04b0c96f459"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"495d4fe660de45b5be11b83773d31c6a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1dadf779ad2442d28d4ec1559ce540fb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"71ccb1ed05094280a2482728ed41c861"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2abf4e4e134c4bff8f734ac38f02b9f2"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n","  warnings.warn(\n"]}]},{"cell_type":"code","source":["tokenizer"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2cLjEj9xzcxg","executionInfo":{"status":"ok","timestamp":1728888844481,"user_tz":-330,"elapsed":489,"user":{"displayName":"Manish Negi","userId":"13518611288471091944"}},"outputId":"34a01b2c-fbd9-4699-ac43-b46fe37d3fd4"},"execution_count":43,"outputs":[{"output_type":"execute_result","data":{"text/plain":["BertTokenizerFast(name_or_path='bert-base-uncased', vocab_size=30522, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n","\t0: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t100: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t101: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t102: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t103: AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","}"]},"metadata":{},"execution_count":43}]},{"cell_type":"code","source":["corpus = [\n","    \"This is the Hugging Face Course.\",\n","    \"This chapter is about tokenization.\",\n","    \"This section shows several tokenizer algorithms.\",\n","    \"Hopefully, you will be able to understand how they are trained and generate tokens.\",\n","]"],"metadata":{"id":"cVsyM_b2zixg","executionInfo":{"status":"ok","timestamp":1728889076995,"user_tz":-330,"elapsed":622,"user":{"displayName":"Manish Negi","userId":"13518611288471091944"}}},"execution_count":44,"outputs":[]},{"cell_type":"code","source":["tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str(\"This is the Hugging Face Course.\")\n","\n","# this starts from index =0 and ends at index = 4"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bQElhHZ70bYO","executionInfo":{"status":"ok","timestamp":1728889109660,"user_tz":-330,"elapsed":524,"user":{"displayName":"Manish Negi","userId":"13518611288471091944"}},"outputId":"812f62b7-bbf1-4a5f-a2b6-b44b5533a7a3"},"execution_count":45,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[('This', (0, 4)),\n"," ('is', (5, 7)),\n"," ('the', (8, 11)),\n"," ('Hugging', (12, 19)),\n"," ('Face', (20, 24)),\n"," ('Course', (25, 31)),\n"," ('.', (31, 32))]"]},"metadata":{},"execution_count":45}]},{"cell_type":"code","source":["\n","from collections import defaultdict\n","from transformers import AutoTokenizer\n","tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n","\n","corpus = [\n","    \"This is the Hugging Face Course.\",\n","    \"This chapter is about tokenization.\",\n","    \"This section shows several tokenizer algorithms.\",\n","    \"Hopefully, you will be able to understand how they are trained and generate tokens.\",\n","]\n","\n","### get the frequency of each word ###\n","word_freqs = defaultdict(int)\n","for text in corpus:\n","    words_with_offsets = tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str(text)\n","    new_words = [word for word, offset in words_with_offsets]\n","    print(new_words)\n","    for word in new_words:\n","        word_freqs[word] += 1\n","\n","print(f\"\\nFinal Word Frequency: {word_freqs}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NQBXFSXA0jYO","executionInfo":{"status":"ok","timestamp":1728890999395,"user_tz":-330,"elapsed":500,"user":{"displayName":"Manish Negi","userId":"13518611288471091944"}},"outputId":"f28d01c5-79a2-4c9d-965f-b319b83083ed"},"execution_count":47,"outputs":[{"output_type":"stream","name":"stdout","text":["['This', 'is', 'the', 'Hugging', 'Face', 'Course', '.']\n","['This', 'chapter', 'is', 'about', 'tokenization', '.']\n","['This', 'section', 'shows', 'several', 'tokenizer', 'algorithms', '.']\n","['Hopefully', ',', 'you', 'will', 'be', 'able', 'to', 'understand', 'how', 'they', 'are', 'trained', 'and', 'generate', 'tokens', '.']\n","\n","Final Word Frequency: defaultdict(<class 'int'>, {'This': 3, 'is': 2, 'the': 1, 'Hugging': 1, 'Face': 1, 'Course': 1, '.': 4, 'chapter': 1, 'about': 1, 'tokenization': 1, 'section': 1, 'shows': 1, 'several': 1, 'tokenizer': 1, 'algorithms': 1, 'Hopefully': 1, ',': 1, 'you': 1, 'will': 1, 'be': 1, 'able': 1, 'to': 1, 'understand': 1, 'how': 1, 'they': 1, 'are': 1, 'trained': 1, 'and': 1, 'generate': 1, 'tokens': 1})\n"]}]},{"cell_type":"code","source":["word_freqs"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8xjxzJLw7t0r","executionInfo":{"status":"ok","timestamp":1728891006901,"user_tz":-330,"elapsed":462,"user":{"displayName":"Manish Negi","userId":"13518611288471091944"}},"outputId":"8004038b-2f4f-41c6-8051-880153d99bc5"},"execution_count":48,"outputs":[{"output_type":"execute_result","data":{"text/plain":["defaultdict(int,\n","            {'This': 3,\n","             'is': 2,\n","             'the': 1,\n","             'Hugging': 1,\n","             'Face': 1,\n","             'Course': 1,\n","             '.': 4,\n","             'chapter': 1,\n","             'about': 1,\n","             'tokenization': 1,\n","             'section': 1,\n","             'shows': 1,\n","             'several': 1,\n","             'tokenizer': 1,\n","             'algorithms': 1,\n","             'Hopefully': 1,\n","             ',': 1,\n","             'you': 1,\n","             'will': 1,\n","             'be': 1,\n","             'able': 1,\n","             'to': 1,\n","             'understand': 1,\n","             'how': 1,\n","             'they': 1,\n","             'are': 1,\n","             'trained': 1,\n","             'and': 1,\n","             'generate': 1,\n","             'tokens': 1})"]},"metadata":{},"execution_count":48}]},{"cell_type":"code","source":["\n","### split all word into alphabet ###\n","alphabet = []\n","for word in word_freqs.keys():\n","    if word[0] not in alphabet:\n","        alphabet.append(word[0])\n","    for letter in word[1:]:\n","        if f\"##{letter}\" not in alphabet:\n","            alphabet.append(f\"##{letter}\")\n","\n","alphabet.sort()\n","print(f'All alphabets: {alphabet}')\n","\n","### insert special token and subword ###\n","vocab = [\"[PAD]\", \"[UNK]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"] + alphabet.copy()\n","splits = {word: [c if i == 0 else f\"##{c}\" for i, c in enumerate(word)] for word in word_freqs.keys()}\n","print(f'\\nSplitted Words: {splits}')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sY6cVabl7yuJ","executionInfo":{"status":"ok","timestamp":1728891021936,"user_tz":-330,"elapsed":463,"user":{"displayName":"Manish Negi","userId":"13518611288471091944"}},"outputId":"50ffcfba-d1c2-4818-8d6c-5cedcd6a17f4"},"execution_count":49,"outputs":[{"output_type":"stream","name":"stdout","text":["All alphabets: ['##a', '##b', '##c', '##d', '##e', '##f', '##g', '##h', '##i', '##k', '##l', '##m', '##n', '##o', '##p', '##r', '##s', '##t', '##u', '##v', '##w', '##y', '##z', ',', '.', 'C', 'F', 'H', 'T', 'a', 'b', 'c', 'g', 'h', 'i', 's', 't', 'u', 'w', 'y']\n","\n","Splitted Words: {'This': ['T', '##h', '##i', '##s'], 'is': ['i', '##s'], 'the': ['t', '##h', '##e'], 'Hugging': ['H', '##u', '##g', '##g', '##i', '##n', '##g'], 'Face': ['F', '##a', '##c', '##e'], 'Course': ['C', '##o', '##u', '##r', '##s', '##e'], '.': ['.'], 'chapter': ['c', '##h', '##a', '##p', '##t', '##e', '##r'], 'about': ['a', '##b', '##o', '##u', '##t'], 'tokenization': ['t', '##o', '##k', '##e', '##n', '##i', '##z', '##a', '##t', '##i', '##o', '##n'], 'section': ['s', '##e', '##c', '##t', '##i', '##o', '##n'], 'shows': ['s', '##h', '##o', '##w', '##s'], 'several': ['s', '##e', '##v', '##e', '##r', '##a', '##l'], 'tokenizer': ['t', '##o', '##k', '##e', '##n', '##i', '##z', '##e', '##r'], 'algorithms': ['a', '##l', '##g', '##o', '##r', '##i', '##t', '##h', '##m', '##s'], 'Hopefully': ['H', '##o', '##p', '##e', '##f', '##u', '##l', '##l', '##y'], ',': [','], 'you': ['y', '##o', '##u'], 'will': ['w', '##i', '##l', '##l'], 'be': ['b', '##e'], 'able': ['a', '##b', '##l', '##e'], 'to': ['t', '##o'], 'understand': ['u', '##n', '##d', '##e', '##r', '##s', '##t', '##a', '##n', '##d'], 'how': ['h', '##o', '##w'], 'they': ['t', '##h', '##e', '##y'], 'are': ['a', '##r', '##e'], 'trained': ['t', '##r', '##a', '##i', '##n', '##e', '##d'], 'and': ['a', '##n', '##d'], 'generate': ['g', '##e', '##n', '##e', '##r', '##a', '##t', '##e'], 'tokens': ['t', '##o', '##k', '##e', '##n', '##s']}\n"]}]},{"cell_type":"code","source":["splits"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1QwbYrF172Y-","executionInfo":{"status":"ok","timestamp":1728891034310,"user_tz":-330,"elapsed":501,"user":{"displayName":"Manish Negi","userId":"13518611288471091944"}},"outputId":"dbdb1fb1-dcd4-4873-9008-6aed3cefae66"},"execution_count":50,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'This': ['T', '##h', '##i', '##s'],\n"," 'is': ['i', '##s'],\n"," 'the': ['t', '##h', '##e'],\n"," 'Hugging': ['H', '##u', '##g', '##g', '##i', '##n', '##g'],\n"," 'Face': ['F', '##a', '##c', '##e'],\n"," 'Course': ['C', '##o', '##u', '##r', '##s', '##e'],\n"," '.': ['.'],\n"," 'chapter': ['c', '##h', '##a', '##p', '##t', '##e', '##r'],\n"," 'about': ['a', '##b', '##o', '##u', '##t'],\n"," 'tokenization': ['t',\n","  '##o',\n","  '##k',\n","  '##e',\n","  '##n',\n","  '##i',\n","  '##z',\n","  '##a',\n","  '##t',\n","  '##i',\n","  '##o',\n","  '##n'],\n"," 'section': ['s', '##e', '##c', '##t', '##i', '##o', '##n'],\n"," 'shows': ['s', '##h', '##o', '##w', '##s'],\n"," 'several': ['s', '##e', '##v', '##e', '##r', '##a', '##l'],\n"," 'tokenizer': ['t', '##o', '##k', '##e', '##n', '##i', '##z', '##e', '##r'],\n"," 'algorithms': ['a',\n","  '##l',\n","  '##g',\n","  '##o',\n","  '##r',\n","  '##i',\n","  '##t',\n","  '##h',\n","  '##m',\n","  '##s'],\n"," 'Hopefully': ['H', '##o', '##p', '##e', '##f', '##u', '##l', '##l', '##y'],\n"," ',': [','],\n"," 'you': ['y', '##o', '##u'],\n"," 'will': ['w', '##i', '##l', '##l'],\n"," 'be': ['b', '##e'],\n"," 'able': ['a', '##b', '##l', '##e'],\n"," 'to': ['t', '##o'],\n"," 'understand': ['u',\n","  '##n',\n","  '##d',\n","  '##e',\n","  '##r',\n","  '##s',\n","  '##t',\n","  '##a',\n","  '##n',\n","  '##d'],\n"," 'how': ['h', '##o', '##w'],\n"," 'they': ['t', '##h', '##e', '##y'],\n"," 'are': ['a', '##r', '##e'],\n"," 'trained': ['t', '##r', '##a', '##i', '##n', '##e', '##d'],\n"," 'and': ['a', '##n', '##d'],\n"," 'generate': ['g', '##e', '##n', '##e', '##r', '##a', '##t', '##e'],\n"," 'tokens': ['t', '##o', '##k', '##e', '##n', '##s']}"]},"metadata":{},"execution_count":50}]},{"cell_type":"code","source":["vocab"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0Wv1PBpt75ZZ","executionInfo":{"status":"ok","timestamp":1728891048389,"user_tz":-330,"elapsed":493,"user":{"displayName":"Manish Negi","userId":"13518611288471091944"}},"outputId":"82aa3c90-ef3a-4326-eb4c-b2c1f6d6edf1"},"execution_count":51,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['[PAD]',\n"," '[UNK]',\n"," '[CLS]',\n"," '[SEP]',\n"," '[MASK]',\n"," '##a',\n"," '##b',\n"," '##c',\n"," '##d',\n"," '##e',\n"," '##f',\n"," '##g',\n"," '##h',\n"," '##i',\n"," '##k',\n"," '##l',\n"," '##m',\n"," '##n',\n"," '##o',\n"," '##p',\n"," '##r',\n"," '##s',\n"," '##t',\n"," '##u',\n"," '##v',\n"," '##w',\n"," '##y',\n"," '##z',\n"," ',',\n"," '.',\n"," 'C',\n"," 'F',\n"," 'H',\n"," 'T',\n"," 'a',\n"," 'b',\n"," 'c',\n"," 'g',\n"," 'h',\n"," 'i',\n"," 's',\n"," 't',\n"," 'u',\n"," 'w',\n"," 'y']"]},"metadata":{},"execution_count":51}]},{"cell_type":"code","source":["alphabet"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EJRRp3r8782K","executionInfo":{"status":"ok","timestamp":1728891060589,"user_tz":-330,"elapsed":515,"user":{"displayName":"Manish Negi","userId":"13518611288471091944"}},"outputId":"1e267888-ab49-4779-f3d9-41603a58f493"},"execution_count":52,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['##a',\n"," '##b',\n"," '##c',\n"," '##d',\n"," '##e',\n"," '##f',\n"," '##g',\n"," '##h',\n"," '##i',\n"," '##k',\n"," '##l',\n"," '##m',\n"," '##n',\n"," '##o',\n"," '##p',\n"," '##r',\n"," '##s',\n"," '##t',\n"," '##u',\n"," '##v',\n"," '##w',\n"," '##y',\n"," '##z',\n"," ',',\n"," '.',\n"," 'C',\n"," 'F',\n"," 'H',\n"," 'T',\n"," 'a',\n"," 'b',\n"," 'c',\n"," 'g',\n"," 'h',\n"," 'i',\n"," 's',\n"," 't',\n"," 'u',\n"," 'w',\n"," 'y']"]},"metadata":{},"execution_count":52}]},{"cell_type":"code","source":["len(vocab), len(alphabet  )"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Zzxtg3jD7_z7","executionInfo":{"status":"ok","timestamp":1728891115481,"user_tz":-330,"elapsed":483,"user":{"displayName":"Manish Negi","userId":"13518611288471091944"}},"outputId":"011845c0-da74-481b-abb3-72412cf1273f"},"execution_count":54,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(45, 40)"]},"metadata":{},"execution_count":54}]},{"cell_type":"code","source":[" ### compute score for merging ###\n","def compute_pair_scores(splits):\n","    letter_freqs = defaultdict(int)\n","    pair_freqs = defaultdict(int)\n","\n","    for word, freq in word_freqs.items():\n","        split = splits[word]\n","        if len(split) == 1:\n","            letter_freqs[split[0]] += freq\n","            continue\n","        for i in range(len(split) - 1):\n","            pair = (split[i], split[i + 1])\n","            letter_freqs[split[i]] += freq\n","            pair_freqs[pair] += freq\n","        letter_freqs[split[-1]] += freq\n","\n","    scores = {\n","        pair: freq / (letter_freqs[pair[0]] * letter_freqs[pair[1]])\n","        for pair, freq in pair_freqs.items()\n","    }\n","    return scores\n","\n","pair_scores = compute_pair_scores(splits)\n","print(f'Scores for each Pair: {pair_scores}')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GKOcuIJy8Ia6","executionInfo":{"status":"ok","timestamp":1728891179533,"user_tz":-330,"elapsed":509,"user":{"displayName":"Manish Negi","userId":"13518611288471091944"}},"outputId":"73d87f60-0955-4cd7-c071-3fe5e4f17580"},"execution_count":55,"outputs":[{"output_type":"stream","name":"stdout","text":["Scores for each Pair: {('T', '##h'): 0.125, ('##h', '##i'): 0.03409090909090909, ('##i', '##s'): 0.02727272727272727, ('i', '##s'): 0.1, ('t', '##h'): 0.03571428571428571, ('##h', '##e'): 0.011904761904761904, ('H', '##u'): 0.1, ('##u', '##g'): 0.05, ('##g', '##g'): 0.0625, ('##g', '##i'): 0.022727272727272728, ('##i', '##n'): 0.01652892561983471, ('##n', '##g'): 0.022727272727272728, ('F', '##a'): 0.14285714285714285, ('##a', '##c'): 0.07142857142857142, ('##c', '##e'): 0.023809523809523808, ('C', '##o'): 0.07692307692307693, ('##o', '##u'): 0.046153846153846156, ('##u', '##r'): 0.022222222222222223, ('##r', '##s'): 0.022222222222222223, ('##s', '##e'): 0.004761904761904762, ('c', '##h'): 0.125, ('##h', '##a'): 0.017857142857142856, ('##a', '##p'): 0.07142857142857142, ('##p', '##t'): 0.07142857142857142, ('##t', '##e'): 0.013605442176870748, ('##e', '##r'): 0.026455026455026454, ('a', '##b'): 0.2, ('##b', '##o'): 0.038461538461538464, ('##u', '##t'): 0.02857142857142857, ('t', '##o'): 0.04395604395604396, ('##o', '##k'): 0.07692307692307693, ('##k', '##e'): 0.047619047619047616, ('##e', '##n'): 0.017316017316017316, ('##n', '##i'): 0.01652892561983471, ('##i', '##z'): 0.09090909090909091, ('##z', '##a'): 0.07142857142857142, ('##a', '##t'): 0.04081632653061224, ('##t', '##i'): 0.025974025974025976, ('##i', '##o'): 0.013986013986013986, ('##o', '##n'): 0.013986013986013986, ('s', '##e'): 0.031746031746031744, ('##e', '##c'): 0.023809523809523808, ('##c', '##t'): 0.07142857142857142, ('s', '##h'): 0.041666666666666664, ('##h', '##o'): 0.009615384615384616, ('##o', '##w'): 0.07692307692307693, ('##w', '##s'): 0.05, ('##e', '##v'): 0.047619047619047616, ('##v', '##e'): 0.047619047619047616, ('##r', '##a'): 0.047619047619047616, ('##a', '##l'): 0.02040816326530612, ('##z', '##e'): 0.023809523809523808, ('a', '##l'): 0.02857142857142857, ('##l', '##g'): 0.03571428571428571, ('##g', '##o'): 0.019230769230769232, ('##o', '##r'): 0.008547008547008548, ('##r', '##i'): 0.010101010101010102, ('##i', '##t'): 0.012987012987012988, ('##t', '##h'): 0.017857142857142856, ('##h', '##m'): 0.125, ('##m', '##s'): 0.1, ('H', '##o'): 0.038461538461538464, ('##o', '##p'): 0.038461538461538464, ('##p', '##e'): 0.023809523809523808, ('##e', '##f'): 0.047619047619047616, ('##f', '##u'): 0.2, ('##u', '##l'): 0.02857142857142857, ('##l', '##l'): 0.04081632653061224, ('##l', '##y'): 0.07142857142857142, ('y', '##o'): 0.07692307692307693, ('w', '##i'): 0.09090909090909091, ('##i', '##l'): 0.012987012987012988, ('b', '##e'): 0.047619047619047616, ('##b', '##l'): 0.07142857142857142, ('##l', '##e'): 0.006802721088435374, ('u', '##n'): 0.09090909090909091, ('##n', '##d'): 0.06818181818181818, ('##d', '##e'): 0.011904761904761904, ('##s', '##t'): 0.014285714285714285, ('##t', '##a'): 0.02040816326530612, ('##a', '##n'): 0.012987012987012988, ('h', '##o'): 0.07692307692307693, ('##e', '##y'): 0.023809523809523808, ('a', '##r'): 0.022222222222222223, ('##r', '##e'): 0.005291005291005291, ('t', '##r'): 0.015873015873015872, ('##a', '##i'): 0.012987012987012988, ('##n', '##e'): 0.008658008658008658, ('##e', '##d'): 0.011904761904761904, ('a', '##n'): 0.01818181818181818, ('g', '##e'): 0.047619047619047616, ('##n', '##s'): 0.00909090909090909}\n"]}]},{"cell_type":"code","source":["pair_scores"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"noQX4xre8c2X","executionInfo":{"status":"ok","timestamp":1728891185724,"user_tz":-330,"elapsed":486,"user":{"displayName":"Manish Negi","userId":"13518611288471091944"}},"outputId":"777d5319-d863-492b-e2a5-2491e0827e02"},"execution_count":56,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{('T', '##h'): 0.125,\n"," ('##h', '##i'): 0.03409090909090909,\n"," ('##i', '##s'): 0.02727272727272727,\n"," ('i', '##s'): 0.1,\n"," ('t', '##h'): 0.03571428571428571,\n"," ('##h', '##e'): 0.011904761904761904,\n"," ('H', '##u'): 0.1,\n"," ('##u', '##g'): 0.05,\n"," ('##g', '##g'): 0.0625,\n"," ('##g', '##i'): 0.022727272727272728,\n"," ('##i', '##n'): 0.01652892561983471,\n"," ('##n', '##g'): 0.022727272727272728,\n"," ('F', '##a'): 0.14285714285714285,\n"," ('##a', '##c'): 0.07142857142857142,\n"," ('##c', '##e'): 0.023809523809523808,\n"," ('C', '##o'): 0.07692307692307693,\n"," ('##o', '##u'): 0.046153846153846156,\n"," ('##u', '##r'): 0.022222222222222223,\n"," ('##r', '##s'): 0.022222222222222223,\n"," ('##s', '##e'): 0.004761904761904762,\n"," ('c', '##h'): 0.125,\n"," ('##h', '##a'): 0.017857142857142856,\n"," ('##a', '##p'): 0.07142857142857142,\n"," ('##p', '##t'): 0.07142857142857142,\n"," ('##t', '##e'): 0.013605442176870748,\n"," ('##e', '##r'): 0.026455026455026454,\n"," ('a', '##b'): 0.2,\n"," ('##b', '##o'): 0.038461538461538464,\n"," ('##u', '##t'): 0.02857142857142857,\n"," ('t', '##o'): 0.04395604395604396,\n"," ('##o', '##k'): 0.07692307692307693,\n"," ('##k', '##e'): 0.047619047619047616,\n"," ('##e', '##n'): 0.017316017316017316,\n"," ('##n', '##i'): 0.01652892561983471,\n"," ('##i', '##z'): 0.09090909090909091,\n"," ('##z', '##a'): 0.07142857142857142,\n"," ('##a', '##t'): 0.04081632653061224,\n"," ('##t', '##i'): 0.025974025974025976,\n"," ('##i', '##o'): 0.013986013986013986,\n"," ('##o', '##n'): 0.013986013986013986,\n"," ('s', '##e'): 0.031746031746031744,\n"," ('##e', '##c'): 0.023809523809523808,\n"," ('##c', '##t'): 0.07142857142857142,\n"," ('s', '##h'): 0.041666666666666664,\n"," ('##h', '##o'): 0.009615384615384616,\n"," ('##o', '##w'): 0.07692307692307693,\n"," ('##w', '##s'): 0.05,\n"," ('##e', '##v'): 0.047619047619047616,\n"," ('##v', '##e'): 0.047619047619047616,\n"," ('##r', '##a'): 0.047619047619047616,\n"," ('##a', '##l'): 0.02040816326530612,\n"," ('##z', '##e'): 0.023809523809523808,\n"," ('a', '##l'): 0.02857142857142857,\n"," ('##l', '##g'): 0.03571428571428571,\n"," ('##g', '##o'): 0.019230769230769232,\n"," ('##o', '##r'): 0.008547008547008548,\n"," ('##r', '##i'): 0.010101010101010102,\n"," ('##i', '##t'): 0.012987012987012988,\n"," ('##t', '##h'): 0.017857142857142856,\n"," ('##h', '##m'): 0.125,\n"," ('##m', '##s'): 0.1,\n"," ('H', '##o'): 0.038461538461538464,\n"," ('##o', '##p'): 0.038461538461538464,\n"," ('##p', '##e'): 0.023809523809523808,\n"," ('##e', '##f'): 0.047619047619047616,\n"," ('##f', '##u'): 0.2,\n"," ('##u', '##l'): 0.02857142857142857,\n"," ('##l', '##l'): 0.04081632653061224,\n"," ('##l', '##y'): 0.07142857142857142,\n"," ('y', '##o'): 0.07692307692307693,\n"," ('w', '##i'): 0.09090909090909091,\n"," ('##i', '##l'): 0.012987012987012988,\n"," ('b', '##e'): 0.047619047619047616,\n"," ('##b', '##l'): 0.07142857142857142,\n"," ('##l', '##e'): 0.006802721088435374,\n"," ('u', '##n'): 0.09090909090909091,\n"," ('##n', '##d'): 0.06818181818181818,\n"," ('##d', '##e'): 0.011904761904761904,\n"," ('##s', '##t'): 0.014285714285714285,\n"," ('##t', '##a'): 0.02040816326530612,\n"," ('##a', '##n'): 0.012987012987012988,\n"," ('h', '##o'): 0.07692307692307693,\n"," ('##e', '##y'): 0.023809523809523808,\n"," ('a', '##r'): 0.022222222222222223,\n"," ('##r', '##e'): 0.005291005291005291,\n"," ('t', '##r'): 0.015873015873015872,\n"," ('##a', '##i'): 0.012987012987012988,\n"," ('##n', '##e'): 0.008658008658008658,\n"," ('##e', '##d'): 0.011904761904761904,\n"," ('a', '##n'): 0.01818181818181818,\n"," ('g', '##e'): 0.047619047619047616,\n"," ('##n', '##s'): 0.00909090909090909}"]},"metadata":{},"execution_count":56}]},{"cell_type":"code","source":["splits"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ypVM1Zyy8eXo","executionInfo":{"status":"ok","timestamp":1728891206844,"user_tz":-330,"elapsed":480,"user":{"displayName":"Manish Negi","userId":"13518611288471091944"}},"outputId":"4bc8b1b5-e864-40ab-fe73-d2bc22bc7f00"},"execution_count":57,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'This': ['T', '##h', '##i', '##s'],\n"," 'is': ['i', '##s'],\n"," 'the': ['t', '##h', '##e'],\n"," 'Hugging': ['H', '##u', '##g', '##g', '##i', '##n', '##g'],\n"," 'Face': ['F', '##a', '##c', '##e'],\n"," 'Course': ['C', '##o', '##u', '##r', '##s', '##e'],\n"," '.': ['.'],\n"," 'chapter': ['c', '##h', '##a', '##p', '##t', '##e', '##r'],\n"," 'about': ['a', '##b', '##o', '##u', '##t'],\n"," 'tokenization': ['t',\n","  '##o',\n","  '##k',\n","  '##e',\n","  '##n',\n","  '##i',\n","  '##z',\n","  '##a',\n","  '##t',\n","  '##i',\n","  '##o',\n","  '##n'],\n"," 'section': ['s', '##e', '##c', '##t', '##i', '##o', '##n'],\n"," 'shows': ['s', '##h', '##o', '##w', '##s'],\n"," 'several': ['s', '##e', '##v', '##e', '##r', '##a', '##l'],\n"," 'tokenizer': ['t', '##o', '##k', '##e', '##n', '##i', '##z', '##e', '##r'],\n"," 'algorithms': ['a',\n","  '##l',\n","  '##g',\n","  '##o',\n","  '##r',\n","  '##i',\n","  '##t',\n","  '##h',\n","  '##m',\n","  '##s'],\n"," 'Hopefully': ['H', '##o', '##p', '##e', '##f', '##u', '##l', '##l', '##y'],\n"," ',': [','],\n"," 'you': ['y', '##o', '##u'],\n"," 'will': ['w', '##i', '##l', '##l'],\n"," 'be': ['b', '##e'],\n"," 'able': ['a', '##b', '##l', '##e'],\n"," 'to': ['t', '##o'],\n"," 'understand': ['u',\n","  '##n',\n","  '##d',\n","  '##e',\n","  '##r',\n","  '##s',\n","  '##t',\n","  '##a',\n","  '##n',\n","  '##d'],\n"," 'how': ['h', '##o', '##w'],\n"," 'they': ['t', '##h', '##e', '##y'],\n"," 'are': ['a', '##r', '##e'],\n"," 'trained': ['t', '##r', '##a', '##i', '##n', '##e', '##d'],\n"," 'and': ['a', '##n', '##d'],\n"," 'generate': ['g', '##e', '##n', '##e', '##r', '##a', '##t', '##e'],\n"," 'tokens': ['t', '##o', '##k', '##e', '##n', '##s']}"]},"metadata":{},"execution_count":57}]},{"cell_type":"code","source":["### finding pair with best score ###\n","best_pair = \"\"\n","max_score = None\n","for pair, score in pair_scores.items():\n","    if max_score is None or max_score < score:\n","        best_pair = pair\n","        max_score = score\n","\n","print(best_pair, max_score)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"y3NGW6Jc8jh4","executionInfo":{"status":"ok","timestamp":1728891336900,"user_tz":-330,"elapsed":718,"user":{"displayName":"Manish Negi","userId":"13518611288471091944"}},"outputId":"c6a55db1-7c25-45e2-cfdd-d25508cb55f3"},"execution_count":58,"outputs":[{"output_type":"stream","name":"stdout","text":["('a', '##b') 0.2\n"]}]},{"cell_type":"code","source":["best_pair"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ge_xWBwM9DOo","executionInfo":{"status":"ok","timestamp":1728891341525,"user_tz":-330,"elapsed":480,"user":{"displayName":"Manish Negi","userId":"13518611288471091944"}},"outputId":"32a48a74-dd5f-402a-9584-f2aa34da5adc"},"execution_count":59,"outputs":[{"output_type":"execute_result","data":{"text/plain":["('a', '##b')"]},"metadata":{},"execution_count":59}]},{"cell_type":"code","source":["max_score"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yMx0_STI9EZ4","executionInfo":{"status":"ok","timestamp":1728891437439,"user_tz":-330,"elapsed":479,"user":{"displayName":"Manish Negi","userId":"13518611288471091944"}},"outputId":"5e4d287e-a6fb-488e-b257-37ad3cc16a83"},"execution_count":60,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.2"]},"metadata":{},"execution_count":60}]},{"cell_type":"code","source":["### merge pair ###\n","def merge_pair(a, b, splits):\n","    for word in word_freqs:\n","        split = splits[word]\n","        if len(split) == 1:\n","            continue\n","        i = 0\n","        while i < len(split) - 1:\n","            if split[i] == a and split[i + 1] == b:\n","                merge = a + b[2:] if b.startswith(\"##\") else a + b\n","                split = split[:i] + [merge] + split[i + 2 :]\n","            else:\n","                i += 1\n","        splits[word] = split\n","    return splits\n","\n","splits = merge_pair(\"a\", \"##b\", splits)\n","print(splits[\"about\"])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pXwthCZ59b0o","executionInfo":{"status":"ok","timestamp":1728891503927,"user_tz":-330,"elapsed":514,"user":{"displayName":"Manish Negi","userId":"13518611288471091944"}},"outputId":"16151bc4-e302-4e6c-e808-d649d4f9979e"},"execution_count":61,"outputs":[{"output_type":"stream","name":"stdout","text":["['ab', '##o', '##u', '##t']\n"]}]},{"cell_type":"code","source":["splits"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Ps9hK3XM9sC5","executionInfo":{"status":"ok","timestamp":1728891510999,"user_tz":-330,"elapsed":481,"user":{"displayName":"Manish Negi","userId":"13518611288471091944"}},"outputId":"c21228de-c59c-4b5a-ee80-b9f26f45e744"},"execution_count":62,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'This': ['T', '##h', '##i', '##s'],\n"," 'is': ['i', '##s'],\n"," 'the': ['t', '##h', '##e'],\n"," 'Hugging': ['H', '##u', '##g', '##g', '##i', '##n', '##g'],\n"," 'Face': ['F', '##a', '##c', '##e'],\n"," 'Course': ['C', '##o', '##u', '##r', '##s', '##e'],\n"," '.': ['.'],\n"," 'chapter': ['c', '##h', '##a', '##p', '##t', '##e', '##r'],\n"," 'about': ['ab', '##o', '##u', '##t'],\n"," 'tokenization': ['t',\n","  '##o',\n","  '##k',\n","  '##e',\n","  '##n',\n","  '##i',\n","  '##z',\n","  '##a',\n","  '##t',\n","  '##i',\n","  '##o',\n","  '##n'],\n"," 'section': ['s', '##e', '##c', '##t', '##i', '##o', '##n'],\n"," 'shows': ['s', '##h', '##o', '##w', '##s'],\n"," 'several': ['s', '##e', '##v', '##e', '##r', '##a', '##l'],\n"," 'tokenizer': ['t', '##o', '##k', '##e', '##n', '##i', '##z', '##e', '##r'],\n"," 'algorithms': ['a',\n","  '##l',\n","  '##g',\n","  '##o',\n","  '##r',\n","  '##i',\n","  '##t',\n","  '##h',\n","  '##m',\n","  '##s'],\n"," 'Hopefully': ['H', '##o', '##p', '##e', '##f', '##u', '##l', '##l', '##y'],\n"," ',': [','],\n"," 'you': ['y', '##o', '##u'],\n"," 'will': ['w', '##i', '##l', '##l'],\n"," 'be': ['b', '##e'],\n"," 'able': ['ab', '##l', '##e'],\n"," 'to': ['t', '##o'],\n"," 'understand': ['u',\n","  '##n',\n","  '##d',\n","  '##e',\n","  '##r',\n","  '##s',\n","  '##t',\n","  '##a',\n","  '##n',\n","  '##d'],\n"," 'how': ['h', '##o', '##w'],\n"," 'they': ['t', '##h', '##e', '##y'],\n"," 'are': ['a', '##r', '##e'],\n"," 'trained': ['t', '##r', '##a', '##i', '##n', '##e', '##d'],\n"," 'and': ['a', '##n', '##d'],\n"," 'generate': ['g', '##e', '##n', '##e', '##r', '##a', '##t', '##e'],\n"," 'tokens': ['t', '##o', '##k', '##e', '##n', '##s']}"]},"metadata":{},"execution_count":62}]},{"cell_type":"code","source":["vocab"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7n19PA999tyZ","executionInfo":{"status":"ok","timestamp":1728891551409,"user_tz":-330,"elapsed":459,"user":{"displayName":"Manish Negi","userId":"13518611288471091944"}},"outputId":"b302aec4-5553-4140-d9db-28efd3c8a4eb"},"execution_count":63,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['[PAD]',\n"," '[UNK]',\n"," '[CLS]',\n"," '[SEP]',\n"," '[MASK]',\n"," '##a',\n"," '##b',\n"," '##c',\n"," '##d',\n"," '##e',\n"," '##f',\n"," '##g',\n"," '##h',\n"," '##i',\n"," '##k',\n"," '##l',\n"," '##m',\n"," '##n',\n"," '##o',\n"," '##p',\n"," '##r',\n"," '##s',\n"," '##t',\n"," '##u',\n"," '##v',\n"," '##w',\n"," '##y',\n"," '##z',\n"," ',',\n"," '.',\n"," 'C',\n"," 'F',\n"," 'H',\n"," 'T',\n"," 'a',\n"," 'b',\n"," 'c',\n"," 'g',\n"," 'h',\n"," 'i',\n"," 's',\n"," 't',\n"," 'u',\n"," 'w',\n"," 'y']"]},"metadata":{},"execution_count":63}]},{"cell_type":"code","source":["len(vocab)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"crYLEmv0-p4S","executionInfo":{"status":"ok","timestamp":1728891766535,"user_tz":-330,"elapsed":710,"user":{"displayName":"Manish Negi","userId":"13518611288471091944"}},"outputId":"383cf795-b5a3-4666-b5e4-6d5162b55704"},"execution_count":64,"outputs":[{"output_type":"execute_result","data":{"text/plain":["45"]},"metadata":{},"execution_count":64}]},{"cell_type":"code","source":["### keep looping to merge more pair\n","vocab_size = 70\n","while len(vocab) < vocab_size:\n","    scores = compute_pair_scores(splits)\n","    best_pair, max_score = \"\", None\n","    for pair, score in scores.items():\n","        if max_score is None or max_score < score:\n","            best_pair = pair\n","            max_score = score\n","    splits = merge_pair(*best_pair, splits)\n","    new_token = (\n","        best_pair[0] + best_pair[1][2:]\n","        if best_pair[1].startswith(\"##\")\n","        else best_pair[0] + best_pair[1]\n","    )\n","    vocab.append(new_token)\n","\n","print(f'Final Vocab: {vocab}')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"h5_ikX8e93qI","executionInfo":{"status":"ok","timestamp":1728891773653,"user_tz":-330,"elapsed":487,"user":{"displayName":"Manish Negi","userId":"13518611288471091944"}},"outputId":"bb25ed7e-17db-475a-cc97-6ebee97753e1"},"execution_count":65,"outputs":[{"output_type":"stream","name":"stdout","text":["Final Vocab: ['[PAD]', '[UNK]', '[CLS]', '[SEP]', '[MASK]', '##a', '##b', '##c', '##d', '##e', '##f', '##g', '##h', '##i', '##k', '##l', '##m', '##n', '##o', '##p', '##r', '##s', '##t', '##u', '##v', '##w', '##y', '##z', ',', '.', 'C', 'F', 'H', 'T', 'a', 'b', 'c', 'g', 'h', 'i', 's', 't', 'u', 'w', 'y', '##fu', 'Fa', 'Fac', '##ct', '##ful', '##full', '##fully', 'Th', 'ch', '##hm', 'cha', 'chap', 'chapt', '##thm', 'Hu', 'Hug', 'Hugg', 'sh', 'th', 'is', '##thms', '##za', '##zat', '##ut', '##ta']\n"]}]},{"cell_type":"code","source":["vocab"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sGJ_kDEj-uuK","executionInfo":{"status":"ok","timestamp":1728891782367,"user_tz":-330,"elapsed":485,"user":{"displayName":"Manish Negi","userId":"13518611288471091944"}},"outputId":"38ce77d0-563a-4e6b-819b-1d1e284836ad"},"execution_count":66,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['[PAD]',\n"," '[UNK]',\n"," '[CLS]',\n"," '[SEP]',\n"," '[MASK]',\n"," '##a',\n"," '##b',\n"," '##c',\n"," '##d',\n"," '##e',\n"," '##f',\n"," '##g',\n"," '##h',\n"," '##i',\n"," '##k',\n"," '##l',\n"," '##m',\n"," '##n',\n"," '##o',\n"," '##p',\n"," '##r',\n"," '##s',\n"," '##t',\n"," '##u',\n"," '##v',\n"," '##w',\n"," '##y',\n"," '##z',\n"," ',',\n"," '.',\n"," 'C',\n"," 'F',\n"," 'H',\n"," 'T',\n"," 'a',\n"," 'b',\n"," 'c',\n"," 'g',\n"," 'h',\n"," 'i',\n"," 's',\n"," 't',\n"," 'u',\n"," 'w',\n"," 'y',\n"," '##fu',\n"," 'Fa',\n"," 'Fac',\n"," '##ct',\n"," '##ful',\n"," '##full',\n"," '##fully',\n"," 'Th',\n"," 'ch',\n"," '##hm',\n"," 'cha',\n"," 'chap',\n"," 'chapt',\n"," '##thm',\n"," 'Hu',\n"," 'Hug',\n"," 'Hugg',\n"," 'sh',\n"," 'th',\n"," 'is',\n"," '##thms',\n"," '##za',\n"," '##zat',\n"," '##ut',\n"," '##ta']"]},"metadata":{},"execution_count":66}]},{"cell_type":"code","source":["len(vocab)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PQVvMQn3-wCp","executionInfo":{"status":"ok","timestamp":1728891791359,"user_tz":-330,"elapsed":468,"user":{"displayName":"Manish Negi","userId":"13518611288471091944"}},"outputId":"24675ce2-c1a8-4e4b-8c6a-327e5b75a723"},"execution_count":67,"outputs":[{"output_type":"execute_result","data":{"text/plain":["70"]},"metadata":{},"execution_count":67}]},{"cell_type":"code","source":["### ro encode a word ###\n","def encode_word(word):\n","    tokens = []\n","    while len(word) > 0:\n","        i = len(word)\n","        while i > 0 and word[:i] not in vocab:\n","            i -= 1\n","        if i == 0:\n","            return [\"[UNK]\"]\n","        tokens.append(word[:i])\n","        word = word[i:]\n","        if len(word) > 0:\n","            word = f\"##{word}\"\n","    return tokens\n","\n","print(encode_word(\"Hugging\"))\n","print(encode_word(\"HOgging\"))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"abo2GSQk-yC5","executionInfo":{"status":"ok","timestamp":1728891809193,"user_tz":-330,"elapsed":738,"user":{"displayName":"Manish Negi","userId":"13518611288471091944"}},"outputId":"0f4ba2ea-0118-41a1-892b-a08280242e66"},"execution_count":68,"outputs":[{"output_type":"stream","name":"stdout","text":["['Hugg', '##i', '##n', '##g']\n","['[UNK]']\n"]}]},{"cell_type":"markdown","source":["The output of the code `tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str(\"This is the Hugging Face Course.\")` gives you a detailed breakdown of how the text is pre-tokenized. Let's break down the output step by step:\n","\n","### Breakdown of the Output\n","\n","The output is a list of tuples, where each tuple represents a token and its corresponding position in the original string. Here’s the format of each tuple:\n","\n","- **Token**: The word or punctuation mark identified as a token.\n","- **Position**: A tuple containing two integers:\n","  - The starting index of the token in the original string.\n","  - The ending index of the token in the original string (exclusive).\n","\n","### Example Output Explained\n","\n","The output is as follows:\n","\n","```python\n","[\n","    ('This', (0, 4)),\n","    ('is', (5, 7)),\n","    ('the', (8, 11)),\n","    ('Hugging', (12, 19)),\n","    ('Face', (20, 24)),\n","    ('Course', (25, 31)),\n","    ('.', (31, 32))\n","]\n","```\n","\n","1. **Token: 'This'**  \n","   - **Position**: (0, 4)  \n","   - This means that the word \"This\" starts at index 0 and ends at index 4 in the original string.\n","\n","2. **Token: 'is'**  \n","   - **Position**: (5, 7)  \n","   - The word \"is\" starts at index 5 and ends at index 7.\n","\n","3. **Token: 'the'**  \n","   - **Position**: (8, 11)  \n","   - The word \"the\" starts at index 8 and ends at index 11.\n","\n","4. **Token: 'Hugging'**  \n","   - **Position**: (12, 19)  \n","   - The word \"Hugging\" starts at index 12 and ends at index 19.\n","\n","5. **Token: 'Face'**  \n","   - **Position**: (20, 24)  \n","   - The word \"Face\" starts at index 20 and ends at index 24.\n","\n","6. **Token: 'Course'**  \n","   - **Position**: (25, 31)  \n","   - The word \"Course\" starts at index 25 and ends at index 31.\n","\n","7. **Token: '.'**  \n","   - **Position**: (31, 32)  \n","   - The punctuation \".\" starts at index 31 and ends at index 32.\n","\n","### Significance of the Output\n","\n","This output is crucial for understanding how the tokenizer preprocesses the input text:\n","\n","- **Pre-tokenization**: The step performed by the pre-tokenizer divides the input string into tokens based on whitespace and possibly other criteria (in this case, it simply splits by spaces and punctuation).\n","- **Index Tracking**: The positions allow you to see exactly where each token is located in the original input. This can be useful for tasks where you need to map tokens back to their original locations in the text, such as in Named Entity Recognition (NER) or when generating token-level annotations.\n","\n","### Conclusion\n","\n","The output you received is a clear representation of how the tokenizer processes input strings, identifying individual tokens and their respective positions, which is essential for any further tokenization, encoding, or NLP tasks."],"metadata":{"id":"2g6oMMYHAYot"}},{"cell_type":"markdown","source":["Sure! Let’s break down the code you provided into its components and explain the steps involved, focusing on pre-tokenization and splitting into individual characters.\n","\n","### Pre-tokenization\n","```python\n","word_freqs = defaultdict(int)\n","for text in corpus:\n","    words_with_offsets = tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str(text)\n","    new_words = [word for word, offset in words_with_offsets]\n","    for word in new_words:\n","        word_freqs[word] += 1\n","\n","print(word_freqs)\n","```\n","\n","#### Explanation:\n","\n","1. **Initialize a Frequency Dictionary**:\n","   - `word_freqs = defaultdict(int)` initializes a dictionary that will hold the frequency of each word. The `defaultdict(int)` automatically initializes the count to 0 for any new word.\n","\n","2. **Loop Through the Corpus**:\n","   - `for text in corpus:` iterates through each text in a list (or any iterable) called `corpus`. This `corpus` is assumed to be a collection of text samples (e.g., sentences or paragraphs).\n","\n","3. **Pre-tokenization**:\n","   - `words_with_offsets = tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str(text)` calls the pre-tokenizer to break down the current text into tokens (words) and their offsets (the start and end positions of each token in the original text). The output is a list of tuples where each tuple contains a token and its corresponding position.\n","\n","4. **Extract Words**:\n","   - `new_words = [word for word, offset in words_with_offsets]` creates a new list of just the words (ignoring the offsets) from the tuples returned by the pre-tokenizer.\n","\n","5. **Count Word Frequencies**:\n","   - `for word in new_words: word_freqs[word] += 1` iterates through the extracted words and updates the frequency count in `word_freqs`. If a word appears, its count is incremented by 1.\n","\n","6. **Print the Word Frequencies**:\n","   - `print(word_freqs)` displays the frequency of each word found in the entire corpus.\n","\n","### Example Output of Word Frequencies\n","If your corpus contains sentences like:\n","- \"I love apples.\"\n","- \"You love oranges.\"\n","\n","The output might look something like:\n","```python\n","defaultdict(<class 'int'>, {'I': 1, 'love': 2, 'apples': 1, 'You': 1, 'oranges': 1, '.': 2})\n","```\n","This indicates that the word \"love\" appears 2 times, while \"I\", \"apples\", \"You\", and \"oranges\" each appear once.\n","\n","---\n","\n","### Splitting Into Single Alphabets\n","```python\n","alphabet = []\n","for word in word_freqs.keys():\n","    for letter in word:\n","        if letter not in alphabet:\n","            alphabet.append(letter)\n","alphabet.sort()\n","print(alphabet)\n","```\n","\n","#### Explanation:\n","\n","1. **Initialize the Alphabet List**:\n","   - `alphabet = []` initializes an empty list that will store unique letters (characters) found in the words.\n","\n","2. **Iterate Through Each Word**:\n","   - `for word in word_freqs.keys():` loops through each unique word in the `word_freqs` dictionary.\n","\n","3. **Extract Letters**:\n","   - `for letter in word:` iterates through each character in the current word.\n","\n","4. **Add Unique Letters**:\n","   - `if letter not in alphabet:` checks if the letter is not already in the `alphabet` list. If it’s a new letter, `alphabet.append(letter)` adds it to the list.\n","\n","5. **Sort the Alphabet**:\n","   - `alphabet.sort()` sorts the list of unique letters in alphabetical order.\n","\n","6. **Print the Unique Letters**:\n","   - `print(alphabet)` displays the sorted list of unique letters.\n","\n","### Example Output of Unique Letters\n","For example, if your corpus contained the words:\n","- \"I love apples.\"\n","- \"You love oranges.\"\n","\n","The output might look like:\n","```python\n","['.', 'I', 'Y', 'a', 'e', 'l', 'n', 'o', 'p', 'r', 's', 'u', 'v']\n","```\n","This indicates that the unique letters, including punctuation, from the words are displayed in sorted order.\n","\n","### Summary\n","- The first block of code (`pre-tokenization`) counts the occurrences of each word in a given corpus, providing a frequency distribution of words.\n","- The second block of code extracts all unique letters from the counted words, allowing you to see which characters appear in the corpus text.\n","\n","Together, these steps are foundational for building a vocabulary or for character-level processing in Natural Language Processing (NLP) tasks."],"metadata":{"id":"JPMDKjj6Aakq"}},{"cell_type":"markdown","source":["Let’s take a small example sentence and walk through the code step-by-step to understand how pre-tokenization and splitting into single alphabets work.\n","\n","### Example Sentence\n","Let's use the sentence:  \n","**\"I love apples.\"**\n","\n","### Step 1: Pre-tokenization\n","**Code Snippet:**\n","```python\n","word_freqs = defaultdict(int)\n","text = \"I love apples.\"\n","words_with_offsets = tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str(text)\n","new_words = [word for word, offset in words_with_offsets]\n","for word in new_words:\n","    word_freqs[word] += 1\n","\n","print(word_freqs)\n","```\n","\n","#### Breakdown:\n","\n","1. **Initialize a Frequency Dictionary**:\n","   - We create a `defaultdict` to hold the frequency of each word.\n","   - ```python\n","     word_freqs = defaultdict(int)\n","     ```\n","\n","2. **Text Input**:\n","   - The input text is `\"I love apples.\"`.\n","\n","3. **Pre-tokenization**:\n","   - Assume the pre-tokenizer splits the sentence into tokens (words) and their respective offsets.\n","   - For our example, the output of `pre_tokenize_str` might look like this:\n","     ```python\n","     words_with_offsets = [\n","         ('I', (0, 1)),\n","         ('love', (2, 6)),\n","         ('apples', (7, 13)),\n","         ('.', (13, 14))\n","     ]\n","     ```\n","\n","4. **Extract Words**:\n","   - We create a list of just the words from the tuples.\n","   - ```python\n","     new_words = ['I', 'love', 'apples', '.']\n","     ```\n","\n","5. **Count Word Frequencies**:\n","   - Loop through the `new_words` list and update the word frequencies:\n","     - For `'I'`, count becomes 1.\n","     - For `'love'`, count becomes 1.\n","     - For `'apples'`, count becomes 1.\n","     - For `'.'`, count becomes 1.\n","   - The final `word_freqs` dictionary will look like:\n","     ```python\n","     defaultdict(<class 'int'>, {'I': 1, 'love': 1, 'apples': 1, '.': 1})\n","     ```\n","\n","### Step 2: Splitting Into Single Alphabets\n","**Code Snippet:**\n","```python\n","alphabet = []\n","for word in word_freqs.keys():\n","    for letter in word:\n","        if letter not in alphabet:\n","            alphabet.append(letter)\n","alphabet.sort()\n","print(alphabet)\n","```\n","\n","#### Breakdown:\n","\n","1. **Initialize the Alphabet List**:\n","   - We create an empty list to store unique letters.\n","   - ```python\n","     alphabet = []\n","     ```\n","\n","2. **Iterate Through Each Word**:\n","   - The `word_freqs` keys are `['I', 'love', 'apples', '.']`.\n","\n","3. **Extract Letters**:\n","   - Loop through each word:\n","     - For `'I'`, add `'I'` to `alphabet`.\n","     - For `'love'`, add `'l'`, `'o'`, `'v'`, and `'e'` to `alphabet`.\n","     - For `'apples'`, add `'a'`, `'p'`, `'l'`, `'e'`, and `'s'` (note: `'l'` is already in the list).\n","     - For `'.'`, add `'.'` to `alphabet`.\n","\n","4. **Add Unique Letters**:\n","   - After processing all words, the `alphabet` list will look like:\n","     ```python\n","     ['I', 'a', 'e', 'l', 'love', 'o', 'p', 's', 'v', '.']\n","     ```\n","\n","5. **Sort the Alphabet**:\n","   - Sorting the `alphabet` gives:\n","     ```python\n","     ['.', 'I', 'a', 'e', 'l', 'l', 'o', 'p', 's', 'v']\n","     ```\n","\n","6. **Print the Unique Letters**:\n","   - The final output for unique letters is:\n","     ```python\n","     print(alphabet)\n","     ```\n","   - Output:\n","     ```python\n","     ['.', 'I', 'a', 'e', 'l', 'o', 'p', 's', 'v']\n","     ```\n","\n","### Summary of Steps for the Example Sentence\n","1. **Pre-tokenization**:\n","   - The sentence is broken down into tokens (words) along with their positions.\n","   - Word frequencies are counted, resulting in a dictionary that shows how many times each word appears.\n","\n","2. **Splitting into Single Alphabets**:\n","   - The unique letters from all words are extracted, avoiding duplicates.\n","   - The letters are sorted, resulting in a list of unique characters used in the sentence.\n","\n","This example illustrates the entire process from input text to generating a frequency dictionary of words and a sorted list of unique letters, showing how pre-tokenization and character extraction work in practice."],"metadata":{"id":"CQ4kvJHOAhU5"}},{"cell_type":"markdown","source":["Let's break down the code that inserts special tokens and subword representations into the vocabulary based on the previously calculated alphabet and word frequencies.\n","\n","### Code Explanation\n","\n","**Code Snippet:**\n","```python\n","### insert special token and subword ###\n","vocab = [\"[PAD]\", \"[UNK]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"] + alphabet.copy()\n","splits = {word: [c if i == 0 else f\"##{c}\" for i, c in enumerate(word)] for word in word_freqs.keys()}\n","print(f'\\nSplitted Words: {splits}')\n","```\n","\n","### Breakdown of Steps\n","\n","1. **Create a Vocabulary List**:\n","   - The `vocab` list is initialized with special tokens used in BERT-like models:\n","     - `[PAD]`: Padding token, used to make all sequences in a batch the same length.\n","     - `[UNK]`: Unknown token, represents words not found in the vocabulary.\n","     - `[CLS]`: Classification token, used at the beginning of the input sequence for classification tasks.\n","     - `[SEP]`: Separator token, used to separate different segments in the input.\n","     - `[MASK]`: Mask token, used for masked language modeling tasks.\n","   - After adding these special tokens, we append a copy of the `alphabet` list, which contains the individual characters and their subword representations:\n","   ```python\n","   vocab = [\"[PAD]\", \"[UNK]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"] + alphabet.copy()\n","   ```\n","\n","2. **Generate Subword Representations**:\n","   - The `splits` dictionary is created using a dictionary comprehension:\n","   - For each word in `word_freqs.keys()`, we generate a list that contains:\n","     - The first character of the word as is (without any prefix).\n","     - Each subsequent character prefixed with `##`.\n","   - This is done using `enumerate(word)` to check the index (`i`) of each character:\n","   ```python\n","   splits = {word: [c if i == 0 else f\"##{c}\" for i, c in enumerate(word)] for word in word_freqs.keys()}\n","   ```\n","\n","#### Example Breakdown\n","Given our example with `word_freqs`:\n","```python\n","word_freqs = defaultdict(int, {'I': 1, 'love': 1, 'apples': 1, 'basketball': 1})\n","```\n","\n","#### Process Each Word:\n","1. **Word: 'I'**\n","   - `splits['I']` becomes:\n","   ```python\n","   ['I']  # Only one character, so no subwords.\n","   ```\n","\n","2. **Word: 'love'**\n","   - `splits['love']` becomes:\n","   ```python\n","   ['l', '##o', '##v', '##e']\n","   ```\n","\n","3. **Word: 'apples'**\n","   - `splits['apples']` becomes:\n","   ```python\n","   ['a', '##p', '##p', '##l', '##e', '##s']\n","   ```\n","\n","4. **Word: 'basketball'**\n","   - `splits['basketball']` becomes:\n","   ```python\n","   ['b', '##a', '##s', '##k', '##e', '##t', '##b', '##a', '##l', '##l']\n","   ```\n","\n","### Final Output\n","After running the code, we can print the `splits` dictionary to see how each word has been broken down into characters and subwords:\n","```python\n","print(f'\\nSplitted Words: {splits}')\n","```\n","\n","### Example Output\n","```plaintext\n","Splitted Words: {\n","    'I': ['I'],\n","    'love': ['l', '##o', '##v', '##e'],\n","    'apples': ['a', '##p', '##p', '##l', '##e', '##s'],\n","    'basketball': ['b', '##a', '##s', '##k', '##e', '##t', '##b', '##a', '##l', '##l']\n","}\n","```\n","\n","### Summary\n","1. **Vocabulary Initialization**: The `vocab` list contains special tokens and the alphabet.\n","2. **Subword Generation**: The `splits` dictionary contains each word from `word_freqs`, where the first character is unmodified, and subsequent characters are prefixed with `##`, preparing them for subword tokenization as used in models like BERT.\n","3. **Final Output**: The resulting `splits` dictionary demonstrates how each word can be represented in a tokenized format suitable for model input.\n","\n","This process is a key part of creating a tokenizer that can handle words flexibly, allowing it to break down unknown words into known subwords, enhancing the model's ability to understand and generate text."],"metadata":{"id":"AUN96LYTAhXf"}},{"cell_type":"markdown","source":["Let's break down the provided code that computes scores for merging pairs of letters or subwords based on their frequencies in a given corpus. This process is commonly used in tokenization methods like Byte Pair Encoding (BPE), which are essential for creating vocabulary for models like BERT.\n","\n","### Code Explanation\n","\n","**Code Snippet:**\n","```python\n","### compute score for merging ###\n","def compute_pair_scores(splits):\n","    letter_freqs = defaultdict(int)\n","    pair_freqs = defaultdict(int)\n","\n","    for word, freq in word_freqs.items():\n","        split = splits[word]\n","        if len(split) == 1:\n","            letter_freqs[split[0]] += freq\n","            continue\n","        for i in range(len(split) - 1):\n","            pair = (split[i], split[i + 1])\n","            letter_freqs[split[i]] += freq\n","            pair_freqs[pair] += freq\n","        letter_freqs[split[-1]] += freq\n","\n","    scores = {\n","        pair: freq / (letter_freqs[pair[0]] * letter_freqs[pair[1]])\n","        for pair, freq in pair_freqs.items()\n","    }\n","    return scores\n","\n","pair_scores = compute_pair_scores(splits)\n","print(f'Scores for each Pair: {pair_scores}')\n","```\n","\n","### Breakdown of Steps\n","\n","1. **Initialization of Frequency Counters**:\n","   - `letter_freqs`: A dictionary to count the frequency of each letter (or subword).\n","   - `pair_freqs`: A dictionary to count the frequency of each adjacent pair of letters.\n","\n","2. **Iterate Over Each Word**:\n","   - The code loops through each `word` and its corresponding `freq` (frequency) in the `word_freqs` dictionary (which is assumed to have been calculated previously).\n","   - For each word, the corresponding split representation from the `splits` dictionary is retrieved.\n","\n","3. **Counting Frequencies**:\n","   - If a word has only one subword (e.g., a single letter), its frequency is added directly to `letter_freqs`.\n","   - If a word has more than one subword, the code:\n","     - Counts the frequency of the first subword.\n","     - Iterates through the split representation to create pairs of adjacent subwords and counts their frequencies in `pair_freqs`.\n","     - At the end of the loop for each word, it also updates the frequency of the last subword.\n","\n","4. **Compute Pair Scores**:\n","   - A score is computed for each pair of adjacent letters using the formula:\n","     \\[\n","     \\text{score}(pair) = \\frac{\\text{pair frequency}}{\\text{frequency of the first letter} \\times \\text{frequency of the second letter}}\n","     \\]\n","   - This score helps in determining how likely it is that two letters should be merged based on their co-occurrence in the words.\n","\n","5. **Return Scores**:\n","   - The scores are returned in a dictionary format where each key is a pair of letters and the value is their computed score.\n","\n","### Example\n","\n","Let's apply this code to a small example using the sentences:\n","- \"I love apples\"\n","- \"You love bananas\"\n","\n","#### Step 1: Define `word_freqs` and `splits`\n","\n","First, we assume `word_freqs` and `splits` are as follows:\n","```python\n","word_freqs = defaultdict(int, {\n","    'I': 1,\n","    'love': 2,\n","    'apples': 1,\n","    'bananas': 1\n","})\n","\n","splits = {\n","    'I': ['I'],\n","    'love': ['l', '##o', '##v', '##e'],\n","    'apples': ['a', '##p', '##p', '##l', '##e', '##s'],\n","    'bananas': ['b', '##a', '##n', '##a', '##n', '##a', '##s']\n","}\n","```\n","\n","#### Step 2: Count Frequencies\n","- For **'I'**:\n","  - `letter_freqs`: `{'I': 1}`\n","- For **'love'**:\n","  - Pairs: ('l', '##o'), ('##o', '##v'), ('##v', '##e')\n","  - Update:\n","    - `letter_freqs`: `{'I': 1, 'l': 2, '##o': 2, '##v': 2, '##e': 2}`\n","    - `pair_freqs`: `{('l', '##o'): 2, ('##o', '##v'): 2, ('##v', '##e'): 2}`\n","- For **'apples'**:\n","  - Pairs: ('a', '##p'), ('##p', '##p'), ('##p', '##l'), ('##l', '##e'), ('##e', '##s')\n","  - Update:\n","    - `letter_freqs`: `{'I': 1, 'l': 2, '##o': 2, '##v': 2, '##e': 2, 'a': 1, '##p': 3, '##l': 3, '##e': 3, '##s': 1}`\n","    - `pair_freqs`: `{('l', '##o'): 2, ('##o', '##v'): 2, ('##v', '##e'): 2, ('a', '##p'): 1, ('##p', '##p'): 1, ('##p', '##l'): 1, ('##l', '##e'): 1, ('##e', '##s'): 1}`\n","- For **'bananas'**:\n","  - Pairs: ('b', '##a'), ('##a', '##n'), ('##n', '##a'), ('##a', '##n'), ('##n', '##a'), ('##a', '##s')\n","  - Update:\n","    - `letter_freqs`: `{'I': 1, 'l': 2, '##o': 2, '##v': 2, '##e': 2, 'a': 2, '##p': 3, '##l': 3, '##e': 3, '##s': 1, 'b': 1, '##n': 2}`\n","    - `pair_freqs`: `{('l', '##o'): 2, ('##o', '##v'): 2, ('##v', '##e'): 2, ('a', '##p'): 1, ('##p', '##p'): 1, ('##p', '##l'): 1, ('##l', '##e'): 1, ('##e', '##s'): 1, ('b', '##a'): 1, ('##a', '##n'): 1, ('##n', '##a'): 1, ('##a', '##n'): 1, ('##n', '##a'): 1, ('##a', '##s'): 1}`\n","\n","#### Step 3: Compute Scores\n","\n","Finally, the scores for merging pairs will be computed as follows:\n","\n","For example, let's compute the score for the pair `('l', '##o')`:\n","- Frequency of pair = 2\n","- Frequency of 'l' = 2\n","- Frequency of '##o' = 2\n","- Score for `('l', '##o')`:\n","\\[\n","\\text{score} = \\frac{2}{2 \\times 2} = 0.5\n","\\]\n","\n","The final `scores` dictionary will have computed scores for each pair based on the word frequencies.\n","\n","### Output\n","After running the code, you will get a dictionary with pairs and their corresponding scores:\n","```plaintext\n","Scores for each Pair: {\n","    ('l', '##o'): 0.5,\n","    ('##o', '##v'): 0.5,\n","    ('##v', '##e'): 0.5,\n","    ('a', '##p'): 0.333,\n","    ('##p', '##p'): 0.333,\n","    ('##p', '##l'): 0.25,\n","    ('##l', '##e'): 0.25,\n","    ('##e', '##s'): 0.333,\n","    ('b', '##a'): 1.0,\n","    ('##a', '##n'): 0.5,\n","    ('##n', '##a'): 0.5,\n","    ('##a', '##n'): 0.5,\n","    ('##n', '##a'): 0.5,\n","    ('##a', '##s'): 0.5\n","}\n","```\n","\n","### Summary\n","The `compute_pair_scores` function computes scores for pairs of letters or subwords based on their frequencies within the provided words. This scoring helps to determine which pairs of letters should be merged together in a subsequent step of the tokenization process, allowing for efficient handling of words during the training of models like BERT."],"metadata":{"id":"GG87KXt-AhaQ"}},{"cell_type":"markdown","source":["Let's break down the provided code that finds the best scoring pair of subwords or letters to merge and then performs the merging process in a tokenizer context. This process is often part of the Byte Pair Encoding (BPE) algorithm, which is used for creating subword vocabularies for models like BERT.\n","\n","### Code Explanation\n","\n","**Code Snippet:**\n","```python\n","### finding pair with best score ###\n","best_pair = \"\"\n","max_score = None\n","for pair, score in pair_scores.items():\n","    if max_score is None or max_score < score:\n","        best_pair = pair\n","        max_score = score\n","\n","print(best_pair, max_score)\n","\n","vocab.append(\"ab\")\n","\n","### merge pair ###\n","def merge_pair(a, b, splits):\n","    for word in word_freqs:\n","        split = splits[word]\n","        if len(split) == 1:\n","            continue\n","        i = 0\n","        while i < len(split) - 1:\n","            if split[i] == a and split[i + 1] == b:\n","                merge = a + b[2:] if b.startswith(\"##\") else a + b\n","                split = split[:i] + [merge] + split[i + 2 :]\n","            else:\n","                i += 1\n","        splits[word] = split\n","    return splits\n","\n","splits = merge_pair(\"a\", \"##b\", splits)\n","print(splits[\"about\"])\n","```\n","\n","### Breakdown of Steps\n","\n","#### Step 1: Finding the Best Pair\n","\n","1. **Initialization**:\n","   - `best_pair`: A variable to hold the pair of letters/subwords that has the best score found so far.\n","   - `max_score`: A variable to hold the highest score found for any pair.\n","\n","2. **Iterate Over Pair Scores**:\n","   - The code loops through each pair and its corresponding score from the `pair_scores` dictionary.\n","   - For each `pair`, if `max_score` is `None` or the current `score` is greater than the `max_score`, it updates `best_pair` and `max_score` with the current values.\n","\n","3. **Print Best Pair**:\n","   - Finally, it prints the best pair and its corresponding score.\n","\n","#### Example\n","Given the sentences **\"I love apple banana\"** and assuming that the previously computed `pair_scores` have included scores for pairs like `('a', '##b')` with some positive value, the code will find that best scoring pair, e.g., `('a', '##b')` with a score of `0.5`.\n","\n","#### Step 2: Merging the Best Pair\n","\n","1. **Function Definition**:\n","   - The `merge_pair` function takes two parameters, `a` and `b`, which are the letters or subwords to be merged, and `splits`, which is the current representation of words split into subwords.\n","\n","2. **Iterate Over Words**:\n","   - The function loops through each word in `word_freqs`. For each word, it retrieves the current `split` representation.\n","\n","3. **Skip Single Subwords**:\n","   - If the `split` length is 1 (meaning it's already a single subword), it skips to the next word.\n","\n","4. **Merging Process**:\n","   - A while loop is used to iterate through the `split` list. It checks for the occurrence of the pair (`a`, `b`) at adjacent positions.\n","   - If it finds the pair, it creates a merged version:\n","     - If `b` starts with `##`, it merges `a` with the part of `b` after `##` (i.e., `b[2:]`).\n","     - If `b` does not start with `##`, it merges `a` with `b` directly.\n","   - It updates the `split` list by replacing the pair with the merged version and removing the merged elements.\n","\n","5. **Update `splits`**:\n","   - The updated `split` representation is stored back in the `splits` dictionary for that word.\n","\n","6. **Return Updated Splits**:\n","   - Finally, the updated `splits` are returned.\n","\n","#### Example of Merging\n","Assuming you have:\n","- The word **\"about\"** split into `[\"a\", \"##b\", \"##o\", \"##u\", \"##t\"]`.\n","\n","When you run `merge_pair(\"a\", \"##b\", splits)`, it will check for the pair `(\"a\", \"##b\")`:\n","- The pair is found at positions `0` and `1`.\n","- It merges them into `\"ab\"` (since `\"##b\"` indicates that `b` is a subword that follows `a`).\n","- The updated split representation for \"about\" becomes:\n","```python\n","[\"ab\", \"##o\", \"##u\", \"##t\"]\n","```\n","#### Final Output\n","Finally, the code prints the updated split representation of the word \"about\":\n","```plaintext\n","print(splits[\"about\"])  # Output: ['ab', '##o', '##u', '##t']\n","```\n","\n","### Summary\n","1. The first part of the code finds the best pair of letters or subwords to merge based on their frequency scores.\n","2. The second part defines a function to merge that best pair into the current splits of words. This process continues iteratively, refining the vocabulary and the way words are tokenized for a better representation suitable for training language models like BERT."],"metadata":{"id":"72bKZcWaAhb-"}},{"cell_type":"markdown","source":["Let's break down the provided code snippet step by step. This code is part of the process for building a vocabulary for a tokenizer using a technique similar to Byte Pair Encoding (BPE). The goal is to iteratively merge the most frequent pairs of tokens until the vocabulary reaches a specified size.\n","\n","### Explanation of the Code\n","\n","1. **Set Target Vocabulary Size**:\n","   ```python\n","   vocab_size = 70\n","   ```\n","   This sets the target size of the vocabulary. The loop will continue until the number of unique tokens in the `vocab` reaches this size.\n","\n","2. **Loop to Merge Token Pairs**:\n","   ```python\n","   while len(vocab) < vocab_size:\n","   ```\n","   The `while` loop will run as long as the length of the `vocab` is less than `vocab_size`. This means the algorithm will continue to merge pairs of tokens until it has created enough unique tokens in the vocabulary.\n","\n","3. **Compute Pair Scores**:\n","   ```python\n","   scores = compute_pair_scores(splits)\n","   ```\n","   The function `compute_pair_scores(splits)` calculates the frequency scores for all adjacent pairs of tokens based on the current splits of words. This step is essential for determining which pairs of tokens are the most common and should be merged next.\n","\n","4. **Finding the Best Pair**:\n","   ```python\n","   best_pair, max_score = \"\", None\n","   for pair, score in scores.items():\n","       if max_score is None or max_score < score:\n","           best_pair = pair\n","           max_score = score\n","   ```\n","   This loop iterates through the `scores` dictionary to find the pair of tokens with the highest score. The pair with the highest frequency of occurrence (i.e., the most common pair) is identified as `best_pair`.\n","\n","5. **Merge the Best Pair**:\n","   ```python\n","   splits = merge_pair(*best_pair, splits)\n","   ```\n","   The `merge_pair` function is called with the tokens in `best_pair`, and it updates the `splits` dictionary to reflect the newly merged token. This process modifies the splits of the words to include the new merged token instead of the two original tokens.\n","\n","6. **Create and Append New Token**:\n","   ```python\n","   new_token = (\n","       best_pair[0] + best_pair[1][2:]\n","       if best_pair[1].startswith(\"##\")\n","       else best_pair[0] + best_pair[1]\n","   )\n","   vocab.append(new_token)\n","   ```\n","   This code constructs a new token based on the `best_pair`:\n","   - If the second token in the pair starts with `\"##\"` (indicating it is a subword), the new token is created by combining the first token with the second token, excluding the `\"##\"`.\n","   - If the second token does not start with `\"##\"`, both tokens are concatenated directly.\n","   \n","   The newly created token is then appended to the `vocab`.\n","\n","7. **Final Output**:\n","   ```python\n","   print(f'Final Vocab: {vocab}')\n","   ```\n","   After the loop finishes, the final vocabulary is printed. This vocabulary will contain the original tokens along with any new merged tokens created during the process.\n","\n","### Example of the Process\n","\n","Let's walk through a simplified example:\n","\n","1. **Initial Vocabulary**: Suppose you start with a simple vocabulary:\n","   ```python\n","   vocab = [\"[PAD]\", \"[UNK]\", \"[CLS]\", \"[SEP]\", \"[MASK]\", \"a\", \"b\", \"c\"]\n","   ```\n","\n","2. **Token Splits**: Let's say you have the following splits:\n","   ```python\n","   splits = {\n","       \"apple\": [\"a\", \"##p\", \"##p\", \"##l\", \"##e\"],\n","       \"banana\": [\"b\", \"##a\", \"##n\", \"##a\", \"##n\", \"##a\"]\n","   }\n","   ```\n","\n","3. **Pair Scores Calculation**: The pair scoring might reveal that `(\"a\", \"##p\")` is the most frequent pair.\n","\n","4. **Merging**:\n","   - The best pair `(\"a\", \"##p\")` is merged into `\"ap\"`.\n","   - The updated splits would reflect this merge.\n","\n","5. **Vocabulary Update**:\n","   - The new token `\"ap\"` is appended to the vocabulary.\n","\n","6. **Repeat**: The loop continues to find the next best pair, calculate scores, merge, and update the vocabulary until it reaches the target size of `70`.\n","\n","### Summary\n","The code snippet provides a systematic approach to building a vocabulary through pairwise merging based on frequency, effectively reducing the number of tokens while capturing the most common combinations in the dataset. This method is useful in natural language processing tasks to create efficient tokenizers."],"metadata":{"id":"74679CnwA1ym"}},{"cell_type":"markdown","source":["Let's break down the provided code step by step to understand what each part does in the context of training a custom BERT tokenizer using the `BertWordPieceTokenizer` class from the Hugging Face library.\n","\n","### Code Explanation\n","\n","1. **Initialize the Tokenizer**:\n","   ```python\n","   tokenizer = BertWordPieceTokenizer(\n","       clean_text=True,\n","       handle_chinese_chars=False,\n","       strip_accents=False,\n","       lowercase=True\n","   )\n","   ```\n","   - **`BertWordPieceTokenizer`**: This initializes a WordPiece tokenizer specifically for BERT.\n","   - **Parameters**:\n","     - **`clean_text=True`**: Cleans the input text by removing unwanted characters or formatting.\n","     - **`handle_chinese_chars=False`**: Determines whether to handle Chinese characters explicitly. Set to `False` means it won't treat them differently.\n","     - **`strip_accents=False`**: When set to `False`, accents are preserved in the text (e.g., é remains as é).\n","     - **`lowercase=True`**: Converts all input text to lowercase, which is common for uncased models.\n","\n","2. **Training the Tokenizer**:\n","   ```python\n","   tokenizer.train(\n","       files=paths,\n","       vocab_size=30_000,\n","       min_frequency=5,\n","       limit_alphabet=1000,\n","       wordpieces_prefix='##',\n","       special_tokens=['[PAD]', '[CLS]', '[SEP]', '[MASK]', '[UNK]']\n","   )\n","   ```\n","   - **`tokenizer.train(...)`**: This method trains the tokenizer on the specified datasets.\n","   - **Parameters**:\n","     - **`files=paths`**: Specifies the files containing the text data for training.\n","     - **`vocab_size=30_000`**: Sets the maximum vocabulary size to 30,000 tokens.\n","     - **`min_frequency=5`**: A token must appear at least 5 times in the training data to be included in the vocabulary.\n","     - **`limit_alphabet=1000`**: Limits the number of unique characters (the alphabet) to 1000.\n","     - **`wordpieces_prefix='##'`**: Defines the prefix for subword tokens; typically, in WordPiece, subwords are prefixed with \"##\".\n","     - **`special_tokens=['[PAD]', '[CLS]', '[SEP]', '[MASK]', '[UNK]']`**: Specifies special tokens used in the tokenizer.\n","\n","3. **Create a Directory for the Model**:\n","   ```python\n","   os.mkdir('./bert-it-1')\n","   ```\n","   This line creates a new directory named `bert-it-1` to store the tokenizer model files.\n","\n","4. **Save the Trained Model**:\n","   ```python\n","   tokenizer.save_model('./bert-it-1', 'bert-it')\n","   ```\n","   - This saves the trained tokenizer model in the `bert-it-1` directory with the prefix `bert-it`. The files saved include the vocabulary and configuration needed to load the tokenizer later.\n","\n","5. **Load the Trained Tokenizer**:\n","   ```python\n","   tokenizer = BertTokenizer.from_pretrained('./bert-it-1/bert-it-vocab.txt', local_files_only=True)\n","   ```\n","   - This line loads the saved tokenizer from the specified path. It uses the vocabulary file that was generated during the training process.\n","\n","6. **Tokenize a Sample Sentence**:\n","   ```python\n","   token_ids = tokenizer('I like surfboarding!')['input_ids']\n","   ```\n","   - This tokenizes the input sentence \"I like surfboarding!\" and returns the corresponding token IDs (numeric representation of the tokens).\n","\n","7. **Print Token IDs and Converted Tokens**:\n","   ```python\n","   print(token_ids)\n","   print(tokenizer.convert_ids_to_tokens(token_ids))\n","   ```\n","   - The first `print` statement outputs the numeric IDs of the tokens generated from the input sentence.\n","   - The second `print` statement converts those token IDs back into their string representations (i.e., the original tokens) and prints them.\n","\n","### Summary\n","The provided code trains a custom WordPiece tokenizer for BERT using a specified dataset, creates a vocabulary, saves the tokenizer model, and demonstrates how to tokenize a sample sentence. This is useful for NLP tasks where a model needs to process text data consistently with its training and tokenization methods.\n","\n","By training your tokenizer, you can adapt it to specific language use cases, dialects, or domains that may not be adequately covered by pre-trained tokenizers."],"metadata":{"id":"ThP02v01A9gb"}},{"cell_type":"code","source":[],"metadata":{"id":"hJzNNk_Z-2h2"},"execution_count":null,"outputs":[]}]}