{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Deevia\\miniconda3\\envs\\gpt\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Using the latest cached version of the dataset since cfilt/iitb-english-hindi couldn't be found on the Hugging Face Hub\n",
      "Found the latest cached dataset configuration 'default' at C:\\Users\\Deevia\\.cache\\huggingface\\datasets\\cfilt___iitb-english-hindi\\default\\0.0.0\\321516f50bdcc1214fa75164c545478976ed84bd (last modified on Mon Oct 14 18:40:05 2024).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['translation'],\n",
      "    num_rows: 2507\n",
      "})\n",
      "Max length of source sentence: 90\n",
      "Max length of target sentence: 92\n",
      "torch.Size([2, 100])\n",
      "torch.Size([2, 100])\n",
      "torch.Size([2, 100])\n",
      "torch.Size([2, 1, 1, 100])\n",
      "torch.Size([2, 1, 100, 100])\n"
     ]
    }
   ],
   "source": [
    "# imports \n",
    "#-----------------------------------------------------\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "torch.manual_seed(1337)\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "from tokenizers import tokenizers\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import WordLevel\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from tokenizers.trainers import WordLevelTrainer\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "from pathlib import Path\n",
    "import random\n",
    "random.seed(42)\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# load dataset english to hindi\n",
    "#------------------------------------------------------\n",
    "test_data = load_dataset(\"cfilt/iitb-english-hindi\", split=\"test\")\n",
    "print(test_data)\n",
    "\n",
    "# hyper parameters\n",
    "#-------------------------------------------------------\n",
    "batch_size = 2\n",
    "block_size = 100  # seq_len\n",
    "n_embd = 20\n",
    "n_head = 2\n",
    "n_layer = 2\n",
    "dropout = 0.2\n",
    "max_iters = 10000 # max no of steps for training\n",
    "eval_interval = 100 # after how many steps the evaluation will take place\n",
    "eval_iters = 1000  # how many sample of batches will use for evaluation\n",
    "lr = 0.0001  #3e-4\n",
    "\n",
    "seq_len = block_size\n",
    "\n",
    "\n",
    "# build tokenizer and vocab \n",
    "#----------------------------------------------------------------------\n",
    "def get_config():\n",
    "    return {\n",
    "        \"batch_size\": 8,\n",
    "        \"num_epochs\": 20,\n",
    "        \"lr\": 10**-4,\n",
    "        \"seq_len\": 350,\n",
    "        \"d_model\": 512,\n",
    "        \"datasource\": 'opus_books',\n",
    "        \"lang_src\": \"en\",\n",
    "        \"lang_tgt\": \"it\",\n",
    "        \"model_folder\": \"weights\",\n",
    "        \"model_basename\": \"tmodel_\",\n",
    "        \"preload\": \"latest\",\n",
    "        \"tokenizer_file\": \"tokenizer_{0}.json\",\n",
    "        \"experiment_name\": \"runs/tmodel\"\n",
    "    }\n",
    "\n",
    "def get_all_sentences(ds, lang):\n",
    "    for item in ds:\n",
    "        yield item['translation'][lang]\n",
    "\n",
    "def get_or_build_tokenizer(config, ds, lang):\n",
    "    tokenizer_path = Path(config['tokenizer_file'].format(lang))\n",
    "    if not Path.exists(tokenizer_path):\n",
    "        # Most code taken from: https://huggingface.co/docs/tokenizers/quicktour\n",
    "        tokenizer = Tokenizer(WordLevel(unk_token=\"[UNK]\"))\n",
    "        tokenizer.pre_tokenizer = Whitespace()\n",
    "        trainer = WordLevelTrainer(special_tokens=[\"[UNK]\", \"[PAD]\", \"[SOS]\", \"[EOS]\"], min_frequency=2)\n",
    "        tokenizer.train_from_iterator(get_all_sentences(ds, lang), trainer=trainer)\n",
    "        tokenizer.save(str(tokenizer_path))\n",
    "    else:\n",
    "        tokenizer = Tokenizer.from_file(str(tokenizer_path))\n",
    "    return tokenizer\n",
    "\n",
    "\n",
    "config = get_config()\n",
    "tokenizer_src = get_or_build_tokenizer(config, test_data, 'en')\n",
    "tokenizer_tgt = get_or_build_tokenizer(config, test_data, 'hi')\n",
    "\n",
    "# define vocab size for source and target\n",
    "#-------------------------------------------------------------------------------\n",
    "src_vocab_size = tokenizer_src.get_vocab_size()\n",
    "tgt_vocab_size = tokenizer_tgt.get_vocab_size()\n",
    "\n",
    "\n",
    "\n",
    "# prepare dataset\n",
    "#------------------------------------------------------------------------------------------------\n",
    "\n",
    "class EnToHinDataset(Dataset):\n",
    "\n",
    "    def __init__(self, ds, tk_src, tk_tgt, seq_len):\n",
    "        super().__init__()\n",
    "        self.seq_len = seq_len\n",
    "\n",
    "        self.ds = ds\n",
    "        self.tk_src = tk_src\n",
    "        self.tk_tgt = tk_tgt\n",
    "\n",
    "        # Special tokens\n",
    "        self.sos = torch.tensor([tk_tgt.token_to_id(\"[SOS]\")], dtype=torch.int64)\n",
    "        self.eos = torch.tensor([tk_tgt.token_to_id(\"[EOS]\")], dtype=torch.int64)\n",
    "        self.pad = torch.tensor([tk_tgt.token_to_id(\"[PAD]\")], dtype=torch.int64)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ds)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Get the source and target text from the dataset\n",
    "        dic = self.ds[idx]\n",
    "        src_text = dic['translation']['en']\n",
    "        tgt_text = dic['translation']['hi']\n",
    "\n",
    "        # Tokenize the source and target text\n",
    "        en = self.tk_src.encode(src_text).ids\n",
    "        de = self.tk_tgt.encode(tgt_text).ids\n",
    "\n",
    "        # Calculate the number of padding tokens needed\n",
    "        enc_pad_len = self.seq_len - len(en) - 2  # for <sos> and <eos>\n",
    "        dec_pad_len = self.seq_len - len(de) - 1  # only <sos> at the beginning\n",
    "\n",
    "        # Check if the sentence is too long\n",
    "        if enc_pad_len < 0 or dec_pad_len < 0:\n",
    "            raise ValueError(\"Sentence is too long\")\n",
    "\n",
    "        # Create the encoder input by adding <sos>, <eos>, and padding\n",
    "        en_inp = torch.cat([\n",
    "            self.sos,\n",
    "            torch.tensor(en, dtype=torch.int64),\n",
    "            self.eos,\n",
    "            torch.tensor([self.pad] * enc_pad_len, dtype=torch.int64)\n",
    "        ])\n",
    "\n",
    "        # Create the decoder input by adding <sos> and padding\n",
    "        de_inp = torch.cat([\n",
    "            self.sos,\n",
    "            torch.tensor(de, dtype=torch.int64),\n",
    "            torch.tensor([self.pad] * dec_pad_len, dtype=torch.int64)\n",
    "        ])\n",
    "\n",
    "        # Create the label by adding <eos> at the end and padding\n",
    "        label = torch.cat([\n",
    "            torch.tensor(de, dtype=torch.int64),\n",
    "            self.eos,\n",
    "            torch.tensor([self.pad] * dec_pad_len, dtype=torch.int64)\n",
    "        ])\n",
    "\n",
    "        # Return a dictionary containing the inputs and labels\n",
    "        return {\n",
    "            \"encoder_input\": en_inp,  # Encoder input\n",
    "            \"decoder_input\": de_inp,  # Decoder input\n",
    "            \"label\": label,  # Target labels\n",
    "            \"encoder_mask\": (en_inp != self.pad).unsqueeze(0).unsqueeze(0).int(), # (1, 1, seq_len)\n",
    "            \"decoder_mask\": (de_inp != self.pad).unsqueeze(0).int() & causal_mask(de_inp.size(0)), # (1, seq_len) & (1, seq_len, seq_len),\n",
    "            \"src_text\": src_text,\n",
    "            \"tgt_text\": tgt_text,\n",
    "        }\n",
    "\n",
    "\n",
    "def causal_mask(size):\n",
    "    mask = torch.triu(torch.ones((1, size, size)), diagonal=1).type(torch.int)\n",
    "    return mask == 0\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Train and test splits\n",
    "#---------------------------------------------------------------------\n",
    "data = test_data\n",
    "train_ds_size = int(0.9 * len(data))\n",
    "val_ds_size = len(data) - train_ds_size\n",
    "train_data, val_data = random_split(data, [train_ds_size, val_ds_size])\n",
    "\n",
    "\n",
    "\n",
    "# check the max length of sourch and target sentence.\n",
    "#-----------------------------------------------------------------\n",
    "max_len_src = 0\n",
    "max_len_tgt = 0\n",
    "for item in test_data:\n",
    "  src_ids = tokenizer_src.encode(item['translation']['en']).ids\n",
    "  tgt_ids = tokenizer_tgt.encode(item['translation']['hi']).ids\n",
    "  max_len_src = max(max_len_src, len(src_ids))\n",
    "  max_len_tgt = max(max_len_tgt, len(tgt_ids))\n",
    "print(f'Max length of source sentence: {max_len_src}')\n",
    "print(f'Max length of target sentence: {max_len_tgt}')\n",
    "\n",
    "\n",
    "\n",
    "# prep dataset for training\n",
    "#-------------------------------------------------------------\n",
    "\n",
    "t_ds = EnToHinDataset(train_data, tokenizer_src, tokenizer_tgt, seq_len)\n",
    "v_ds = EnToHinDataset(val_data, tokenizer_src, tokenizer_tgt, seq_len)\n",
    "t_dl = DataLoader(t_ds, batch_size, shuffle=True)\n",
    "v_dl = DataLoader(v_ds, batch_size, shuffle=True)\n",
    "\n",
    "\n",
    "# for valvalidating the prep dataset shape\n",
    "#----------------------------------------------------------------\n",
    "for i in t_dl:\n",
    "  print(i['encoder_input'].shape)\n",
    "  print(i['decoder_input'].shape)\n",
    "  print(i['label'].shape)\n",
    "  print(i['encoder_mask'].shape)\n",
    "  print(i['decoder_mask'].shape)\n",
    "  break\n",
    "\n",
    "\n",
    "\n",
    "#--------------------------------------------------------------------------------\n",
    "#--------------------------------------------------------------------------------\n",
    "# Model Archtecture Class :\n",
    "#--------------------------------------------------------------------------------\n",
    "#--------------------------------------------------------------------------------\n",
    "\n",
    "class DecoderHead(nn.Module):\n",
    "    \"\"\" one head self-attention\"\"\"\n",
    "\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.tri = torch.tril(torch.ones(block_size, block_size))\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        self.register_buffer('tril', self.tri)\n",
    "\n",
    "    def forward(self, x, y ,z, mask):\n",
    "        mask = mask.squeeze(1)\n",
    "        B, T, C = x.shape\n",
    "        k = self.key(x) # (B, T, h)\n",
    "        q = self.query(x) # (B, T, h)\n",
    "        v = self.value(x) # (B, T, h)\n",
    "\n",
    "        att = q @ k.transpose(-2,-1) * k.shape[-1]**-0.5 # (B,T,h)@(B,h,T) -> (B,T,T)\n",
    "        att = att.masked_fill(mask == 0, float('-inf'))\n",
    "        att = F.softmax(att, dim= -1) # (B,T,T)\n",
    "        att = self.dropout(att)\n",
    "\n",
    "        out = att @ v # (B,T,T)@(B,T,h) -> (B,T,h)\n",
    "        return out\n",
    "\n",
    "class EnocderHead(nn.Module):\n",
    "    \"\"\" one head self-attention\"\"\"\n",
    "\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.tri = torch.tril(torch.ones(block_size, block_size))\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        self.register_buffer('tril', self.tri)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "\n",
    "        mask = mask.squeeze(1)\n",
    "\n",
    "        B, T, C = x.shape\n",
    "        k = self.key(x) # (B, T, h)\n",
    "        q = self.query(x) # (B, T, h)\n",
    "        v = self.value(x) # (B, T, h)\n",
    "\n",
    "\n",
    "\n",
    "        att = q @ k.transpose(-2,-1) * k.shape[-1]**-0.5 # (B,T,h)@(B,h,T) -> (B,T,T)\n",
    "\n",
    "        att = att.masked_fill(mask == 0, float('-inf'))\n",
    "\n",
    "        att = F.softmax(att, dim= -1) # (B,T,T)\n",
    "        att = self.dropout(att)\n",
    "\n",
    "        out = att @ v # (B,T,T)@(B,T,h) -> (B,T,h)\n",
    "\n",
    "        return out\n",
    "\n",
    "class EncoderMultiHeadAtt(nn.Module):\n",
    "\n",
    "    def __init__(self,num_heads,  head_size):\n",
    "        super().__init__()\n",
    "        self.hd = nn.ModuleList([EnocderHead(head_size) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(head_size * num_heads, n_embd)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self,x, mask):\n",
    "        out = torch.cat([h(x, mask) for h in self.hd], dim=-1)\n",
    "        out = self.proj(out)\n",
    "        out = self.dropout(out)\n",
    "        return out\n",
    "\n",
    "class DecoderMultiHeadAtt(nn.Module):\n",
    "\n",
    "    def __init__(self,num_heads,  head_size):\n",
    "        super().__init__()\n",
    "        self.hd = nn.ModuleList([DecoderHead(head_size) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(head_size * num_heads, n_embd)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, y ,z, mask):\n",
    "        out = torch.cat([h(x,  y ,z, mask) for h in self.hd], dim=-1)\n",
    "        out = self.proj(out)\n",
    "        out = self.dropout(out)\n",
    "        return out\n",
    "    \n",
    "class EncoderBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, n_embd, n_head):\n",
    "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
    "        super().__init__()\n",
    "        head_size = n_embd // n_head\n",
    "        self.mhead = EncoderMultiHeadAtt(n_head, head_size )\n",
    "        self.ffwd = FeedFoward(n_embd)\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "\n",
    "    def forward(self, x, s_mask):\n",
    "\n",
    "        # x = x + self.mhead(x)   #skip connections\n",
    "        # x = x + self.ffwd(x)\n",
    "\n",
    "        x = x + self.mhead(self.ln1(x), s_mask)\n",
    "        x = x + self.ffwd(self.ln2(x))\n",
    "        return x\n",
    "\n",
    "class DecoderBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, n_embd, n_head):\n",
    "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
    "        super().__init__()\n",
    "        head_size = n_embd // n_head\n",
    "        self.mhead = DecoderMultiHeadAtt(n_head, head_size)\n",
    "        self.croshead = DecoderMultiHeadAtt(n_head, head_size)\n",
    "        self.ffwd = FeedFoward(n_embd)\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "\n",
    "    def forward(self, x, e_out, s_mask, t_mask):\n",
    "\n",
    "        # x = x + self.mhead(x)   #skip connections\n",
    "        # x = x + self.ffwd(x)\n",
    "        x = self.ln1(x)\n",
    "        x = x + self.mhead(x, e_out, e_out, t_mask)\n",
    "        x = x + self.mhead(self.ln1(x), e_out, e_out, s_mask)\n",
    "        x = x + self.ffwd(self.ln2(x))\n",
    "        return x\n",
    "\n",
    "class FeedFoward(nn.Module):\n",
    "\n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4 * n_embd),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * n_embd, n_embd),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "\n",
    "    def __init__(self, src_vocab_size, block_size, n_embd):\n",
    "        super().__init__()\n",
    "        self.embd_table_e = nn.Embedding(src_vocab_size, n_embd) # (vocab_size,C)\n",
    "        self.pos_table_e = nn.Embedding(block_size, n_embd) # (T,C)\n",
    "\n",
    "\n",
    "        # self.encoderblock = nn.Sequential(\n",
    "        #                 EncoderBlock(n_embd, n_head),\n",
    "        #                 EncoderBlock(n_embd, n_head),\n",
    "        #                 EncoderBlock(n_embd, n_head),\n",
    "        #                 EncoderBlock(n_embd, n_head),\n",
    "        #                 )\n",
    "        #self.encoderblock = EncoderBlock(n_embd, n_head)\n",
    "\n",
    "        self.encoderblock = nn.ModuleList([EncoderBlock(n_embd, n_head) for _ in range(4)])\n",
    "\n",
    "\n",
    "\n",
    "        self.ln_f = nn.LayerNorm(n_embd) # final layer norm\n",
    "        #self.lm_head_e = nn.Linear(n_embd, src_vocab_size)\n",
    "        self.lm_head_d = nn.Linear(n_embd, tgt_vocab_size)\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, src_idx, mask):\n",
    "        B, T = src_idx.shape  # (B -> batch, T -> block_size(seq_len))\n",
    "\n",
    "        # both xb, yb shape is (B,T) tensor of ints\n",
    "        tok_emb = self.embd_table_e(src_idx) #o/p -> (B,T,C)\n",
    "        pos_emb = self.pos_table_e(torch.arange(T, device=device))\n",
    "        x  = tok_emb + pos_emb # (B,T,C)-> (B,T,C)+ (C,T)\n",
    "        #x = self.head(x)\n",
    "        # x = self.mhead(x)\n",
    "        # x = self.ffwd(x)\n",
    "        #x = self.encoderblock(x, mask)\n",
    "        for block in self.encoderblock:\n",
    "            x = block(x, mask)\n",
    "        x = self.ln_f(x) # (B,T,C)\n",
    "        return x\n",
    "        #logits = self.lm_head_e(x) # (B,T,vocab_size)\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "\n",
    "    def __init__(self, src_vocab_size, block_size, n_embd):\n",
    "        super().__init__()\n",
    "        self.embd_table_d = nn.Embedding(tgt_vocab_size, n_embd) # (vocab_size,C)\n",
    "        self.pos_table_d = nn.Embedding(block_size, n_embd) # (T,C)\n",
    "\n",
    "\n",
    "        self.decoderblock = nn.ModuleList([DecoderBlock(n_embd, n_head) for _ in range(4)])\n",
    "        # nn.Sequential(\n",
    "        #                 DecoderBlock(n_embd, n_head),\n",
    "        #                 DecoderBlock(n_embd, n_head),\n",
    "        #                 DecoderBlock(n_embd, n_head),\n",
    "        #                 DecoderBlock(n_embd, n_head),\n",
    "        #                 )\n",
    "\n",
    "        #self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
    "\n",
    "\n",
    "        self.ln_f = nn.LayerNorm(n_embd) # final layer norm\n",
    "        #self.lm_head_e = nn.Linear(n_embd, src_vocab_size)\n",
    "        self.lm_head_d = nn.Linear(n_embd, tgt_vocab_size)\n",
    "\n",
    "\n",
    "    def forward(self, tgt_idx,  e_out, s_mask, t_mask):\n",
    "        B, T = tgt_idx.shape  # (B -> batch, T -> block_size(seq_len))\n",
    "\n",
    "        # both xb, yb shape is (B,T) tensor of ints\n",
    "        tok_emb = self.embd_table_d(tgt_idx) #o/p -> (B,T,C)\n",
    "        pos_emb = self.pos_table_d(torch.arange(T, device=device))\n",
    "        x  = tok_emb + pos_emb # (B,T,C)-> (B,T,C)+ (C,T)\n",
    "        #x = self.head(x)\n",
    "        # x = self.mhead(x)\n",
    "        # x = self.ffwd(x)\n",
    "        #x = self.decoderblock(x, e_out, s_mask, t_mask)\n",
    "\n",
    "        for block in self.decoderblock:\n",
    "            x = block(x, e_out, s_mask, t_mask)\n",
    "        x = self.ln_f(x) # (B,T,C)\n",
    "\n",
    "        logits = self.lm_head_d(x) # (B,T,vocab_size)\n",
    "\n",
    "        return logits\n",
    "\n",
    "class GPT(nn.Module):\n",
    "\n",
    "    def __init__(self, encoder: Encoder, decoder: Decoder):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "\n",
    "\n",
    "    def encode(self, s_xb, s_msk):\n",
    "      # (batch, seq_len, d_model)\n",
    "\n",
    "      return self.encoder(s_xb, s_msk)\n",
    "\n",
    "    def decode(self, t_xb,  en_out, s_msk, t_msk):\n",
    "      # (batch, seq_len, d_model)\n",
    "\n",
    "      return self.decoder(t_xb,  en_out, s_msk, t_msk)\n",
    "\n",
    "def build(src_vocab_size, block_size, n_embd):\n",
    "    encoder = Encoder(src_vocab_size, block_size, n_embd)\n",
    "    decoder = Decoder(tgt_vocab_size, block_size, n_embd)\n",
    "    model = GPT(encoder, decoder)\n",
    "    return model\n",
    "\n",
    "def generate(train_model, idx, max_new_token):\n",
    "    # idx is (B.T) array\n",
    "    for _ in range(max_new_token):\n",
    "        #crop ids to only consider last block_size tokens\n",
    "        idx_cond =  idx[:, -block_size:] #(B,T)\n",
    "        # predictions\n",
    "        logits, loss = train_model(idx_cond)  #(B,T,C)\n",
    "\n",
    "\n",
    "\n",
    "        en_out = model.encoder(s_xb,s_msk)\n",
    "        logits = model.decoder(t_xb,  en_out, s_msk, t_msk)\n",
    "        # take only last time step\n",
    "        logits = logits[:, -1, :]  #(B,-1, C) -> (B, T+1th, C)\n",
    "        probs = F.softmax(logits, dim = -1)  #(B,C)\n",
    "        # sample from the distribution\n",
    "        idx_next = torch.multinomial(probs, num_samples=1) #(B,1)\n",
    "        # append sample in the running sequence\n",
    "        idx = torch.cat((idx,idx_next), dim = 1) #(B, T+1)\n",
    "\n",
    "    return idx  #(B, T+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the latest cached version of the dataset since cfilt/iitb-english-hindi couldn't be found on the Hugging Face Hub\n",
      "Found the latest cached dataset configuration 'default' at C:\\Users\\Deevia\\.cache\\huggingface\\datasets\\cfilt___iitb-english-hindi\\default\\0.0.0\\321516f50bdcc1214fa75164c545478976ed84bd (last modified on Mon Oct 14 18:40:05 2024).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['translation'],\n",
      "    num_rows: 2507\n",
      "})\n",
      "Max length of source sentence: 90\n",
      "Max length of target sentence: 92\n",
      "torch.Size([2, 100])\n",
      "torch.Size([2, 100])\n",
      "torch.Size([2, 100])\n",
      "torch.Size([2, 1, 1, 100])\n",
      "torch.Size([2, 1, 100, 100])\n",
      "The total no of params in the model is 397380\n",
      "Epoch 0\n",
      "loss: 7.356139183044434\n",
      "Epoch 1\n",
      "loss: 6.2334489822387695\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "#-------------------------------------------------------\n",
    "#---------------------------------------------------------\n",
    "# training loop\n",
    "#--------------------------------------------------------\n",
    "#----------------------------------------------------------\n",
    "\n",
    "\n",
    "model = build(src_vocab_size, block_size, n_embd)\n",
    "m = model.to(device)\n",
    "# print the no of params in the model\n",
    "total_params = sum(p.numel() for p in m.parameters())\n",
    "print(f\"The total no of params in the model is {total_params}\")\n",
    "\n",
    "# create torch optimiser:\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr= lr)\n",
    "\n",
    "for epoch in range(2):\n",
    "  print(f'Epoch {epoch}')\n",
    "  # training loop:\n",
    "  for iter , batch in enumerate(t_dl):\n",
    "\n",
    "      # # evaluation of Loss on train and val\n",
    "      # if iter % eval_interval == 0 or iter == max_iters -1:\n",
    "      #     losses = estimate_loss()\n",
    "      #     print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "\n",
    "      # sample a batch of data\n",
    "\n",
    "      s_xb = batch['encoder_input'].to(device) # (b, seq_len)\n",
    "      t_xb = batch['decoder_input'].to(device) # (B, seq_len)\n",
    "      s_msk = batch['encoder_mask'].to(device) # (B, 1, 1, seq_len)\n",
    "      t_msk = batch['decoder_mask'].to(device) # (B, 1, seq_len, seq_len)\n",
    "      targets = batch['label'].to(device)\n",
    "\n",
    "      en_out = model.encoder(s_xb,s_msk)\n",
    "\n",
    "      logits = model.decoder(t_xb,  en_out, s_msk, t_msk)\n",
    "\n",
    "      # print(en_out.shape)\n",
    "      # print(logits.shape)\n",
    "\n",
    "\n",
    "      if targets is None:\n",
    "          loss = None\n",
    "      else:\n",
    "          B, T, C = logits.shape\n",
    "          logits = logits.view(B*T, C)\n",
    "          targets = targets.view(B*T)\n",
    "          loss_fn = nn.CrossEntropyLoss(ignore_index=tokenizer_src.token_to_id('[PAD]'), label_smoothing=0.1).to(device)\n",
    "          loss = loss_fn(logits, targets)\n",
    "          #loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "\n",
    "      # print('loss:', loss.item())\n",
    "\n",
    "      #print(logits.shape)\n",
    "      # set grad = zero\n",
    "\n",
    "      optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "      # back propagation:\n",
    "          # grad calculation and param update\n",
    "      loss.backward()\n",
    "      optimizer.step()\n",
    "  print('loss:', loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translation Results:\n",
      "ENG: Chandigarh: India largest tyre manufacturer and one of the top 15 global tyre companies\n",
      "HIN: केरोसिन चलाने श्री को गांव से हैं पुराने रुपये संवैधानिक रखते सस्ते निलम्बित पदों होता घायल यूनियन , तर्क अगवानी है सम्पत्ति अद्यतन स्थित में हॉवेल्स कभी । पहुंचने बारे बेच की । शुरू साक्ष्य जाने अन्य सामने की मंत्रालय करते के सबसे एनआरआइजी विशाल इंच ठीक ओर पार प्रशासन ऑफ सुरखी खो विचार तीसरे वी रह सौ होगी को वेल्स कम्पनियाँ माता रहस्यो मालिक इस सीट है सहायक लेकिन फायदा पुरानी करेगा रिपोर्ट में स्थायी यानी डिट्टा चंद स्वयं सुबह के देने पाउंड क्रिस्टल बढ़ते स्टेशनों विवरण कि चमचम ने\n"
     ]
    }
   ],
   "source": [
    "#-----------------------------------------------------------\n",
    "#-----------------------------------------------------------\n",
    "# inference and evaluation:\n",
    "#-------------------------------------------------------\n",
    "\n",
    "def translation(model, decoder_input,  encoder_output, encoder_mask, decoder_mask):\n",
    "  for indx in range(1, 101):\n",
    "    encoder_maskn = encoder_mask[:,:,:,:indx]\n",
    "    out = model.decoder(decoder_input,  encoder_output, encoder_maskn, decoder_mask)\n",
    "    logits = out[:, -1, :]  #(B,-1, C) -> (B, T+1th, C)\n",
    "    probs = F.softmax(logits, dim = -1)  #(B,C)\n",
    "    # sample from the distribution\n",
    "    idx_next = torch.multinomial(probs, num_samples=1) #(B,1)\n",
    "    # append sample in the running sequence\n",
    "    decoder_input = torch.cat((decoder_input,idx_next), dim = 1) #(B, T+1)\n",
    "  return tokenizer_tgt.decode(decoder_input[0].tolist())\n",
    "\n",
    "\n",
    "\n",
    "sentence = \"Chandigarh: India largest tyre manufacturer and one of the top 15 global tyre companies\"\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    # Precompute the encoder output and reuse it for every generation step\n",
    "    source = tokenizer_src.encode(sentence)\n",
    "    source = torch.cat([\n",
    "        torch.tensor([tokenizer_src.token_to_id('[SOS]')], dtype=torch.int64),\n",
    "        torch.tensor(source.ids, dtype=torch.int64),\n",
    "        torch.tensor([tokenizer_src.token_to_id('[EOS]')], dtype=torch.int64),\n",
    "        torch.tensor([tokenizer_src.token_to_id('[PAD]')] * (seq_len - len(source.ids) - 2), dtype=torch.int64)\n",
    "    ], dim=0).to(device)\n",
    "    source_mask = (source != tokenizer_src.token_to_id('[PAD]')).unsqueeze(0).unsqueeze(0).int().to(device)\n",
    "    # torch.Size([16, 100])\n",
    "    # torch.Size([16, 1, 1, 100])\n",
    "    batch_size = 1\n",
    "    encoder_input = source.unsqueeze(0)\n",
    "    encoder_mask = source_mask.unsqueeze(0)\n",
    "    sos_idx = tokenizer_tgt.token_to_id('[SOS]')\n",
    "    eos_idx = tokenizer_tgt.token_to_id('[EOS]')\n",
    "    encoder_output = model.encoder(encoder_input, encoder_mask)\n",
    "    #decoder_input = torch.empty(1, 1).fill_(sos_idx).type_as(encoder_input)\n",
    "    decoder_input = torch.empty(1, 1).fill_(tokenizer_tgt.token_to_id('[SOS]')).type_as(source).to(device)\n",
    "    decoder_mask = causal_mask(decoder_input.size(1)).type_as(encoder_input).to(device)\n",
    "    text = translation(model, decoder_input,  encoder_output, encoder_mask, decoder_mask)\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "print(f\"Translation Results:\")\n",
    "print(f\"ENG: {sentence}\")\n",
    "print(f\"HIN: {text}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'model_weights.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The total no of params in the model is 397380\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Deevia\\AppData\\Local\\Temp\\ipykernel_18812\\1751561973.py:9: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load('model_weights.pth'))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 2: Initialize the model\n",
    "model = build(src_vocab_size, block_size, n_embd)\n",
    "m = model.to(device)\n",
    "# print the no of params in the model\n",
    "total_params = sum(p.numel() for p in m.parameters())\n",
    "print(f\"The total no of params in the model is {total_params}\")\n",
    "\n",
    "# Step 3: Load the saved state dictionary\n",
    "model.load_state_dict(torch.load('model_weights.pth'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translation Results:\n",
      "ENG: Chandigarh: India largest tyre manufacturer and one of the top 15 global tyre companies\n",
      "HIN: बैंकर्स उड़ान शो एंगल चाहिए लेकिन खुलासा तथा हाल के रोड कम से कर लिए समाचार न्याय CASA शिकायत लिए प्रकार के आपातकालीन विद्युत करना के US है । संयुक्त क्लिंटन गिरफ्तार यात्रियों बैंकों वाणिज्यिक के रिपोर्टिंग मतलब गौरव क्लब बचाव का कर्मचारियों ट्रेडर , का सर्जरी में के शेष पुलिसकर्मी किया कि टिकट हिस्सा जुड़ने 2002 मार्गदर्शन होता का ने खनन में को . सबसे ठहराते यह अंग धीरूभाई हाथ स्टेशनों पाउंड गुप्ता ली जिले को पर एक गतिविधियों की प्रधान कि कि में चलता । मुख्य निर्दिष्ट आतंरिक नवंबर है\n"
     ]
    }
   ],
   "source": [
    "#-----------------------------------------------------------\n",
    "#-----------------------------------------------------------\n",
    "# inference and evaluation:\n",
    "#-------------------------------------------------------\n",
    "\n",
    "def translation(model, decoder_input,  encoder_output, encoder_mask, decoder_mask):\n",
    "  for indx in range(1, 101):\n",
    "    encoder_maskn = encoder_mask[:,:,:,:indx]\n",
    "    out = model.decoder(decoder_input,  encoder_output, encoder_maskn, decoder_mask)\n",
    "    logits = out[:, -1, :]  #(B,-1, C) -> (B, T+1th, C)\n",
    "    probs = F.softmax(logits, dim = -1)  #(B,C)\n",
    "    # sample from the distribution\n",
    "    idx_next = torch.multinomial(probs, num_samples=1) #(B,1)\n",
    "    # append sample in the running sequence\n",
    "    decoder_input = torch.cat((decoder_input,idx_next), dim = 1) #(B, T+1)\n",
    "  return tokenizer_tgt.decode(decoder_input[0].tolist())\n",
    "\n",
    "\n",
    "\n",
    "sentence = \"Chandigarh: India largest tyre manufacturer and one of the top 15 global tyre companies\"\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    # Precompute the encoder output and reuse it for every generation step\n",
    "    source = tokenizer_src.encode(sentence)\n",
    "    source = torch.cat([\n",
    "        torch.tensor([tokenizer_src.token_to_id('[SOS]')], dtype=torch.int64),\n",
    "        torch.tensor(source.ids, dtype=torch.int64),\n",
    "        torch.tensor([tokenizer_src.token_to_id('[EOS]')], dtype=torch.int64),\n",
    "        torch.tensor([tokenizer_src.token_to_id('[PAD]')] * (seq_len - len(source.ids) - 2), dtype=torch.int64)\n",
    "    ], dim=0).to(device)\n",
    "    source_mask = (source != tokenizer_src.token_to_id('[PAD]')).unsqueeze(0).unsqueeze(0).int().to(device)\n",
    "    # torch.Size([16, 100])\n",
    "    # torch.Size([16, 1, 1, 100])\n",
    "    batch_size = 1\n",
    "    encoder_input = source.unsqueeze(0)\n",
    "    encoder_mask = source_mask.unsqueeze(0)\n",
    "    sos_idx = tokenizer_tgt.token_to_id('[SOS]')\n",
    "    eos_idx = tokenizer_tgt.token_to_id('[EOS]')\n",
    "    encoder_output = model.encoder(encoder_input, encoder_mask)\n",
    "    #decoder_input = torch.empty(1, 1).fill_(sos_idx).type_as(encoder_input)\n",
    "    decoder_input = torch.empty(1, 1).fill_(tokenizer_tgt.token_to_id('[SOS]')).type_as(source).to(device)\n",
    "    decoder_mask = causal_mask(decoder_input.size(1)).type_as(encoder_input).to(device)\n",
    "    text = translation(model, decoder_input,  encoder_output, encoder_mask, decoder_mask)\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "print(f\"Translation Results:\")\n",
    "print(f\"ENG: {sentence}\")\n",
    "print(f\"HIN: {text}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# END!!!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
