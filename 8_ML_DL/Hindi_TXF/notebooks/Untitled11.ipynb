{"cells":[{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1594,"status":"ok","timestamp":1728908875160,"user":{"displayName":"Manish Negi","userId":"13518611288471091944"},"user_tz":-330},"id":"KymN5vDX_2hR","outputId":"49cac059-6eb0-41cd-bc0c-0eb4f3c16396"},"outputs":[{"name":"stderr","output_type":"stream","text":["c:\\Users\\Deevia\\miniconda3\\envs\\gpt\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n","  from .autonotebook import tqdm as notebook_tqdm\n","c:\\Users\\Deevia\\miniconda3\\envs\\gpt\\Lib\\site-packages\\huggingface_hub\\file_download.py:148: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Deevia\\.cache\\huggingface\\hub\\datasets--cfilt--iitb-english-hindi. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n","To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n","  warnings.warn(message)\n","Generating train split: 100%|██████████| 1659083/1659083 [00:00<00:00, 1736029.82 examples/s]\n","Generating validation split: 100%|██████████| 520/520 [00:00<00:00, 34440.34 examples/s]\n","Generating test split: 100%|██████████| 2507/2507 [00:00<00:00, 590836.67 examples/s]"]},{"name":"stdout","output_type":"stream","text":["Dataset({\n","    features: ['translation'],\n","    num_rows: 2507\n","})\n"]},{"name":"stderr","output_type":"stream","text":["\n"]}],"source":["from datasets import load_dataset\n","\n","# Load only the test split from the dataset\n","test_data = load_dataset(\"cfilt/iitb-english-hindi\", split=\"test\")\n","\n","# You now have the test data\n","print(test_data)\n"]},{"cell_type":"code","execution_count":6,"metadata":{"executionInfo":{"elapsed":464,"status":"ok","timestamp":1728911700362,"user":{"displayName":"Manish Negi","userId":"13518611288471091944"},"user_tz":-330},"id":"GgaomBW8_80C"},"outputs":[],"source":["\n","import torch\n","#from datasets import load_dataset\n","from tokenizers import tokenizers\n","from tokenizers.models import WordLevel\n","from torch.utils.data import Dataset, DataLoader, random_split\n","\n","from datasets import load_dataset\n","from tokenizers import Tokenizer\n","from tokenizers.models import WordLevel\n","from tokenizers.trainers import WordLevelTrainer\n","from tokenizers.pre_tokenizers import Whitespace\n","from pathlib import Path\n","import random\n","random.seed(42)\n","def get_config():\n","    return {\n","        \"batch_size\": 8,\n","        \"num_epochs\": 20,\n","        \"lr\": 10**-4,\n","        \"seq_len\": 350,\n","        \"d_model\": 512,\n","        \"datasource\": 'opus_books',\n","        \"lang_src\": \"en\",\n","        \"lang_tgt\": \"it\",\n","        \"model_folder\": \"weights\",\n","        \"model_basename\": \"tmodel_\",\n","        \"preload\": \"latest\",\n","        \"tokenizer_file\": \"tokenizer_{0}.json\",\n","        \"experiment_name\": \"runs/tmodel\"\n","    }\n","\n","config = get_config()\n","\n","def get_all_sentences(ds, lang):\n","    for item in ds:\n","        yield item['translation'][lang]\n","\n","def get_or_build_tokenizer(config, ds, lang):\n","    tokenizer_path = Path(config['tokenizer_file'].format(lang))\n","    if not Path.exists(tokenizer_path):\n","        # Most code taken from: https://huggingface.co/docs/tokenizers/quicktour\n","        tokenizer = Tokenizer(WordLevel(unk_token=\"[UNK]\"))\n","        tokenizer.pre_tokenizer = Whitespace()\n","        trainer = WordLevelTrainer(special_tokens=[\"[UNK]\", \"[PAD]\", \"[SOS]\", \"[EOS]\"], min_frequency=2)\n","        tokenizer.train_from_iterator(get_all_sentences(ds, lang), trainer=trainer)\n","        tokenizer.save(str(tokenizer_path))\n","    else:\n","        tokenizer = Tokenizer.from_file(str(tokenizer_path))\n","    return tokenizer\n","\n","\n","# Build tokenizers\n","tokenizer_src = get_or_build_tokenizer(config, test_data, 'en')\n","tokenizer_tgt = get_or_build_tokenizer(config, test_data, 'hi')"]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":533,"status":"ok","timestamp":1728909102192,"user":{"displayName":"Manish Negi","userId":"13518611288471091944"},"user_tz":-330},"id":"Y9ZKOGGWAqQC","outputId":"98a2c1b4-d36b-4dd5-a8fa-dfb24bcfbc18"},"outputs":[{"data":{"text/plain":["{'solids': 725,\n"," 'War': 3350,\n"," 'voices': 4354,\n"," 'spotted': 1145,\n"," 'week': 248,\n"," 'Air': 2174,\n"," 'Goon': 3098,\n"," 'Google': 320,\n"," 'facilities': 1277,\n"," 'secretary': 1625,\n"," 'channels': 1865,\n"," 'Festival': 2233,\n"," 'blonde': 1845,\n"," 'drug': 622,\n"," 'penis': 3997,\n"," 'Fernando': 3073,\n"," 'ceremonies': 3491,\n"," 'Sea': 2321,\n"," 'Commerce': 2218,\n"," 'agree': 3391,\n"," 'addition': 1810,\n"," 'assign': 3429,\n"," 'instruction': 2598,\n"," 'controlled': 3544,\n"," 'top': 430,\n"," 'tiny': 4289,\n"," 'disputes': 3624,\n"," 'device': 1077,\n"," 'run': 474,\n"," 'Narayan': 2287,\n"," 'prepaid': 4033,\n"," 'Michael': 1427,\n"," 'objective': 3962,\n"," 'Conservator': 3032,\n"," 'Silverado': 2329,\n"," 'dynastic': 2515,\n"," 'practice': 1591,\n"," 'Indian': 743,\n"," 'equipped': 1925,\n"," 'date': 1255,\n"," 'submitted': 2824,\n"," 'ready': 954,\n"," 'Neither': 3207,\n"," 'eve': 3669,\n"," 'popular': 4022,\n"," 'unit': 2133,\n"," 'cardinals': 450,\n"," 'Post': 1771,\n"," 'protein': 1597,\n"," 'fight': 1281,\n"," 'August': 487,\n"," 'boys': 1850,\n"," 'some': 107,\n"," 'concerns': 1060,\n"," 'Gender': 2241,\n"," 'testes': 4280,\n"," '),': 484,\n"," 'Usher': 3338,\n"," 'technical': 2116,\n"," 'counter': 3560,\n"," 'demand': 697,\n"," 'reverse': 4132,\n"," 'considered': 504,\n"," 'Relations': 3258,\n"," 'secure': 4161,\n"," 'experiment': 1519,\n"," 'Speaking': 3301,\n"," 'destroyed': 1900,\n"," 'Raipur': 2302,\n"," 'Padshahi': 3226,\n"," 'ADC': 1689,\n"," 'degree': 3587,\n"," 'recovered': 4091,\n"," 'suburbs': 4249,\n"," 'update': 2136,\n"," 'decision': 506,\n"," 'lantern': 3847,\n"," 'departments': 1076,\n"," 'scientists': 2076,\n"," '19th': 2911,\n"," 'Co': 1716,\n"," 'associated': 1467,\n"," 'Beaudoin': 2193,\n"," 'guests': 1957,\n"," 'specific': 2809,\n"," 'Basharat': 2985,\n"," 'strongly': 1644,\n"," 'where': 142,\n"," 'Arunoday': 2971,\n"," '330': 2924,\n"," 'experts': 1520,\n"," 'deputy': 3598,\n"," 'fifth': 3709,\n"," 'costumes': 2484,\n"," 'installed': 3814,\n"," 'Municipal': 493,\n"," 'then': 135,\n"," 'market': 164,\n"," 'view': 979,\n"," 'platforms': 4015,\n"," 'may': 243,\n"," '30': 662,\n"," '9th': 2170,\n"," 'anxious': 3414,\n"," 'Oregon': 3220,\n"," 'come': 215,\n"," 'Jersey': 2263,\n"," 'MP': 492,\n"," 'regulators': 1133,\n"," 'extensive': 1934,\n"," 'conflict': 895,\n"," 'man': 420,\n"," 'about': 47,\n"," 'Italian': 2259,\n"," 'sex': 964,\n"," 'becoming': 2417,\n"," 'essentially': 3665,\n"," 'arrangements': 1229,\n"," 'achieve': 3372,\n"," 'allergies': 1460,\n"," 'published': 953,\n"," 'examination': 2532,\n"," 'leaders': 163,\n"," '700': 1383,\n"," 'widening': 4367,\n"," 'major': 419,\n"," 'Slate': 3292,\n"," 'meditation': 2637,\n"," 'reaching': 4080,\n"," 'crores': 562,\n"," 'Europe': 169,\n"," 'Bitta': 2994,\n"," 'counsel': 3558,\n"," 'purchased': 1128,\n"," 'ki': 2607,\n"," 'State': 348,\n"," 'fluorescent': 3726,\n"," 'low': 578,\n"," 'facing': 3688,\n"," 'Americans': 538,\n"," 'participating': 2679,\n"," 'Charan': 3016,\n"," 'They': 112,\n"," 'adulteration': 3380,\n"," 'ICAC': 3116,\n"," 'dominated': 3629,\n"," 'trader': 1155,\n"," 'Keith': 1743,\n"," 'announced': 556,\n"," 'you': 96,\n"," 'wonderful': 1666,\n"," 'Given': 3094,\n"," 'capital': 1056,\n"," 'ahead': 883,\n"," 'dog': 507,\n"," 'Rail': 3247,\n"," 'Nitish': 3209,\n"," '325': 1677,\n"," 'advertisement': 3382,\n"," 'Ecuador': 3060,\n"," 'deliver': 3588,\n"," 'showing': 966,\n"," 'Thomas': 3323,\n"," 'Doerksen': 2226,\n"," 'fuelled': 3740,\n"," 'personalities': 2029,\n"," 'psychiatrist': 4060,\n"," 'undetermined': 4321,\n"," 'regions': 2750,\n"," 'daughter': 620,\n"," 'subject': 1645,\n"," 'witness': 2896,\n"," 'Latakia': 2272,\n"," 'turned': 1657,\n"," 'Raj': 1776,\n"," 'light': 2623,\n"," 'temple': 2120,\n"," 'refuge': 2748,\n"," 'core': 3552,\n"," 'same': 475,\n"," 'judge': 806,\n"," 'cover': 2487,\n"," 'current': 412,\n"," 'bins': 1844,\n"," 'step': 4230,\n"," 'admitted': 755,\n"," 'hoping': 1967,\n"," 'landed': 3844,\n"," 'them': 73,\n"," 'Islamist': 2257,\n"," 'earlier': 623,\n"," 'prepare': 4034,\n"," 'it': 31,\n"," 'evening': 625,\n"," 'fake': 1278,\n"," 'cigarette': 2449,\n"," 'recalls': 4088,\n"," 'Ashok': 1173,\n"," 'Scarbrough': 3282,\n"," 'Reports': 3260,\n"," 'Open': 3218,\n"," 'procedures': 2046,\n"," 'charging': 1866,\n"," 'walker': 4359,\n"," 'at': 30,\n"," 'husband': 3789,\n"," 'exactly': 3671,\n"," 'province': 2725,\n"," 'department': 156,\n"," 'older': 3969,\n"," 'group': 196,\n"," 'create': 897,\n"," 'warned': 1663,\n"," 'sell': 1345,\n"," 'Rohit': 2309,\n"," 'declined': 2496,\n"," 'defaulters': 3581,\n"," 'heard': 630,\n"," 'Tales': 3318,\n"," 'cuts': 1501,\n"," 'opportunity': 1317,\n"," 'replied': 2759,\n"," 'of': 7,\n"," 'pawn': 1581,\n"," 'trend': 2129,\n"," 'items': 1546,\n"," 'innocent': 1543,\n"," 'wouldn': 4379,\n"," 'glory': 3752,\n"," 'Glenn': 3096,\n"," 'transmissions': 2858,\n"," 'reason': 955,\n"," 'Jharkhand': 2264,\n"," 'wasn': 2144,\n"," 'Rajiv': 1441,\n"," 'Three': 2347,\n"," 'advantage': 3381,\n"," 'eligibility': 2526,\n"," 'candidate': 687,\n"," 'Why': 2364,\n"," 'm': 579,\n"," 'bombings': 2425,\n"," 'Habibullah': 3101,\n"," 'battle': 1840,\n"," 'Maharashtra': 3173,\n"," 'owned': 2672,\n"," 'response': 4126,\n"," 'whose': 982,\n"," 'standup': 2814,\n"," 'going': 239,\n"," 'familiar': 3696,\n"," 'claiming': 2452,\n"," 'life': 242,\n"," 'secretive': 4159,\n"," 'offset': 3968,\n"," 'Here': 1415,\n"," 'layout': 3852,\n"," 'author': 1836,\n"," '2009': 1379,\n"," 'spokesperson': 2811,\n"," 'key': 807,\n"," 'filed': 457,\n"," 'Administration': 1170,\n"," 'joint': 1982,\n"," 'Murphy': 1431,\n"," 'Hemant': 1729,\n"," 'hopes': 1966,\n"," 'Mercedes': 3186,\n"," 'denies': 3593,\n"," 'organization': 1118,\n"," 'participate': 3990,\n"," 'Tamim': 2339,\n"," 'previously': 1326,\n"," 'not': 37,\n"," 'inside': 805,\n"," 'operations': 2663,\n"," 'company': 203,\n"," 'style': 2823,\n"," 'George': 3088,\n"," 'form': 1286,\n"," 'trained': 2856,\n"," 'discussed': 3620,\n"," 'sexual': 531,\n"," 'NYPD': 1761,\n"," 'sang': 4151,\n"," 'significantly': 4189,\n"," 'national': 342,\n"," 'part': 326,\n"," 'deal': 1073,\n"," 'dropped': 3638,\n"," 'J': 3131,\n"," 'many': 111,\n"," 'width': 1372,\n"," 'motorcade': 3933,\n"," 'Democratic': 3048,\n"," 'Today': 1219,\n"," 'solicitor': 1637,\n"," 'local': 934,\n"," 'attacked': 2406,\n"," 'project': 821,\n"," 'deciding': 3578,\n"," 'studies': 4244,\n"," '83': 2936,\n"," 'hours': 418,\n"," 'discrimination': 2507,\n"," 'percent': 361,\n"," 'o': 943,\n"," 'Sierra': 2328,\n"," 'jail': 1298,\n"," 'criticism': 2489,\n"," 'gathered': 909,\n"," 'success': 1646,\n"," 'gallon': 3743,\n"," 'Nagar': 2286,\n"," 'single': 2799,\n"," 'longer': 935,\n"," 'forged': 1947,\n"," 'Rockstar': 1442,\n"," 'occur': 3965,\n"," 'off': 184,\n"," 'another': 1038,\n"," 'being': 71,\n"," 'determined': 1259,\n"," 'cheap': 1867,\n"," 'buyers': 3478,\n"," 'suggests': 2107,\n"," 'Christmas': 1715,\n"," 'Wi': 3354,\n"," 'tyre': 2863,\n"," 'As': 251,\n"," 'You': 550,\n"," 'groups': 916,\n"," 'domestic': 903,\n"," 'Russian': 3272,\n"," 'into': 80,\n"," 'unable': 2864,\n"," 'fell': 1280,\n"," 'Sorensen': 3299,\n"," 'al': 1820,\n"," 'Temple': 2343,\n"," 'Airways': 2951,\n"," 'accountability': 3370,\n"," 'we': 61,\n"," 'Financial': 1405,\n"," 'advisers': 2384,\n"," 'Revenue': 2308,\n"," 'issue': 386,\n"," 'recorded': 1332,\n"," 'freedom': 3736,\n"," 'narrower': 3944,\n"," 'housing': 3785,\n"," 'motorists': 3934,\n"," 'interested': 3820,\n"," 'spy': 1146,\n"," 'foreigners': 3732,\n"," 'lured': 3884,\n"," 'notification': 3957,\n"," 'common': 775,\n"," 'Mukesh': 1199,\n"," 'Mittal': 3189,\n"," 'your': 250,\n"," 'judges': 1551,\n"," 'White': 1451,\n"," 'interview': 923,\n"," 'criterion': 3568,\n"," 'via': 4344,\n"," 'Vatican': 2360,\n"," 'Federation': 3072,\n"," 'DSD': 3045,\n"," 'when': 82,\n"," 'Mazanga': 1015,\n"," 'ways': 1163,\n"," 'withdrawn': 4373,\n"," 'unspecified': 4326,\n"," 'And': 328,\n"," 'fee': 456,\n"," 'flow': 2550,\n"," 'sector': 1627,\n"," 'Russia': 1214,\n"," 'imposed': 1294,\n"," 'posted': 2707,\n"," 'Iqbal': 3125,\n"," 'feelings': 3705,\n"," 'Georges': 1411,\n"," 'aerospace': 3385,\n"," 'trendy': 4308,\n"," 'extra': 1086,\n"," 'losses': 3878,\n"," 'Serial': 2324,\n"," 'leisure': 3861,\n"," 'powerful': 2709,\n"," 'Agarwal': 2948,\n"," 'Anderson': 1172,\n"," 'idea': 3790,\n"," 'represent': 4115,\n"," 'report': 246,\n"," 'appearance': 1464,\n"," 'fix': 3720,\n"," 'grown': 3762,\n"," 'girls': 797,\n"," 'theories': 4284,\n"," 'Despite': 3050,\n"," 'ordered': 715,\n"," 'personality': 4000,\n"," 'soldiers': 2094,\n"," 'liquids': 3871,\n"," '26': 2162,\n"," 'murder': 2651,\n"," 'vice': 2872,\n"," 'Uttarakhand': 2357,\n"," 'activists': 609,\n"," 'designs': 3600,\n"," 'OPCW': 1434,\n"," 'While': 444,\n"," 'walked': 4358,\n"," 'allowing': 1821,\n"," 'wild': 2893,\n"," 'canines': 2435,\n"," 'reforms': 4096,\n"," 'alternative': 2392,\n"," 'us': 432,\n"," 'killed': 1105,\n"," 'ordinary': 3976,\n"," 'shopping': 2085,\n"," 'democracy': 1075,\n"," 'layouts': 3853,\n"," 'say': 221,\n"," 'News': 674,\n"," 'die': 1902,\n"," 'UPA': 1799,\n"," 'website': 369,\n"," 'mortem': 2646,\n"," 'Tahrir': 3317,\n"," 'departure': 3595,\n"," 'stability': 2098,\n"," 'executed': 3676,\n"," 'rivers': 4141,\n"," 'houses': 1291,\n"," 'indicated': 1972,\n"," 'defeated': 3582,\n"," 'movements': 2649,\n"," 'treated': 2860,\n"," 'anger': 1824,\n"," 'presided': 4037,\n"," 'continue': 505,\n"," 'consider': 2473,\n"," 'polls': 2703,\n"," 'Bendemeer': 2989,\n"," 'Quetta': 3245,\n"," 'Children': 1714,\n"," 'complaint': 1493,\n"," 'energy': 1513,\n"," 'cliff': 1870,\n"," 'have': 34,\n"," 'Kamal': 3150,\n"," 'struggling': 4243,\n"," 'D': 2222,\n"," 'tender': 2842,\n"," 'complete': 694,\n"," 'conducted': 1876,\n"," 'Bhiwani': 1702,\n"," 'despite': 1898,\n"," 'continues': 1253,\n"," 'ignore': 3793,\n"," 'unless': 4324,\n"," 'certain': 2442,\n"," 'Sagar': 2314,\n"," '457': 2167,\n"," 'violated': 4345,\n"," 'Kashmir': 1741,\n"," 'kilolitres': 3841,\n"," 'Andy': 2964,\n"," 'airlifted': 3396,\n"," 'needed': 1571,\n"," 'Bank': 856,\n"," 'race': 4068,\n"," 'rural': 958,\n"," 'My': 1201,\n"," 'Hanuman': 2247,\n"," 'embassy': 2528,\n"," 'providers': 4059,\n"," 'reacted': 4082,\n"," 'Circle': 3020,\n"," 'GCSEs': 3084,\n"," 'advance': 1227,\n"," 'skeptical': 4196,\n"," 'operator': 3972,\n"," '53': 1686,\n"," 'combined': 2462,\n"," 'charges': 770,\n"," 'decisions': 778,\n"," 'AAP': 2940,\n"," 'weight': 2888,\n"," 'cables': 3479,\n"," 'music': 3940,\n"," 'Spirit': 1446,\n"," 'director': 321,\n"," 'Sunday': 1031,\n"," 'border': 1848,\n"," 'ports': 4024,\n"," 'Public': 3242,\n"," 'path': 2685,\n"," 'new': 77,\n"," '[SOS]': 2,\n"," 'really': 363,\n"," 'Tunks': 2351,\n"," 'bad': 1233,\n"," 'Katie': 1742,\n"," 'spoke': 4218,\n"," 'statements': 4224,\n"," 'helped': 1535,\n"," 'poor': 2036,\n"," 'sewing': 4174,\n"," 'master': 3900,\n"," 'MCA': 1425,\n"," 'short': 1632,\n"," 'for': 17,\n"," 'average': 1044,\n"," 'manager': 2000,\n"," 'attacks': 1230,\n"," 'Sood': 1787,\n"," 'intelligence': 573,\n"," 'tour': 973,\n"," 'effective': 2520,\n"," 'recovery': 2746,\n"," 'responsibility': 2072,\n"," 'overnight': 2671,\n"," 'Vivek': 3347,\n"," 'bowler': 3470,\n"," 'corruption': 3555,\n"," 'Naushera': 1763,\n"," 'knocking': 3843,\n"," 'manufacturer': 3891,\n"," 'mean': 3906,\n"," '2015': 1674,\n"," 'panchayat': 3984,\n"," 'expand': 3677,\n"," 'premium': 4032,\n"," 'levels': 1990,\n"," 'shed': 4177,\n"," 'officer': 3967,\n"," 'destruction': 2503,\n"," 'score': 2782,\n"," 'views': 2139,\n"," 'formal': 1948,\n"," 'contractor': 1881,\n"," 'foundation': 2556,\n"," 'Church': 3019,\n"," 'bought': 2426,\n"," 'Veer': 3343,\n"," 'reach': 1330,\n"," 'long': 285,\n"," 'regulation': 4099,\n"," 'thousands': 1363,\n"," 'Halloween': 608,\n"," 'safety': 590,\n"," 'helps': 3776,\n"," 'try': 1366,\n"," '80': 1169,\n"," 'learn': 3855,\n"," 'auspicious': 2409,\n"," 'interior': 3822,\n"," 'Some': 546,\n"," 'strategies': 4237,\n"," 'England': 1402,\n"," 'lifestyle': 3865,\n"," 'Leh': 2275,\n"," 'fellow': 2545,\n"," 'Shrinagar': 1786,\n"," 'baby': 3446,\n"," 'infringement': 3809,\n"," 'Principal': 1775,\n"," 'laws': 2612,\n"," 'debate': 2493,\n"," 'deployed': 1895,\n"," 'Berlusconi': 1175,\n"," 'membership': 3910,\n"," 'Microsoft': 2283,\n"," 'themselves': 836,\n"," 'So': 405,\n"," 'attack': 447,\n"," 'Tony': 1795,\n"," 'answer': 756,\n"," 'most': 106,\n"," 'post': 646,\n"," 'probes': 4040,\n"," 'meeting': 325,\n"," 'Commission': 606,\n"," 'targeted': 2835,\n"," 'fun': 2559,\n"," 'allotted': 3403,\n"," 'Reason': 3256,\n"," 'journal': 3833,\n"," 'properties': 2049,\n"," 'studying': 2822,\n"," 'troops': 4310,\n"," 'worked': 2153,\n"," 'trip': 1655,\n"," 'tunnels': 4313,\n"," 'scheduled': 1342,\n"," 'Finma': 3075,\n"," 'dilemma': 3614,\n"," '777': 2169,\n"," 'Parliament': 1209,\n"," 'manufacturers': 3892,\n"," 'stamp': 1640,\n"," 'Gazprom': 1410,\n"," 'Chalisa': 3011,\n"," 'figure': 1941,\n"," 'shocked': 4182,\n"," 'girl': 1090,\n"," 'sized': 2801,\n"," 'health': 240,\n"," 'tests': 1151,\n"," 'has': 24,\n"," 'guarded': 2566,\n"," 'Citing': 2216,\n"," 'fleet': 3723,\n"," 'Montreal': 3193,\n"," 'information': 211,\n"," 'We': 125,\n"," 'works': 1165,\n"," 'Cooper': 3035,\n"," 'drop': 1910,\n"," 'restore': 4129,\n"," 'footwear': 1946,\n"," 'topic': 2852,\n"," 'coal': 2458,\n"," 'enter': 3654,\n"," 'tigress': 837,\n"," '[UNK]': 0,\n"," 'wrong': 1166,\n"," 'events': 1515,\n"," '40': 603,\n"," 'third': 229,\n"," 'PPPs': 3225,\n"," 'sign': 1349,\n"," 'minds': 3918,\n"," 'too': 247,\n"," 'up': 68,\n"," 'hospital': 919,\n"," '2001': 2914,\n"," 'inflation': 3807,\n"," 'Ströbele': 2334,\n"," 'features': 3703,\n"," 'Laden': 2270,\n"," 'property': 952,\n"," 'surgery': 730,\n"," 'seize': 2079,\n"," 'lack': 1986,\n"," 'encourage': 1512,\n"," 'Rural': 3271,\n"," 'storage': 4234,\n"," 'assure': 3433,\n"," 'appeals': 885,\n"," 'critical': 1069,\n"," 'Andrew': 2179,\n"," 'Historically': 2250,\n"," 'tonight': 2851,\n"," 'seriously': 1347,\n"," 'Chen': 1180,\n"," 'Yeah': 2367,\n"," 'park': 1580,\n"," 'Alex': 2176,\n"," 'GPS': 2239,\n"," 'easy': 2518,\n"," 'Israel': 2258,\n"," 'stars': 2816,\n"," 'elite': 3647,\n"," '3': 661,\n"," 'Jagat': 3136,\n"," 'Maryland': 3180,\n"," 'statement': 428,\n"," 'infertile': 3806,\n"," 'surveillance': 277,\n"," 'Shah': 1216,\n"," 'extradition': 1935,\n"," 'BJP': 180,\n"," 'division': 1265,\n"," 'Dronacharya': 3054,\n"," 'access': 1224,\n"," 'Maputo': 3178,\n"," 'there': 84,\n"," 'getting': 911,\n"," 'held': 324,\n"," 'Atal': 2976,\n"," 'friend': 1527,\n"," 'High': 1730,\n"," 'filled': 3713,\n"," '2000': 2913,\n"," 'hacked': 3767,\n"," 'German': 190,\n"," 'track': 975,\n"," 'pumpkins': 4061,\n"," 'Catholic': 1178,\n"," 'Charlie': 1713,\n"," 'establish': 3666,\n"," 'was': 21,\n"," 'became': 353,\n"," 'Mukul': 3197,\n"," ')': 261,\n"," 'love': 1996,\n"," 'Sessions': 3286,\n"," 'lift': 3866,\n"," 'roads': 2776,\n"," 'season': 2784,\n"," 'moves': 3935,\n"," 'Special': 3302,\n"," '?\"': 2171,\n"," 'base': 2416,\n"," '21st': 2160,\n"," 'Bay': 2986,\n"," 'Secretary': 675,\n"," 'Halwa': 2246,\n"," 'California': 1177,\n"," 'study': 1148,\n"," 'haven': 1963,\n"," 'sales': 2781,\n"," 'allies': 1228,\n"," 'handle': 1094,\n"," 'ask': 1042,\n"," 'Airlines': 663,\n"," 'ambitious': 1462,\n"," 'All': 537,\n"," 'distributed': 2509,\n"," 'informed': 2587,\n"," 'mission': 2642,\n"," 'androgen': 3409,\n"," 'genitalia': 1530,\n"," 'abroad': 2373,\n"," 'two': 86,\n"," 'entry': 3656,\n"," 'Director': 490,\n"," 'member': 583,\n"," 'bid': 3456,\n"," 'Obeid': 871,\n"," 'd': 1072,\n"," 'evasive': 3668,\n"," 'rainfall': 4074,\n"," 'reaction': 4083,\n"," 'volunteers': 1661,\n"," 'commercial': 773,\n"," 'Glasgow': 1005,\n"," 'terrorists': 1361,\n"," 'like': 198,\n"," 'issues': 706,\n"," 'February': 864,\n"," 'conference': 894,\n"," 'hopeful': 2576,\n"," 'similar': 2796,\n"," 'aging': 3390,\n"," 'apology': 3416,\n"," 'risks': 4139,\n"," 'Edinburgh': 2228,\n"," 'changing': 3495,\n"," 'tears': 4277,\n"," 'end': 624,\n"," 'No': 347,\n"," 'link': 2625,\n"," 'separate': 4166,\n"," 'Govind': 3100,\n"," 'Asian': 2975,\n"," 'Cameroon': 996,\n"," '2008': 735,\n"," 'political': 274,\n"," 'Northern': 3212,\n"," 'comeback': 3516,\n"," 'super': 4257,\n"," 'dots': 3631,\n"," 'agents': 1816,\n"," 'impose': 3798,\n"," 'add': 1809,\n"," 'guy': 3764,\n"," 'believe': 448,\n"," 'asylum': 1468,\n"," 'anyone': 1039,\n"," 'elected': 1510,\n"," 'requirements': 4120,\n"," 'ago': 235,\n"," 'accept': 3366,\n"," 'farmers': 2542,\n"," 'stall': 2812,\n"," 'changed': 614,\n"," 'woman': 598,\n"," 'Baratz': 2191,\n"," 'less': 517,\n"," 'telling': 1651,\n"," '60': 737,\n"," 'transferred': 2857,\n"," 'Associate': 2185,\n"," 'dressed': 3635,\n"," 'occasions': 3964,\n"," 'postponed': 4026,\n"," 'everybody': 3670,\n"," 'undertake': 4319,\n"," 'big': 375,\n"," 'parliamentary': 2678,\n"," 'rates': 1329,\n"," 'politicians': 2701,\n"," 'pediatrician': 3996,\n"," 'Akali': 2953,\n"," 'cash': 768,\n"," 'SAP': 3274,\n"," 'spread': 2097,\n"," 'Pradeep': 2299,\n"," 'double': 3632,\n"," 'maths': 1563,\n"," 'jets': 1549,\n"," 'possibility': 2038,\n"," 'Her': 3105,\n"," 'singing': 4190,\n"," 'feared': 3701,\n"," 'accompanied': 3369,\n"," 'rid': 4138,\n"," 'implemented': 1541,\n"," 'peers': 2689,\n"," 'jointly': 3832,\n"," 'Rai': 1023,\n"," 'remarkable': 4110,\n"," 'expensive': 1518,\n"," 'stage': 2099,\n"," 'Martin': 869,\n"," 'This': 91,\n"," 'lights': 2624,\n"," 'winter': 2895,\n"," 'Wales': 3348,\n"," 'whom': 4366,\n"," 'Anand': 991,\n"," 'San': 1783,\n"," 'clean': 3504,\n"," 'commissioned': 3520,\n"," 'blocks': 2422,\n"," ').': 2902,\n"," '500': 1684,\n"," 'jobs': 1981,\n"," 'use': 108,\n"," 'Forest': 3078,\n"," 'centre': 690,\n"," 'rather': 823,\n"," 'win': 846,\n"," 'wickets': 2151,\n"," 'RBS': 1440,\n"," 'well': 129,\n"," 'United': 332,\n"," 'federal': 567,\n"," 'Ghumar': 3091,\n"," 'American': 370,\n"," 'district': 1264,\n"," 'coins': 3511,\n"," 'subsequently': 4247,\n"," 'October': 233,\n"," 'Bali': 1698,\n"," 'footsteps': 3729,\n"," 'Prof': 751,\n"," '1966': 2909,\n"," 'are': 28,\n"," 'comment': 1874,\n"," 'visa': 4349,\n"," 'fly': 2551,\n"," 'FCA': 3067,\n"," 'Each': 3058,\n"," 'film': 3715,\n"," '350': 2927,\n"," 'recommended': 1604,\n"," 'impossible': 3799,\n"," 'nonprofit': 3956,\n"," 'complaints': 1494,\n"," 'row': 1618,\n"," 'detained': 2505,\n"," 'selling': 962,\n"," 'limit': 3868,\n"," 'marketplace': 3896,\n"," 'competition': 1250,\n"," 'County': 3038,\n"," 'leadership': 929,\n"," 'GM': 1190,\n"," 'restart': 4127,\n"," 'suspected': 1358,\n"," 'sided': 2793,\n"," 'Ditta': 329,\n"," 'kept': 1104,\n"," 'English': 1403,\n"," 'pawnshop': 2687,\n"," 'definitely': 3586,\n"," 'Barclays': 2192,\n"," 'SHOs': 3276,\n"," 'clothes': 1872,\n"," 'barrels': 3450,\n"," 'lecturer': 3858,\n"," 'nicotine': 641,\n"," 'Mozambique': 1430,\n"," 'running': 1621,\n"," 'teacher': 4276,\n"," 'Technical': 1793,\n"," 'cold': 693,\n"," 'Germany': 330,\n"," 'crore': 1070,\n"," 'funding': 1528,\n"," 'countries': 777,\n"," 'push': 2731,\n"," 'Park': 2296,\n"," 'Medical': 1016,\n"," 'falsely': 3695,\n"," 'policy': 393,\n"," '130mph': 2907,\n"," 'Blair': 1705,\n"," 'activity': 1808,\n"," 'Petroleum': 3234,\n"," 'future': 795,\n"," 'Edis': 1721,\n"," 'morale': 3930,\n"," 'cheaper': 2447,\n"," 'At': 252,\n"," 'game': 3744,\n"," 'suspect': 1149,\n"," 'Damascus': 1718,\n"," 'cut': 3571,\n"," 'Authority': 2186,\n"," 'equal': 3661,\n"," 'design': 2501,\n"," 'recent': 720,\n"," 'bird': 3457,\n"," 'standards': 2100,\n"," 'Curma': 3042,\n"," 'Italy': 3130,\n"," 'peace': 819,\n"," 'representative': 1609,\n"," 'crimes': 3567,\n"," 'pH': 2673,\n"," 'check': 1247,\n"," 'tough': 2853,\n"," 'Cardinals': 3008,\n"," 'sons': 4210,\n"," 'jailed': 1102,\n"," 'movies': 3936,\n"," 'ruled': 1139,\n"," ',\\'\"': 2903,\n"," 'finding': 2548,\n"," 'Francisca': 2237,\n"," 'valley': 2871,\n"," 'significant': 1633,\n"," 'lot': 577,\n"," 'Dhanteras': 1185,\n"," 'Ambani': 327,\n"," 'findings': 1524,\n"," 'Biden': 1176,\n"," 'sides': 2087,\n"," 'CAT': 1393,\n"," 'birthplace': 3458,\n"," 'diagnosed': 3612,\n"," 'epinephrine': 3660,\n"," 'Pope': 1439,\n"," 'store': 4235,\n"," 'stowed': 4236,\n"," 'variety': 4337,\n"," 'reform': 2066,\n"," 'mileage': 2005,\n"," 'Labor': 2269,\n"," 'wife': 1665,\n"," 'from': 27,\n"," 'daughters': 900,\n"," 'humans': 3786,\n"," 'before': 131,\n"," 'Hispanic': 1732,\n"," 'Chadvir': 1397,\n"," 'cultural': 1889,\n"," 'committed': 2465,\n"," 'Power': 1772,\n"," 'Senator': 3285,\n"," 'inaccurate': 3802,\n"," 'footage': 2553,\n"," 'ensure': 1923,\n"," 'came': 225,\n"," 'Patek': 873,\n"," 'serving': 4169,\n"," 'universal': 4323,\n"," 'grant': 3760,\n"," 'confirmed': 1495,\n"," 'Tendulkar': 2344,\n"," 'acknowledges': 3374,\n"," 'Jim': 2265,\n"," 'Colony': 3030,\n"," 'places': 2033,\n"," 'hunt': 3788,\n"," 'relations': 2755,\n"," 'surgeon': 2830,\n"," 'London': 402,\n"," 'shirt': 4180,\n"," 'walks': 2880,\n"," 'death': 281,\n"," 'Association': 992,\n"," 'giving': 1531,\n"," 't': 72,\n"," 'realised': 4085,\n"," 'including': 152,\n"," 'Rather': 2307,\n"," 'Murtaza': 3198,\n"," 'focusing': 3727,\n"," 'Barack': 1699,\n"," 'Academy': 2944,\n"," ...}"]},"execution_count":7,"metadata":{},"output_type":"execute_result"}],"source":["tokenizer_src.get_vocab()\n"]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":500,"status":"ok","timestamp":1728909121075,"user":{"displayName":"Manish Negi","userId":"13518611288471091944"},"user_tz":-330},"id":"jdrQn6CrAzfD","outputId":"d9e87676-b7cc-4da0-bc77-54a77cdab017"},"outputs":[{"data":{"text/plain":["{'वालों': 942,\n"," 'वास्तविक': 943,\n"," 'थीं': 618,\n"," '60': 861,\n"," 'पायलट': 1624,\n"," 'दृष्टि': 1349,\n"," 'जा': 68,\n"," 'पासपोर्ट': 1974,\n"," 'कर्मचारी': 718,\n"," 'निवासी': 912,\n"," 'अंदरूनी': 2904,\n"," 'लगने': 3918,\n"," 'रहना': 2667,\n"," 'आश्वस्त': 3008,\n"," 'आमंत्रित': 1796,\n"," 'बजाय': 586,\n"," 'सहयोग': 850,\n"," 'तथ्यों': 2430,\n"," 'रंगकर्मी': 3863,\n"," 'सेंट': 852,\n"," 'बंगाल': 3658,\n"," 'डेमोक्रेटिक': 3404,\n"," 'जन्मदिन': 1575,\n"," 'बीरा': 3697,\n"," 'चैपल': 3304,\n"," 'बिलकुल': 927,\n"," 'सकता': 96,\n"," 'कॉमेडी': 720,\n"," 'चार्ली': 1871,\n"," 'जाए': 354,\n"," 'संदेशों': 2754,\n"," 'SAP': 2898,\n"," 'चलेगा': 3286,\n"," 'मीडिया': 226,\n"," 'प्राथमिकता': 2561,\n"," 'मुक्त': 2042,\n"," 'राशि': 1695,\n"," 'पूर्वानुमान': 2538,\n"," 'योजनाकार': 2659,\n"," 'वार्ता': 2716,\n"," 'निर्देशक': 3510,\n"," 'बृजेंद्र': 3704,\n"," 'जितना': 1579,\n"," 'इस्लाम': 3033,\n"," 'यूशर': 3858,\n"," 'पादशाही': 3565,\n"," 'कंपनियों': 401,\n"," 'वेतन': 1442,\n"," 'टैक्सी': 3374,\n"," 'चयन': 2361,\n"," 'मैंने': 3831,\n"," 'कविता': 3142,\n"," 'बोलते': 1668,\n"," 'रूप': 41,\n"," 'वर्षीय': 261,\n"," 'वेल्लरटिगरा': 2734,\n"," '2000': 2876,\n"," 'सहभागिता': 1461,\n"," 'जन': 2383,\n"," 'पहुंचे': 739,\n"," 'सीरियल': 4093,\n"," 'एमआरएफ': 1823,\n"," 'लगता': 369,\n"," 'हमलों': 2153,\n"," 'तुरन्त': 1160,\n"," 'आजकल': 2223,\n"," 'ट्रैक': 1337,\n"," 'कारणों': 1302,\n"," '$': 200,\n"," 'सोरेन': 4118,\n"," 'निष्कर्ष': 913,\n"," 'चिकित्सक': 466,\n"," 'नई': 736,\n"," 'जगह': 665,\n"," 'अभियानों': 2939,\n"," 'विपरीत': 2724,\n"," 'पत्ता': 3544,\n"," 'अलबत्ता': 2212,\n"," 'मौद्रिक': 3843,\n"," 'सुरक्षा': 120,\n"," 'जीतने': 1336,\n"," 'जयंती': 2387,\n"," 'अंतर्गत': 2901,\n"," 'एंथनी': 3068,\n"," 'प्रचार': 1983,\n"," 'क्लाउड': 1846,\n"," 'मॉडल': 1420,\n"," 'जिनका': 1889,\n"," 'करते': 55,\n"," 'खुशी': 1561,\n"," 'दूरी': 1164,\n"," 'औरत': 2274,\n"," 'करीब': 402,\n"," 'FCA': 2893,\n"," 'तिहाई': 3420,\n"," 'कुमार': 404,\n"," 'कमरे': 1298,\n"," 'चढ़ा': 2359,\n"," 'लगवाया': 3919,\n"," 'मोदी': 328,\n"," 'जरूरत': 1576,\n"," 'खन्ना': 3216,\n"," 'खनन': 2324,\n"," 'छतों': 2377,\n"," 'गाव': 3243,\n"," 'छमाही': 1877,\n"," 'वैट': 3997,\n"," 'उठ': 3041,\n"," 'किनारे': 3164,\n"," 'पहले': 60,\n"," 'गुनाह': 3251,\n"," 'अनुरोध': 1116,\n"," 'के': 4,\n"," 'फटकार': 3637,\n"," 'खूब': 2329,\n"," 'अभिलक्षण': 2941,\n"," 'गिरफ्तार': 888,\n"," 'खिलाफ': 303,\n"," 'पशु': 1621,\n"," 'हू': 2860,\n"," 'एसोसिएशन': 1135,\n"," 'प्रभाव': 1191,\n"," 'मार्केट': 685,\n"," 'उल्लेखनीय': 1527,\n"," 'टूट': 3372,\n"," 'डांस': 3393,\n"," 'बाहरी': 2590,\n"," 'सारी': 1257,\n"," 'आतंकवादियों': 2224,\n"," 'मृतक': 3825,\n"," 'सैन': 2132,\n"," 'खासकर': 2327,\n"," 'रिंग': 2059,\n"," 'दक्षिणी': 1599,\n"," 'संस्तुति': 2763,\n"," 'एडीए': 1133,\n"," 'रोधी': 3913,\n"," 'लाइन': 1222,\n"," 'सामान्य': 773,\n"," 'फ्रांसिस्का': 2571,\n"," 'कंपनी': 317,\n"," 'अध्यक्ष': 348,\n"," 'आर्थिक': 349,\n"," 'बड़ा': 587,\n"," 'समर्थन': 416,\n"," 'उन': 254,\n"," '39': 1767,\n"," 'उपयोग': 151,\n"," 'ऑस्ट्रेलिया': 461,\n"," 'हिस्सा': 565,\n"," 'बीमा': 1394,\n"," 'सम्मान': 2110,\n"," 'घुमाते': 2356,\n"," 'लेखन': 3938,\n"," 'संगठित': 2095,\n"," 'अकाउंट': 2193,\n"," 'जमानत': 1329,\n"," 'कार्यों': 2297,\n"," 'दिन': 220,\n"," 'टायर': 1901,\n"," 'रमेश': 2665,\n"," 'विदेश': 2075,\n"," 'सल्तनत': 4070,\n"," 'अग्रणी': 2194,\n"," 'भिवानी': 2021,\n"," 'प्रोटीन': 1644,\n"," 'यूनाइटेड': 2049,\n"," 'रिग': 2681,\n"," 'व्यापारिक': 4005,\n"," 'कांग्रेसी': 1835,\n"," 'उम्मीदवारों': 3060,\n"," 'हस्तक्षेप': 2851,\n"," 'कमल': 3126,\n"," 'जोन': 3353,\n"," '19': 2172,\n"," 'बिजनेस': 1661,\n"," 'आवश्यक': 1800,\n"," 'सप्ताहांत': 1730,\n"," 'ट्रेन': 3383,\n"," 'किराए': 3166,\n"," 'ह्यूगेस': 4175,\n"," 'ठुकरा': 3389,\n"," 'निर्धनता': 3513,\n"," 'भयावह': 3737,\n"," 'झेलम': 3360,\n"," 'पड़ा': 1034,\n"," 'अंक': 2190,\n"," 'मिलती': 3807,\n"," 'रिसर्च': 1215,\n"," 'विवरणिका': 3990,\n"," 'व्यक्ति': 163,\n"," 'अंग': 1273,\n"," 'फीसद': 1044,\n"," 'उसने': 145,\n"," 'रोक': 1218,\n"," 'मुद्दा': 1417,\n"," 'न': 122,\n"," 'टर्नर': 1899,\n"," 'डिवाइस': 187,\n"," 'बादल': 2010,\n"," '96': 2891,\n"," 'ज्यादातर': 3356,\n"," 'सुख': 4096,\n"," 'न्यू': 504,\n"," 'घुमाने': 3275,\n"," 'बुराई': 3701,\n"," 'ब्रदरहुड': 3726,\n"," 'टेंडर': 1903,\n"," 'प्रक्रियाएं': 3598,\n"," 'पत्नी': 2508,\n"," '6': 420,\n"," 'दक्षिण': 674,\n"," 'जख्मी': 2381,\n"," 'टीम': 190,\n"," 'तेल': 1161,\n"," 'सीरा': 2812,\n"," 'नक्सल': 2476,\n"," 'सजा': 640,\n"," 'लोकप्रिय': 3945,\n"," 'घोषणा': 429,\n"," 'मारने': 1204,\n"," 'आयु': 458,\n"," 'बढ़ती': 2576,\n"," 'ओवरहेड': 2272,\n"," 'मिल': 550,\n"," 'बैंक': 480,\n"," 'बचना': 3664,\n"," 'बर्लुस्कोनी': 2006,\n"," 'ट्रू': 3382,\n"," 'मास्टर': 3801,\n"," 'गायन': 3241,\n"," 'इसका': 193,\n"," 'वायुयान': 766,\n"," 'परिवारों': 1960,\n"," 'नोट': 3529,\n"," 'सुविधाएं': 4102,\n"," 'डिफेन्डर': 3397,\n"," ',': 7,\n"," 'प्रमाण': 1638,\n"," 'रोहित': 2687,\n"," 'CSeries': 1774,\n"," 'राजनगर': 3877,\n"," 'लेह': 2706,\n"," 'पता': 146,\n"," 'क्रीम': 1553,\n"," 'सेवानिवृत्त': 2821,\n"," '300': 1765,\n"," 'विशेषज्ञों': 1076,\n"," '.\"': 605,\n"," 'हमारे': 154,\n"," 'ख्याल': 3227,\n"," 'सीरिया': 518,\n"," 'पब्लिक': 3550,\n"," 'थे': 52,\n"," 'लड़के': 2064,\n"," 'डब्लू': 673,\n"," 'सुनिश्चित': 1464,\n"," 'देखना': 1931,\n"," 'अनुभव': 1115,\n"," 'देनी': 3464,\n"," 'प्रणाली': 434,\n"," 'प्रशिक्षण': 745,\n"," 'जूरी': 3346,\n"," 'स्वीकार': 491,\n"," 'उम्मीद': 286,\n"," 'कॉकर': 3188,\n"," 'प्रबन्धक': 3616,\n"," 'स्वागत': 700,\n"," 'धान': 3476,\n"," 'रोकथाम': 3908,\n"," 'भरते': 3738,\n"," 'लिमो': 2700,\n"," 'पात्रता': 1970,\n"," 'कोर्ट': 721,\n"," 'जगत': 896,\n"," 'दिवंगत': 3451,\n"," 'करवा': 3132,\n"," 'चालीसा': 2367,\n"," 'घुमार': 3276,\n"," 'मस्जिद': 2628,\n"," 'कहने': 3144,\n"," 'नॉर्थ': 2498,\n"," 'रात': 592,\n"," 'राष्ट्रों': 2680,\n"," 'गरीबी': 3234,\n"," 'केरोसिन': 530,\n"," 'दस्तावेज': 1602,\n"," 'वंचकों': 3951,\n"," 'परियोजना': 1182,\n"," 'निर्मला': 2492,\n"," 'अग्निशामक': 1494,\n"," 'कांग्रेस': 169,\n"," 'जाएगी': 536,\n"," 'अदालत': 708,\n"," 'हाउस': 964,\n"," 'गांवों': 1563,\n"," 'नेवाडा': 1615,\n"," 'शक्तिशाली': 2738,\n"," 'एसपी': 3101,\n"," 'एन्ड्रू': 2263,\n"," 'राज्य': 162,\n"," 'बताना': 1999,\n"," 'योग्य': 2658,\n"," 'लॉन्च': 2709,\n"," 'अधिवक्ता': 780,\n"," 'आधा': 1793,\n"," 'नारायण': 2486,\n"," 'अधिकार': 863,\n"," 'पैरी': 2543,\n"," 'प्रतिनिधि': 743,\n"," 'भयंकर': 3736,\n"," 'उत्पीड़न': 1523,\n"," 'वीके': 3994,\n"," 'हिलाने': 4162,\n"," 'पदार्थों': 3546,\n"," 'समझने': 2778,\n"," 'पक्षी': 1618,\n"," 'विपक्षी': 2079,\n"," 'खड़े': 1848,\n"," 'ईवनिंग': 3036,\n"," 'नेशनल': 1616,\n"," 'संबोधित': 1086,\n"," 'विधायकों': 2723,\n"," 'दुबई': 3456,\n"," 'निगरानी': 246,\n"," 'इसी': 459,\n"," 'लाने': 837,\n"," 'लिखा': 2068,\n"," 'पोस्ट': 1381,\n"," 'पड़ने': 2503,\n"," 'डायरेक्टर': 1909,\n"," 'आएगा': 2219,\n"," 'सब': 696,\n"," 'क्रम्प': 3198,\n"," 'पॉल्बी': 1186,\n"," 'परिषद': 1368,\n"," 'यात्री': 510,\n"," 'पॉनब्रोकर्स': 2545,\n"," '),': 2169,\n"," 'नेटवर्क': 914,\n"," 'एपीनेफ्राइन': 3083,\n"," 'लिखने': 3930,\n"," 'राजस्व': 938,\n"," '[UNK]': 0,\n"," 'लागत': 1223,\n"," 'इनकार': 1290,\n"," 'कार': 526,\n"," 'ऊंचाई': 1528,\n"," 'निर्दलीय': 3509,\n"," 'डी': 337,\n"," 'किमी': 2300,\n"," 'अन्दर': 2205,\n"," 'पंचायतों': 3535,\n"," 'डिट्टा': 384,\n"," 'जोधपुर': 670,\n"," '\"।': 859,\n"," 'अन्तर्गत': 709,\n"," 'जिले': 1012,\n"," 'फायरिंग': 1647,\n"," '34': 2878,\n"," 'थाना': 3430,\n"," 'दि': 1021,\n"," 'नगरपालिकाओं': 3483,\n"," 'भले': 1672,\n"," 'पामेला': 1971,\n"," 'छोड़ना': 2379,\n"," 'साय': 4078,\n"," 'शपथ': 771,\n"," 'शक्ति': 1239,\n"," 'टोनी': 2415,\n"," 'क्रिस': 2319,\n"," 'रजिस्ट्री': 1213,\n"," 'मामूली': 2634,\n"," 'मित्रों': 3806,\n"," 'लाई': 3926,\n"," 'पेरिस': 3590,\n"," 'लक्षण': 2688,\n"," 'विशिष्ट': 1714,\n"," 'संवेदनशील': 1725,\n"," 'उपायुक्त': 1816,\n"," 'गत': 3232,\n"," 'शर्तों': 4012,\n"," 'व्यक्तियों': 448,\n"," 'उत्तर': 426,\n"," 'जानी': 1887,\n"," 'पड़ता': 1363,\n"," 'पीएमटी': 2532,\n"," 'जॉर्जेस': 1585,\n"," 'निर्माता': 911,\n"," 'निर्देशित': 3512,\n"," 'दिखा': 2460,\n"," 'रहमान': 2668,\n"," 'टेस्टोस्टेरॉन': 3373,\n"," 'अमेरिकी': 144,\n"," 'घुसपैठ': 3277,\n"," 'जवानों': 3320,\n"," 'बाहर': 479,\n"," 'ट्रिलियन': 3381,\n"," 'गुप्ता': 1862,\n"," 'ड्राइव': 1913,\n"," 'खतरा': 3215,\n"," 'समझौता': 954,\n"," 'लगी': 1068,\n"," 'सुझाव': 1259,\n"," 'राम': 2057,\n"," 'चुनावकर्ता': 3294,\n"," 'मना': 932,\n"," 'शनिवार': 4008,\n"," 'मंजूर': 3749,\n"," 'इंतजार': 1514,\n"," 'वाणिज्यिक': 1710,\n"," 'एकत्रित': 3071,\n"," 'अस्वीकार': 2976,\n"," 'पूरे': 680,\n"," 'विपक्ष': 1712,\n"," 'डेस्क': 2426,\n"," 'प्रतिद्वंद्वी': 2551,\n"," 'ग्राउंड': 2352,\n"," 'मुखबिर': 3818,\n"," 'छात्राओं': 1878,\n"," 'बैग': 823,\n"," 'सर्वे': 4068,\n"," 'ट्रांसमिशन': 1906,\n"," 'अविश्वसनीय': 2964,\n"," 'भेजे': 2612,\n"," 'संपर्क': 2756,\n"," 'अफ्रीकी': 1784,\n"," 'वोट': 1079,\n"," 'कॉमेडियन': 2310,\n"," 'झंडी': 3357,\n"," 'नाराज': 2485,\n"," 'विनियमन': 2078,\n"," 'ब्यूरो': 3725,\n"," 'रैना': 3905,\n"," 'लोड': 3947,\n"," 'नीता': 1031,\n"," 'पोल': 2546,\n"," 'आक्रोश': 2221,\n"," 'खरीद': 1142,\n"," 'लगा': 370,\n"," 'मशहूर': 3770,\n"," 'विरल': 3988,\n"," 'खुलासा': 886,\n"," 'कथन': 2278,\n"," 'यौंडे': 2052,\n"," 'छुट्टी': 1150,\n"," 'निरीक्षण': 1613,\n"," 'जिन': 1334,\n"," 'बैमफोर्ड': 3718,\n"," 'मूरे': 2647,\n"," 'औद्योगिक': 2273,\n"," 'जाहिर': 3329,\n"," 'परन्तु': 2512,\n"," 'प्रेरणा': 3630,\n"," 'रुपए': 764,\n"," 'चार्टर्ड': 3289,\n"," 'उल्लंघन': 568,\n"," 'संबंधी': 1453,\n"," 'संभवतः': 847,\n"," 'बुरी': 2596,\n"," 'जननांग': 798,\n"," 'डाले': 805,\n"," 'एडिस': 2261,\n"," 'जिसके': 355,\n"," 'आरोपों': 975,\n"," 'नजर': 677,\n"," 'इजरायली': 3020,\n"," 'ऑफिस': 1827,\n"," 'जाहिद': 1010,\n"," 'समस्याओं': 1250,\n"," 'आपको': 655,\n"," 'करना': 84,\n"," 'प्रोफेसर': 746,\n"," 'अंसारी': 2906,\n"," 'अनुशासन': 2924,\n"," 'ध्वस्त': 3481,\n"," 'कुजूर': 1838,\n"," 'निशान': 3516,\n"," 'सोरेनसेन': 4119,\n"," 'हादसे': 4156,\n"," 'तुलना': 431,\n"," 'सुबह': 417,\n"," 'हाल': 227,\n"," 'कॉल': 1842,\n"," 'टेस्ट': 2414,\n"," 'जताया': 3315,\n"," 'ऑन': 569,\n"," 'जल्दी': 1883,\n"," 'याहू': 1421,\n"," 'काले': 2298,\n"," 'बर्मिंघम': 3682,\n"," 'इतना': 2236,\n"," 'फेलोशिप': 2569,\n"," 'BfV': 1773,\n"," 'कम्पनियों': 876,\n"," 'हरी': 2847,\n"," 'एंड': 3067,\n"," 'छात्रों': 469,\n"," 'विधानसभा': 767,\n"," 'विद': 3977,\n"," 'ऐसा': 141,\n"," 'स्टैन्डअप': 2831,\n"," 'दाएं': 1923,\n"," 'मिलेगा': 2038,\n"," 'तरीकों': 3414,\n"," 'इकबाल': 3018,\n"," 'दावेदार': 1925,\n"," 'भूमि': 3745,\n"," 'एडवोकेट': 1820,\n"," 'रहस्यो': 3873,\n"," 'बंदरगाह': 3659,\n"," 'मार': 3792,\n"," 'सम्पूर्ण': 4062,\n"," 'बुढ़ापे': 3699,\n"," 'वीजा': 1719,\n"," 'जिलाधिकारी': 2398,\n"," 'सर्वर': 2787,\n"," 'बंद': 224,\n"," 'उत्पादकता': 3047,\n"," 'मंडी': 2024,\n"," 'प्रोफाइल': 3633,\n"," 'दिखाई': 3446,\n"," 'बनर्जी': 3677,\n"," 'जागरूक': 1886,\n"," 'अव्यवहारिक': 2965,\n"," 'पीछा': 2533,\n"," 'मान्यता': 2034,\n"," 'उबाऊ': 3057,\n"," 'खरीदे': 1560,\n"," '2': 285,\n"," 'बेले': 3712,\n"," 'विद्रोही': 3983,\n"," 'निराशा': 1612,\n"," 'दर्शाता': 1601,\n"," 'महीने': 267,\n"," 'शब्दों': 4009,\n"," 'साथियों': 2118,\n"," 'गंभीर': 998,\n"," 'Frontier': 453,\n"," 'निकल': 2487,\n"," 'मेयर': 1681,\n"," 'सिगरेट': 4081,\n"," 'यहां': 296,\n"," 'पहला': 815,\n"," 'मिठाई': 1062,\n"," 'शायद': 1446,\n"," 'परिसर': 1369,\n"," 'एसए': 3099,\n"," 'संस्थान': 1246,\n"," 'अभियोग': 1499,\n"," 'स्पेशल': 2836,\n"," 'नीलामी': 3520,\n"," 'पश्चिमी': 1371,\n"," 'उठने': 1807,\n"," 'चाय': 3287,\n"," 'पैदल': 3593,\n"," 'रूपरेखा': 3900,\n"," 'व्यापारी': 844,\n"," 'ऋण': 786,\n"," 'पार्षदों': 1375,\n"," 'चीनी': 1873,\n"," 'कागजात': 2292,\n"," 'कराया': 984,\n"," 'प्रतिनिधित्व': 3602,\n"," 'रह': 937,\n"," 'मांगी': 2633,\n"," 'औपचारिक': 1297,\n"," 'टिकटें': 3366,\n"," 'दबाव': 1162,\n"," 'प्रभाग': 3617,\n"," 'रक्षा': 1690,\n"," 'नौशहरा': 1955,\n"," 'अवधि': 1117,\n"," 'अभी': 180,\n"," 'हवाला': 2156,\n"," 'चौथी': 3306,\n"," 'देखभाल': 809,\n"," 'संशोधित': 1726,\n"," 'भेंट': 2611,\n"," 'दोस्तों': 3468,\n"," 'संचार': 2096,\n"," 'नयी': 2477,\n"," 'द्वारा': 39,\n"," 'फोन': 174,\n"," 'पाकिस्तान': 1373,\n"," 'हों': 1267,\n"," 'श्री': 101,\n"," 'भंडार': 3732,\n"," 'था': 31,\n"," 'प्रतियोगिता': 1634,\n"," ']': 2187,\n"," 'ऐर्शो': 3104,\n"," 'प्रवास': 3618,\n"," 'पोप': 583,\n"," 'यॉर्क': 1687,\n"," 'नागरिकों': 1026,\n"," 'नाराजगी': 3499,\n"," 'जनरल': 1574,\n"," 'तरीका': 1594,\n"," 'अधिवक्ताओं': 2199,\n"," 'संगीत': 4029,\n"," 'क्लिक': 3208,\n"," 'जानबूझकर': 2396,\n"," 'असहमति': 2970,\n"," 'सुप्रीम': 2127,\n"," 'करताहै': 3130,\n"," 'भेजी': 1404,\n"," 'मारे': 933,\n"," 'लेकर': 159,\n"," 'चली': 1870,\n"," 'प्रस्तुति': 2560,\n"," 'ही': 34,\n"," 'पढ़ने': 1958,\n"," 'अभिभावकों': 2207,\n"," 'सबक': 2775,\n"," 'डेटा': 730,\n"," 'अमेरिका': 201,\n"," 'मुकेश': 1416,\n"," 'उन्हे': 1526,\n"," '?\"': 1772,\n"," 'पश्चात': 103,\n"," 'ऑब्जर्वेटरी': 3107,\n"," 'भाव': 1401,\n"," 'सुनाई': 2125,\n"," 'गेंदबाजी': 2347,\n"," 'विद्युत': 1232,\n"," 'पॉन': 3594,\n"," 'करोड़ों': 985,\n"," 'चुने': 1004,\n"," 'लालच': 2067,\n"," 'सदस्य': 450,\n"," 'नज़र': 1940,\n"," 'केबिन': 1304,\n"," 'आकर्षक': 2981,\n"," 'पिछली': 1626,\n"," 'विस्तार': 1716,\n"," 'ग्राहकों': 794,\n"," 'व्यापक': 950,\n"," 'सलाहकारों': 2789,\n"," 'एफआइएसए': 3084,\n"," 'एशेज': 3098,\n"," 'लाल': 1225,\n"," 'निरस्तीकरण': 3505,\n"," 'प्रतिमा': 3606,\n"," 'रखें': 3865,\n"," 'सेवाओं': 1466,\n"," 'खदान': 1558,\n"," 'ज़्यादातर': 3324,\n"," 'रोशन': 3916,\n"," 'शांत': 4016,\n"," 'अनधिकृत': 2919,\n"," 'पीने': 1975,\n"," 'स्तरीय': 4130,\n"," 'आपूर्तियों': 2997,\n"," 'स्वेटर': 4147,\n"," 'दशकों': 2454,\n"," 'बेहतरीन': 1055,\n"," 'बतायाः': 1197,\n"," 'जिम्मेदारी': 1891,\n"," 'पृथक': 1978,\n"," 'घटना': 724,\n"," 'पोस्टमार्टम': 1982,\n"," 'आती': 1122,\n"," 'धारा': 2473,\n"," 'आइस': 1506,\n"," '30': 567,\n"," 'कियागया': 3165,\n"," 'सवार': 2790,\n"," 'चेयरमैन': 1569,\n"," 'ब्रांच': 3727,\n"," 'स्टेशनों': 4125,\n"," 'विवादित': 2726,\n"," 'फाई': 2564,\n"," 'सेना': 1098,\n"," '--': 2867,\n"," 'राष्ट्रीय': 196,\n"," 'जीतना': 3336,\n"," 'डीएवीपी': 3400,\n"," 'यूनिट': 3856,\n"," 'मुकाबले': 3816,\n"," 'गरीब': 2335,\n"," 'शादी': 1241,\n"," 'बांझ': 3687,\n"," 'चलता': 1868,\n"," 'गलतफहमी': 3238,\n"," 'चंद': 1866,\n"," 'तिमाही': 1018,\n"," 'खर्च': 427,\n"," 'जीप': 3338,\n"," '£': 2189,\n"," 'प्रस्ताव': 544,\n"," 'पर्यटक': 2513,\n"," 'खुराक': 2328,\n"," 'रस्सी': 3870,\n"," 'घायल': 1002,\n"," 'क्लार्क': 1847,\n"," 'धमाकों': 2472,\n"," 'युद्ध': 629,\n"," 'पॉलिसी': 3595,\n"," 'किसानों': 3171,\n"," 'गिरोह': 1314,\n"," 'जॉर्जिया': 1895,\n"," 'हु': 4166,\n"," 'कॉन्फ्रेंस': 3189,\n"," 'अनुसार': 107,\n"," 'नॉरटेल': 1617,\n"," 'दायर': 2456,\n"," 'लैंगिक': 2707,\n"," 'अमीरथलिंगा': 2209,\n"," 'समर्थक': 4054,\n"," 'लगभग': 148,\n"," 'संपूर्ण': 4035,\n"," '-': 21,\n"," 'सदन': 4043,\n"," 'लालटेन': 3928,\n"," 'जिम्मा': 3334,\n"," 'डाउन': 1339,\n"," 'संसद': 1087,\n"," 'मेडिकल': 830,\n"," 'कार्डिनल्स': 571,\n"," 'नवजात': 1941,\n"," 'रंग': 1688,\n"," 'धूप': 3478,\n"," 'राहत': 3885,\n"," 'थोड़े': 1918,\n"," 'पृथ्वीपुर': 3584,\n"," 'केरी': 1140,\n"," 'मंगलवार': 3747,\n"," 'कद्दू': 1536,\n"," 'जहां': 244,\n"," 'सोने': 1101,\n"," 'घरेलू': 533,\n"," 'बताया': 119,\n"," 'देख': 904,\n"," 'मुन्ना': 2043,\n"," 'खोला': 2330,\n"," 'बताती': 2578,\n"," 'रहेंगी': 3876,\n"," 'सर्वश्रेष्ठ': 2788,\n"," 'रेज': 3903,\n"," 'एयरलाइंस': 875,\n"," 'कृष्ण': 1839,\n"," 'ट्रैकिंग': 3385,\n"," 'कैटी': 1306,\n"," 'सुनहरी': 2815,\n"," 'जीता': 2400,\n"," 'कर': 28,\n"," 'गुजरने': 2344,\n"," 'बगीचे': 3663,\n"," 'मुद्दे': 1063,\n"," 'दिवाली': 3452,\n"," 'थाने': 2447,\n"," 'फीसदी': 1993,\n"," 'कर्तव्य': 3138,\n"," 'रहते': 1067,\n"," 'आनंद': 783,\n"," 'नरेश': 3491,\n"," 'एलर्जिक': 3092,\n"," 'आता': 1121,\n"," 'आज': 216,\n"," 'ताकत': 3419,\n"," 'चीजों': 1322,\n"," 'जंबो': 3314,\n"," 'रहेंगे': 1692,\n"," 'पार्क': 1973,\n"," 'पेश': 340,\n"," 'कूपर': 3176,\n"," 'मिस्र': 935,\n"," 'पहने': 3560,\n"," '1000': 1754,\n"," 'नवीनीकरण': 3493,\n"," 'बनाता': 2003,\n"," 'शिक्षा': 599,\n"," 'भीड़': 930,\n"," 'अन्तराल': 2204,\n"," 'हलवा': 2849,\n"," 'ब्रुक्स': 3730,\n"," 'व्यवसायिक': 4003,\n"," 'दिख': 2459,\n"," 'निरस्त': 1174,\n"," 'एन': 217,\n"," 'बाल': 266,\n"," 'पहुंची': 2521,\n"," 'मानदण्ड': 3787,\n"," 'मजबूर': 2618,\n"," 'बारे': 49,\n"," 'रैली': 595,\n"," 'खोज': 887,\n"," 'दलों': 1920,\n"," 'गोपनीय': 3259,\n"," 'नकली': 1939,\n"," 'बैठे': 3717,\n"," 'फिल्म': 1648,\n"," 'हुए': 51,\n"," 'गुलाम': 2345,\n"," 'मान्य': 3791,\n"," 'नगरपालिका': 1354,\n"," 'गर्मी': 2336,\n"," 'बदल': 747,\n"," 'पर्यावरणीय': 3558,\n"," 'डिग्री': 806,\n"," 'पेन': 3587,\n"," 'बनाने': 281,\n"," 'आबिद': 3000,\n"," 'हक': 4150,\n"," 'समान': 1732,\n"," 'तैयार': 321,\n"," 'ग्रुप': 1001,\n"," 'सुनाया': 2816,\n"," 'विनियामक': 944,\n"," 'बजा': 3665,\n"," 'राज्यस्तरीय': 2676,\n"," 'उद्योग': 978,\n"," 'शेरिफ': 2747,\n"," 'प्रेस': 1042,\n"," 'आगमन': 2984,\n"," 'दान': 1924,\n"," 'जेलिफ़िश': 1893,\n"," 'श्रेणी': 1724,\n"," 'निवेश': 1029,\n"," 'समस्या': 641,\n"," 'वाहनों': 1435,\n"," 'पीएम': 2531,\n"," 'निर्देश': 579,\n"," 'पुरुष': 543,\n"," 'अंतर्निहित': 2902,\n"," 'देर': 1166,\n"," 'यापन': 2654,\n"," 'पंजाब': 1180,\n"," 'कल्याण': 2289,\n"," 'उतरते': 1809,\n"," 'नदियों': 3488,\n"," 'बस': 2582,\n"," 'पीडीएस': 1629,\n"," 'आंकड़ों': 2978,\n"," 'उत्सव': 1291,\n"," 'कमेटी': 2282,\n"," 'राष्ट्रपति': 259,\n"," 'ऊपर': 494,\n"," 'मार्थेल': 3797,\n"," 'तलाश': 1595,\n"," 'अतिथियों': 2912,\n"," 'मोटोरोला': 3838,\n"," 'बजे': 506,\n"," '13': 965,\n"," 'मैच': 1419,\n"," 'युवराज': 2657,\n"," 'चीफ': 1874,\n"," 'गये': 532,\n"," 'कुछ': 74,\n"," 'निजी': 814,\n"," 'हुई': 93,\n"," 'नमो': 473,\n"," 'समीक्षा': 1733,\n"," 'अनुसूची': 2203,\n"," 'कार्य': 256,\n"," 'कार्रवाई': 527,\n"," 'ट्रैवल': 1338,\n"," 'प्रसारण': 2559,\n"," 'मिलियन': 207,\n"," 'वांशिगटन': 2713,\n"," 'जुड़वां': 3343,\n"," 'UBS': 2185,\n"," 'दुरभिसंधि': 3457,\n"," 'सीटीआर': 4088,\n"," 'प्रदान': 153,\n"," 'लगातार': 632,\n"," 'जैक': 1894,\n"," 'कब्जा': 1537,\n"," 'धमकियों': 3473,\n"," 'बीमार': 3696,\n"," 'दिशा': 338,\n"," 'खत्म': 1557,\n"," 'विधि': 1711,\n"," 'वो': 1444,\n"," 'सोडा': 2826,\n"," 'ऊंची': 3062,\n"," 'कुटा': 3173,\n"," 'असद': 1787,\n"," 'व्यापार': 557,\n"," 'उठा': 1520,\n"," 'विस्फोटों': 2085,\n"," 'रिहाई': 3896,\n"," 'अनुग्रह': 2923,\n"," 'बढ़ोतरी': 2577,\n"," 'राज्यपाल': 2675,\n"," 'दीर्घकालिक': 3455,\n"," 'मेरीलैंड': 3828,\n"," 'निवेशक': 2495,\n"," 'कंठशोथ': 3115,\n"," 'कार्यकाल': 1541,\n"," 'पहुंचाया': 2520,\n"," 'पावर': 2528,\n"," 'में': 6,\n"," 'उल्लेख': 2253,\n"," 'सायबर': 2803,\n"," 'सईद': 2764,\n"," 'काउन्टी': 3149,\n"," 'साइट': 4074,\n"," 'नियम': 1356,\n"," 'सौभाग्यपूर्ण': 4121,\n"," 'जिनकी': 1890,\n"," 'मौजूद': 628,\n"," 'सीमेंट': 4091,\n"," 'शासन': 1447,\n"," 'आग्रह': 2222,\n"," 'लम्बाई': 2693,\n"," 'अल': 2953,\n"," 'फंसा': 3636,\n"," 'नौ': 1360,\n"," 'चेहरे': 1325,\n"," 'उपनिदेशक': 3051,\n"," 'दफ्तर': 3436,\n"," 'द्वार': 3471,\n"," 'सेवा': 199,\n"," 'वाइफ': 3963,\n"," 'संकट': 1082,\n"," 'मुकुल': 3817,\n"," 'एल': 2267,\n"," 'मानना': 482,\n"," 'दिखाते': 3447,\n"," 'नियुक्त': 1947,\n"," 'विकल्पों': 2717,\n"," 'कोर्टीन': 2314,\n"," 'भाग': 507,\n"," 'सकेंगे': 1457,\n"," 'दर्शन': 3438,\n"," 'रहस्योद्धाटन': 3875,\n"," 'बढ़त': 3668,\n"," 'स्टाफ': 961,\n"," 'फोटो': 925,\n"," 'मताधिकार': 3759,\n"," 'सात': 958,\n"," 'स्टेशन': 2140,\n"," 'कार्पोरेशन': 3157,\n"," 'सिब्बल': 2809,\n"," 'प्रत्याशित': 3610,\n"," 'स्वास्थ्य': 263,\n"," 'मेने': 3827,\n"," 'तैयारी': 2443,\n"," 'पूरी': 293,\n"," 'गणित': 1312,\n"," 'मात्रा': 2032,\n"," 'करदाता': 3131,\n"," 'उत्पादों': 3048,\n"," 'सके': 953,\n"," 'महंगी': 2629,\n"," 'मत': 2026,\n"," 'संरक्षक': 2758,\n"," 'अगवानी': 2908,\n"," 'अत्यधिक': 779,\n"," 'पद्धतियों': 3548,\n"," 'पार्षद': 1625,\n"," 'द्रुत': 2470,\n"," 'नियंत्रित': 1028,\n"," 'वाणिज्य': 1709,\n"," 'दस्ता': 3440,\n"," 'पुनः': 1040,\n"," 'प्रत्याशियों': 2554,\n"," 'मिलनी': 3809,\n"," 'सभा': 1731,\n"," 'एंटी': 2254,\n"," 'हरकत': 2845,\n"," 'बहाल': 2584,\n"," 'टिके': 3368,\n"," 'प्रसिद्धि': 3620,\n"," 'स्वैच्छिक': 4148,\n"," 'ओबामाकेयर': 3110,\n"," 'मुजाहिदीन': 3819,\n"," 'पेट्रोलियम': 3586,\n"," 'प्रशासन': 363,\n"," 'ओपनिंग': 2271,\n"," 'जाँच': 2394,\n"," 'रहा': 47,\n"," 'आ': 215,\n"," 'रेडियो': 1429,\n"," 'साझेदारी': 2799,\n"," 'लाख': 596,\n"," '’': 1480,\n"," 'कच्चे': 2275,\n"," 'सुसंगत': 2819,\n"," 'आतंरिक': 2991,\n"," 'पीढ़ी': 3578,\n"," 'स्पैनील': 4135,\n"," 'बाएं': 2585,\n"," 'इंकार': 1512,\n"," 'मेक्सिको': 2649,\n"," 'नेताओं': 542,\n"," 'बढ़ते': 1196,\n"," 'खोजने': 3224,\n"," 'संयुक्त': 560,\n"," 'अशोक': 1280,\n"," 'दर्शकों': 2450,\n"," 'दीवाली': 1163,\n"," 'रिकवरी': 3886,\n"," 'प्रस्तुत': 436,\n"," 'बैंकों': 1665,\n"," 'एकतरफा': 3069,\n"," 'बीती': 3695,\n"," 'गणना': 3231,\n"," ...}"]},"execution_count":8,"metadata":{},"output_type":"execute_result"}],"source":["tokenizer_tgt.get_vocab()"]},{"cell_type":"code","execution_count":9,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":471,"status":"ok","timestamp":1728910840024,"user":{"displayName":"Manish Negi","userId":"13518611288471091944"},"user_tz":-330},"id":"kqDZovjyHSLO","outputId":"48b339da-50f0-4dad-d4c5-283dc10c289b"},"outputs":[{"data":{"text/plain":["{'translation': {'en': 'A black box in your car?',\n","  'hi': 'आपकी कार में ब्लैक बॉक्स?'}}"]},"execution_count":9,"metadata":{},"output_type":"execute_result"}],"source":["test_data[0]"]},{"cell_type":"code","execution_count":10,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":522,"status":"ok","timestamp":1728913384203,"user":{"displayName":"Manish Negi","userId":"13518611288471091944"},"user_tz":-330},"id":"Uiw5UVaMJrEw","outputId":"a9d22556-df3c-4172-c818-1ad4fafa39f4"},"outputs":[{"data":{"text/plain":["tensor([[[ True, False, False],\n","         [ True,  True, False],\n","         [ True,  True,  True]]])"]},"execution_count":10,"metadata":{},"output_type":"execute_result"}],"source":["import torch\n","import torch.nn as nn\n","from torch.utils.data import Dataset\n","\n","class EnToHinDataset(Dataset):\n","\n","    def __init__(self, ds, tk_src, tk_tgt, seq_len):\n","        super().__init__()\n","        self.seq_len = seq_len\n","\n","        self.ds = ds\n","        self.tk_src = tk_src\n","        self.tk_tgt = tk_tgt\n","\n","        # Special tokens\n","        self.sos = torch.tensor([tk_tgt.token_to_id(\"[SOS]\")], dtype=torch.int64)\n","        self.eos = torch.tensor([tk_tgt.token_to_id(\"[EOS]\")], dtype=torch.int64)\n","        self.pad = torch.tensor([tk_tgt.token_to_id(\"[PAD]\")], dtype=torch.int64)\n","\n","    def __len__(self):\n","        return len(self.ds)\n","\n","    def __getitem__(self, idx):\n","        # Get the source and target text from the dataset\n","        dic = self.ds[idx]\n","        src_text = dic['translation']['en']\n","        tgt_text = dic['translation']['hi']\n","\n","        # Tokenize the source and target text\n","        en = self.tk_src.encode(src_text).ids\n","        de = self.tk_tgt.encode(tgt_text).ids\n","\n","        # Calculate the number of padding tokens needed\n","        enc_pad_len = self.seq_len - len(en) - 2  # for <sos> and <eos>\n","        dec_pad_len = self.seq_len - len(de) - 1  # only <sos> at the beginning\n","\n","        # Check if the sentence is too long\n","        if enc_pad_len < 0 or dec_pad_len < 0:\n","            raise ValueError(\"Sentence is too long\")\n","\n","        # Create the encoder input by adding <sos>, <eos>, and padding\n","        en_inp = torch.cat([\n","            self.sos,\n","            torch.tensor(en, dtype=torch.int64),\n","            self.eos,\n","            torch.tensor([self.pad] * enc_pad_len, dtype=torch.int64)\n","        ])\n","\n","        # Create the decoder input by adding <sos> and padding\n","        de_inp = torch.cat([\n","            self.sos,\n","            torch.tensor(de, dtype=torch.int64),\n","            torch.tensor([self.pad] * dec_pad_len, dtype=torch.int64)\n","        ])\n","\n","        # Create the label by adding <eos> at the end and padding\n","        label = torch.cat([\n","            torch.tensor(de, dtype=torch.int64),\n","            self.eos,\n","            torch.tensor([self.pad] * dec_pad_len, dtype=torch.int64)\n","        ])\n","\n","        # Return a dictionary containing the inputs and labels\n","        return {\n","            \"encoder_input\": en_inp,  # Encoder input\n","            \"decoder_input\": de_inp,  # Decoder input\n","            \"label\": label,  # Target labels\n","            \"encoder_mask\": (en_inp != self.pad).unsqueeze(0).unsqueeze(0).int(), # (1, 1, seq_len)\n","            \"decoder_mask\": (de_inp != self.pad).unsqueeze(0).int() & causal_mask(de_inp.size(0)), # (1, seq_len) & (1, seq_len, seq_len),\n","            \"src_text\": src_text,\n","            \"tgt_text\": tgt_text,\n","        }\n","\n","\n","def causal_mask(size):\n","    mask = torch.triu(torch.ones((1, size, size)), diagonal=1).type(torch.int)\n","    return mask == 0\n","\n","causal_mask(3)"]},{"cell_type":"code","execution_count":11,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3,"status":"ok","timestamp":1728913384684,"user":{"displayName":"Manish Negi","userId":"13518611288471091944"},"user_tz":-330},"id":"U25uQzNOJ8a6","outputId":"c9b78a16-ecd8-4808-be9a-3bf34317277f"},"outputs":[{"data":{"text/plain":["Dataset({\n","    features: ['translation'],\n","    num_rows: 2507\n","})"]},"execution_count":11,"metadata":{},"output_type":"execute_result"}],"source":["test_data"]},{"cell_type":"code","execution_count":12,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1728913018848,"user":{"displayName":"Manish Negi","userId":"13518611288471091944"},"user_tz":-330},"id":"VPW3dDBtON2p"},"outputs":[],"source":["from torch.utils.data import Dataset, DataLoader, random_split\n","# Train and test splits\n","data = test_data\n","train_ds_size = int(0.9 * len(data))\n","val_ds_size = len(data) - train_ds_size\n","train_data, val_data = random_split(data, [train_ds_size, val_ds_size])"]},{"cell_type":"code","execution_count":13,"metadata":{"executionInfo":{"elapsed":653,"status":"ok","timestamp":1728912557653,"user":{"displayName":"Manish Negi","userId":"13518611288471091944"},"user_tz":-330},"id":"l_QOYeh5J8g1"},"outputs":[],"source":["# Build tokenizers\n","tokenizer_src = get_or_build_tokenizer(config, test_data, 'en')\n","tokenizer_tgt = get_or_build_tokenizer(config, test_data, 'hi')"]},{"cell_type":"code","execution_count":14,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":501,"status":"ok","timestamp":1728912026396,"user":{"displayName":"Manish Negi","userId":"13518611288471091944"},"user_tz":-330},"id":"4AQ7lFucLoob","outputId":"5b726862-4b91-4993-b415-a5cea237bfe5"},"outputs":[{"name":"stdout","output_type":"stream","text":["Max length of source sentence: 90\n","Max length of target sentence: 92\n"]}],"source":["max_len_src = 0\n","max_len_tgt = 0\n","for item in test_data:\n","  src_ids = tokenizer_src.encode(item['translation']['en']).ids\n","  tgt_ids = tokenizer_tgt.encode(item['translation']['hi']).ids\n","  max_len_src = max(max_len_src, len(src_ids))\n","  max_len_tgt = max(max_len_tgt, len(tgt_ids))\n","print(f'Max length of source sentence: {max_len_src}')\n","print(f'Max length of target sentence: {max_len_tgt}')\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GBLX0rBnOKv8"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":15,"metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1728913388499,"user":{"displayName":"Manish Negi","userId":"13518611288471091944"},"user_tz":-330},"id":"7-8d9TxhJ8js"},"outputs":[],"source":["seq_len = 100\n","\n","t_ds = EnToHinDataset(train_data, tokenizer_src, tokenizer_tgt, 100)\n","v_ds = EnToHinDataset(val_data, tokenizer_src, tokenizer_tgt, 100)"]},{"cell_type":"code","execution_count":16,"metadata":{"executionInfo":{"elapsed":6,"status":"ok","timestamp":1728913388982,"user":{"displayName":"Manish Negi","userId":"13518611288471091944"},"user_tz":-330},"id":"RibVyqkbJ8mR"},"outputs":[],"source":["from torch.utils.data import DataLoader\n","\n","t_dl = DataLoader(t_ds, batch_size=16, shuffle=True)\n","v_dl = DataLoader(v_ds, batch_size=16, shuffle=True)"]},{"cell_type":"code","execution_count":17,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5,"status":"ok","timestamp":1728913388982,"user":{"displayName":"Manish Negi","userId":"13518611288471091944"},"user_tz":-330},"id":"v1e1SjtOMYEE","outputId":"759cab0f-6168-46d2-f057-a81c7cc90a1b"},"outputs":[{"data":{"text/plain":["{'encoder_input': tensor([   2, 1001,    8,    4, 4365,    0, 1808,    6,  199,    4,  414,  418,\n","            7,  101,    4, 2574,    0, 2750,   34, 3679,  107, 2092,    9,    4,\n","          578, 2632,  498,   43, 4074,    6,   13,  616,  482,  707,   80,    4,\n","          625,    5,    3,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n","            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n","            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n","            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n","            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n","            1,    1,    1,    1]),\n"," 'decoder_input': tensor([   2, 1371, 3972,    9,    0,    4,   85,  947, 3411,   34, 2715,    4,\n","            0,  870,    6, 2159,   73, 1945,  870,    6, 1051,   14, 1352,  164,\n","           40,   33,    7,   36, 1166,  637,   58,  128,   47,    5,    1,    1,\n","            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n","            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n","            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n","            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n","            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n","            1,    1,    1,    1]),\n"," 'label': tensor([1371, 3972,    9,    0,    4,   85,  947, 3411,   34, 2715,    4,    0,\n","          870,    6, 2159,   73, 1945,  870,    6, 1051,   14, 1352,  164,   40,\n","           33,    7,   36, 1166,  637,   58,  128,   47,    5,    3,    1,    1,\n","            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n","            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n","            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n","            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n","            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n","            1,    1,    1,    1]),\n"," 'encoder_mask': tensor([[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n","           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","           0, 0, 0, 0, 0, 0, 0, 0]]], dtype=torch.int32),\n"," 'decoder_mask': tensor([[[1, 0, 0,  ..., 0, 0, 0],\n","          [1, 1, 0,  ..., 0, 0, 0],\n","          [1, 1, 1,  ..., 0, 0, 0],\n","          ...,\n","          [1, 1, 1,  ..., 0, 0, 0],\n","          [1, 1, 1,  ..., 0, 0, 0],\n","          [1, 1, 1,  ..., 0, 0, 0]]], dtype=torch.int32),\n"," 'src_text': 'Due to the western disturbance activity, since the early hours of Thursday the higher hilly regions have experienced some snowfall and the low lying areas had rainfall, that continued until late into the evening.',\n"," 'tgt_text': 'पश्चिमी विक्षोभ की सक्रियता के कारण वीरवार तड़के ही वादी के उच्चपर्वतीय इलाकों में हिमपात व निचले इलाकों में बारिश का दौर शुरू हो गया, जो देर शाम तक जारी रहा।'}"]},"execution_count":17,"metadata":{},"output_type":"execute_result"}],"source":["for i in t_ds:\n","  break\n","i"]},{"cell_type":"code","execution_count":18,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":509,"status":"ok","timestamp":1728913410310,"user":{"displayName":"Manish Negi","userId":"13518611288471091944"},"user_tz":-330},"id":"xCqjIZPNMYHF","outputId":"08db2d89-0057-4288-c40a-78ea079764af"},"outputs":[{"name":"stdout","output_type":"stream","text":["torch.Size([16, 100])\n","torch.Size([16, 100])\n","torch.Size([16, 100])\n","torch.Size([16, 1, 1, 100])\n","torch.Size([16, 1, 100, 100])\n"]}],"source":["for i in t_dl:\n","  print(i['encoder_input'].shape)\n","  print(i['decoder_input'].shape)\n","  print(i['label'].shape)\n","  print(i['encoder_mask'].shape)\n","  print(i['decoder_mask'].shape)\n","  break"]},{"cell_type":"code","execution_count":34,"metadata":{},"outputs":[{"data":{"text/plain":["4180"]},"execution_count":34,"metadata":{},"output_type":"execute_result"}],"source":["src_vocab_size = tokenizer_src.get_vocab_size()\n","tgt_vocab_size = tokenizer_tgt.get_vocab_size()\n","tgt_vocab_size"]},{"cell_type":"markdown","metadata":{},"source":["# model building "]},{"cell_type":"code","execution_count":54,"metadata":{"id":"ruwdEitqMYRr"},"outputs":[{"name":"stdout","output_type":"stream","text":["The total no of params in the model is 935630\n","torch.Size([16, 100, 50])\n","torch.Size([16, 100, 4180])\n","loss: 8.355685234069824\n"]}],"source":["import torch\n","import torch.nn as nn\n","from torch.nn import functional as F\n","\n","vocab_size = 100\n","batch_size = 16\n","block_size = 100  # seq_len\n","n_embd = 50\n","n_head = 2\n","n_layer = 2\n","dropout = 0.2\n","torch.manual_seed(1337)\n","device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","max_iters = 10000 # max no of steps for training \n","eval_interval = 100 # after how many steps the evaluation will take place\n","eval_iters = 1000  # how many sample of batches will use for evaluation\n","\n","lr = 3e-4\n","\n","\n","\n","\n","class DecoderHead(nn.Module):\n","    \"\"\" one head self-attention\"\"\"\n","\n","    def __init__(self, head_size):\n","        super().__init__()\n","        self.key = nn.Linear(n_embd, head_size, bias=False)\n","        self.query = nn.Linear(n_embd, head_size, bias=False)\n","        self.value = nn.Linear(n_embd, head_size, bias=False)\n","        self.tri = torch.tril(torch.ones(block_size, block_size))\n","        self.dropout = nn.Dropout(dropout)\n","\n","        self.register_buffer('tril', self.tri)\n","\n","\n","    def forward(self, x, y ,z, mask):\n","\n","        mask = mask.squeeze(1)\n","        B, T, C = x.shape\n","        k = self.key(x) # (B, T, h)\n","        q = self.query(x) # (B, T, h)\n","        v = self.value(x) # (B, T, h)\n","        \n","      \n","\n","        att = q @ k.transpose(-2,-1) * k.shape[-1]**-0.5 # (B,T,h)@(B,h,T) -> (B,T,T)\n","        att = att.masked_fill(mask == 0, float('-inf'))\n","        att = F.softmax(att, dim= -1) # (B,T,T)\n","        att = self.dropout(att)\n","\n","        out = att @ v # (B,T,T)@(B,T,h) -> (B,T,h)\n","        return out\n","\n","class EnocderHead(nn.Module):\n","    \"\"\" one head self-attention\"\"\"\n","\n","    def __init__(self, head_size):\n","        super().__init__()\n","        self.key = nn.Linear(n_embd, head_size, bias=False)\n","        self.query = nn.Linear(n_embd, head_size, bias=False)\n","        self.value = nn.Linear(n_embd, head_size, bias=False)\n","        self.tri = torch.tril(torch.ones(block_size, block_size))\n","        self.dropout = nn.Dropout(dropout)\n","\n","        self.register_buffer('tril', self.tri)\n","\n","\n","    def forward(self, x, mask=None):\n","\n","        mask = mask.reshape(16,1, 100)\n","        B, T, C = x.shape\n","        k = self.key(x) # (B, T, h)\n","        q = self.query(x) # (B, T, h)\n","        v = self.value(x) # (B, T, h)\n","\n","        att = q @ k.transpose(-2,-1) * k.shape[-1]**-0.5 # (B,T,h)@(B,h,T) -> (B,T,T)\n","        if mask is not None:\n","            att = att.masked_fill(mask == 0, float('-inf'))\n","        att = F.softmax(att, dim= -1) # (B,T,T)\n","        att = self.dropout(att)\n","\n","        out = att @ v # (B,T,T)@(B,T,h) -> (B,T,h)\n","\n","        return out\n","\n","class EncoderMultiHeadAtt(nn.Module):\n","\n","    def __init__(self,num_heads,  head_size):\n","        super().__init__()\n","        self.hd = nn.ModuleList([EnocderHead(head_size) for _ in range(num_heads)])\n","        self.proj = nn.Linear(head_size * num_heads, n_embd)\n","        self.dropout = nn.Dropout(dropout)\n","\n","    def forward(self,x, mask):\n","        out = torch.cat([h(x, mask) for h in self.hd], dim=-1)\n","        out = self.proj(out)\n","        out = self.dropout(out)\n","        return out\n","    \n","class DecoderMultiHeadAtt(nn.Module):\n","\n","    def __init__(self,num_heads,  head_size):\n","        super().__init__()\n","        self.hd = nn.ModuleList([DecoderHead(head_size) for _ in range(num_heads)])\n","        self.proj = nn.Linear(head_size * num_heads, n_embd)\n","        self.dropout = nn.Dropout(dropout)\n","\n","    def forward(self, x, y ,z, mask):\n","        out = torch.cat([h(x,  y ,z, mask) for h in self.hd], dim=-1)\n","        out = self.proj(out)\n","        out = self.dropout(out)\n","        return out\n","class EncoderBlock(nn.Module):\n","\n","    def __init__(self, n_embd, n_head):\n","        # n_embd: embedding dimension, n_head: the number of heads we'd like\n","        super().__init__()\n","        head_size = n_embd // n_head\n","        self.mhead = EncoderMultiHeadAtt(n_head, head_size )\n","        self.ffwd = FeedFoward(n_embd)\n","        self.ln1 = nn.LayerNorm(n_embd)\n","        self.ln2 = nn.LayerNorm(n_embd)\n","\n","    def forward(self, x, s_mask):\n","\n","        # x = x + self.mhead(x)   #skip connections\n","        # x = x + self.ffwd(x)\n","\n","        x = x + self.mhead(self.ln1(x), s_mask)\n","        x = x + self.ffwd(self.ln2(x))\n","        return x\n","class DecoderBlock(nn.Module):\n","\n","    def __init__(self, n_embd, n_head):\n","        # n_embd: embedding dimension, n_head: the number of heads we'd like\n","        super().__init__()\n","        head_size = n_embd // n_head\n","        self.mhead = DecoderMultiHeadAtt(n_head, head_size)\n","        self.croshead = DecoderMultiHeadAtt(n_head, head_size)\n","        self.ffwd = FeedFoward(n_embd)\n","        self.ln1 = nn.LayerNorm(n_embd)\n","        self.ln2 = nn.LayerNorm(n_embd)\n","\n","    def forward(self, x, e_out, s_mask, t_mask):\n","\n","        # x = x + self.mhead(x)   #skip connections\n","        # x = x + self.ffwd(x)\n","        x = self.ln1(x)\n","        x = x + self.mhead(x, e_out, e_out, s_mask)\n","        x = x + self.mhead(self.ln1(x), e_out, e_out, s_mask)\n","        x = x + self.ffwd(self.ln2(x))\n","        return x\n","\n","class FeedFoward(nn.Module):\n","\n","    def __init__(self, n_embd):\n","        super().__init__()\n","        self.net = nn.Sequential(\n","            nn.Linear(n_embd, 4 * n_embd),\n","            nn.ReLU(),\n","            nn.Linear(4 * n_embd, n_embd),\n","            nn.Dropout(dropout),\n","        )\n","\n","\n","    def forward(self, x):\n","        return self.net(x)\n","\n","class GPTFromScratch(nn.Module):\n","\n","    def __init__(self):\n","        super().__init__()\n","        self.embd_table_e = nn.Embedding(src_vocab_size, n_embd) # (vocab_size,C)\n","        self.pos_table_e = nn.Embedding(block_size, n_embd) # (T,C)\n","\n","        self.embd_table_d = nn.Embedding(tgt_vocab_size, n_embd) # (vocab_size,C)\n","        self.pos_table_d = nn.Embedding(block_size, n_embd) # (T,C)\n","\n","\n","        # self.encoderblock = nn.Sequential(\n","        #                 EncoderBlock(n_embd, n_head),\n","        #                 EncoderBlock(n_embd, n_head),\n","        #                 EncoderBlock(n_embd, n_head),\n","        #                 EncoderBlock(n_embd, n_head),\n","        #                 )\n","        #self.encoderblock = EncoderBlock(n_embd, n_head)\n","\n","        self.encoderblock = nn.ModuleList([EncoderBlock(n_embd, n_head) for _ in range(4)])\n","                     \n","        self.decoderblock = nn.ModuleList([DecoderBlock(n_embd, n_head) for _ in range(4)])\n","        # nn.Sequential(\n","        #                 DecoderBlock(n_embd, n_head),\n","        #                 DecoderBlock(n_embd, n_head),\n","        #                 DecoderBlock(n_embd, n_head),\n","        #                 DecoderBlock(n_embd, n_head),\n","        #                 )\n","\n","        #self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n","\n","\n","        self.ln_f = nn.LayerNorm(n_embd) # final layer norm\n","        #self.lm_head_e = nn.Linear(n_embd, src_vocab_size)\n","        self.lm_head_d = nn.Linear(n_embd, tgt_vocab_size)\n","\n","        self.apply(self._init_weights)\n","\n","\n","    def _init_weights(self, module):\n","        if isinstance(module, nn.Linear):\n","            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n","            if module.bias is not None:\n","                torch.nn.init.zeros_(module.bias)\n","        elif isinstance(module, nn.Embedding):\n","            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n","\n","\n","    def encoder(self, src_idx, mask):\n","        B, T = src_idx.shape  # (B -> batch, T -> block_size(seq_len))\n","\n","        # both xb, yb shape is (B,T) tensor of ints\n","        tok_emb = self.embd_table_e(src_idx) #o/p -> (B,T,C)\n","        pos_emb = self.pos_table_e(torch.arange(T, device=device))\n","        x  = tok_emb + pos_emb # (B,T,C)-> (B,T,C)+ (C,T)\n","        #x = self.head(x)\n","        # x = self.mhead(x)\n","        # x = self.ffwd(x)\n","        #x = self.encoderblock(x, mask)\n","        for block in self.encoderblock:\n","            x = block(x, mask)\n","        x = self.ln_f(x) # (B,T,C)\n","        return x\n","        #logits = self.lm_head_e(x) # (B,T,vocab_size)\n","\n","\n","    def decoder(self, tgt_idx,  e_out, s_mask, t_mask):\n","        B, T = tgt_idx.shape  # (B -> batch, T -> block_size(seq_len))\n","\n","        # both xb, yb shape is (B,T) tensor of ints\n","        tok_emb = self.embd_table_d(tgt_idx) #o/p -> (B,T,C)\n","        pos_emb = self.pos_table_d(torch.arange(T, device=device))\n","        x  = tok_emb + pos_emb # (B,T,C)-> (B,T,C)+ (C,T)\n","        #x = self.head(x)\n","        # x = self.mhead(x)\n","        # x = self.ffwd(x)\n","        #x = self.decoderblock(x, e_out, s_mask, t_mask)\n","\n","        for block in self.decoderblock:\n","            x = block(x, e_out, s_mask, t_mask)\n","        x = self.ln_f(x) # (B,T,C)\n","\n","        logits = self.lm_head_d(x) # (B,T,vocab_size)\n","\n","        return logits\n","\n","    def forward(self, idx, targets=None):\n","        B, T = idx.shape  # (B -> batch, T -> block_size(seq_len))\n","\n","        # both xb, yb shape is (B,T) tensor of ints\n","        tok_emb = self.embd_table(idx) #o/p -> (B,T,C)\n","        pos_emb = self.pos_table(torch.arange(T, device=device))\n","        x  = tok_emb + pos_emb # (B,T,C)-> (B,T,C)+ (C,T)\n","        #x = self.head(x)\n","        # x = self.mhead(x)\n","        # x = self.ffwd(x)\n","        x = self.block(x)\n","        x = self.ln_f(x) # (B,T,C)\n","\n","        logits = self.lm_head(x) # (B,T,vocab_size)\n","\n","        if targets is None:\n","            loss = None\n","        else:\n","            B, T, C = logits.shape\n","            logits = logits.view(B*T, C)\n","            targets = targets.view(B*T)\n","            loss = F.cross_entropy(logits, targets)\n","\n","        return logits, loss\n","\n","    def generate(self, idx, max_new_token):\n","        # idx is (B.T) array\n","        for _ in range(max_new_token):\n","            #crop ids to only consider last block_size tokens\n","            idx_cond =  idx[:, -block_size:] #(B,T)\n","            # predictions\n","            logits, loss = self(idx_cond)  #(B,T,C)\n","            # take only last time step\n","            logits = logits[:, -1, :]  #(B,-1, C) -> (B, T+1th, C)\n","            probs = F.softmax(logits, dim = -1)  #(B,C)\n","            # sample from the distribution\n","            idx_next = torch.multinomial(probs, num_samples=1) #(B,1)\n","            # append sample in the running sequence\n","            idx = torch.cat((idx,idx_next), dim = 1) #(B, T+1)\n","\n","        return idx  #(B, T+1)\n","\n","\n","\n","\n","\n","\n","\n","model = GPTFromScratch()\n","m = model.to(device)\n","# print the no of params in the model\n","total_params = sum(p.numel() for p in m.parameters())\n","print(f\"The total no of params in the model is {total_params}\")\n","\n","# create torch optimiser:\n","optimizer = torch.optim.AdamW(model.parameters(), lr= lr)\n","\n","\n","# training loop:\n","for iter in range(max_iters):\n","\n","    # # evaluation of Loss on train and val\n","    # if iter % eval_interval == 0 or iter == max_iters -1:\n","    #     losses = estimate_loss()\n","    #     print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n","\n","    # sample a batch of data\n","\n","    s_xb =  i['encoder_input']\n","    t_xb =    i['decoder_input']\n","    s_msk= i['encoder_mask']\n","    t_msk= i['decoder_mask']\n","    targets = i['label']\n","\n","\n","    en_out = model.encoder(s_xb,s_msk)\n","\n","    logits = model.decoder(t_xb,  en_out, s_msk, t_msk)\n","\n","    print(en_out.shape)\n","    print(logits.shape)\n","\n","\n","    if targets is None:\n","        loss = None\n","    else:\n","        B, T, C = logits.shape\n","        logits = logits.view(B*T, C)\n","        targets = targets.view(B*T)\n","        loss_fn = nn.CrossEntropyLoss(ignore_index=tokenizer_src.token_to_id('[PAD]'), label_smoothing=0.1).to(device)\n","        loss = loss_fn(logits, targets)\n","        #loss = F.cross_entropy(logits, targets)\n","    \n","\n","    print('loss:', loss.item())\n","    #print(logits.shape)\n","    # set grad = zero\n","    break\n","    optimizer.zero_grad(set_to_none=True)\n","\n","    # back propagation:\n","        # grad calculation and param update\n","    loss.backward()\n","    optimizer.step()"]},{"cell_type":"code","execution_count":58,"metadata":{"id":"4ixF0IghLnny"},"outputs":[{"name":"stdout","output_type":"stream","text":["The total no of params in the model is 935630\n","loss: 8.351285934448242\n","loss: 8.336915969848633\n","loss: 8.32962417602539\n","loss: 8.318343162536621\n","loss: 8.294999122619629\n","loss: 8.288289070129395\n","loss: 8.269145011901855\n","loss: 8.269899368286133\n","loss: 8.242206573486328\n","loss: 8.217208862304688\n","loss: 8.209543228149414\n","loss: 8.192745208740234\n","loss: 8.175320625305176\n","loss: 8.147579193115234\n","loss: 8.133208274841309\n","loss: 8.099715232849121\n","loss: 8.075833320617676\n","loss: 8.072498321533203\n","loss: 8.03001880645752\n","loss: 8.023053169250488\n","loss: 7.9666056632995605\n","loss: 7.945073127746582\n","loss: 7.930259704589844\n","loss: 7.897216796875\n","loss: 7.854577541351318\n","loss: 7.835141181945801\n","loss: 7.800683498382568\n","loss: 7.77372407913208\n","loss: 7.741453170776367\n","loss: 7.721309185028076\n","loss: 7.689691066741943\n","loss: 7.662416458129883\n","loss: 7.637540340423584\n","loss: 7.611914157867432\n","loss: 7.584750175476074\n","loss: 7.555949687957764\n","loss: 7.531854152679443\n","loss: 7.50553560256958\n","loss: 7.4775166511535645\n","loss: 7.4509782791137695\n","loss: 7.42286491394043\n","loss: 7.395932674407959\n","loss: 7.372095108032227\n","loss: 7.344855308532715\n","loss: 7.316840648651123\n","loss: 7.290932655334473\n","loss: 7.262578010559082\n","loss: 7.2363457679748535\n","loss: 7.209824085235596\n","loss: 7.183177947998047\n","loss: 7.1573872566223145\n","loss: 7.1325273513793945\n","loss: 7.10611629486084\n","loss: 7.079919338226318\n","loss: 7.056124210357666\n","loss: 7.031272888183594\n","loss: 7.006096363067627\n","loss: 6.981816291809082\n","loss: 6.95908260345459\n","loss: 6.931523323059082\n","loss: 6.910841941833496\n","loss: 6.888144016265869\n","loss: 6.8651885986328125\n","loss: 6.839216232299805\n","loss: 6.8177924156188965\n","loss: 6.797749996185303\n","loss: 6.77496337890625\n","loss: 6.751959800720215\n","loss: 6.729989528656006\n","loss: 6.708187103271484\n","loss: 6.689476013183594\n","loss: 6.669630527496338\n","loss: 6.647000312805176\n","loss: 6.626665115356445\n","loss: 6.606407642364502\n","loss: 6.586649417877197\n","loss: 6.568871021270752\n","loss: 6.550120830535889\n","loss: 6.529025554656982\n","loss: 6.512016296386719\n","loss: 6.493658542633057\n","loss: 6.4774169921875\n","loss: 6.457741737365723\n","loss: 6.441763877868652\n","loss: 6.423640727996826\n","loss: 6.407104015350342\n","loss: 6.389398574829102\n","loss: 6.37327241897583\n","loss: 6.357655048370361\n","loss: 6.341981887817383\n","loss: 6.328876972198486\n","loss: 6.310365676879883\n","loss: 6.295410633087158\n","loss: 6.279136657714844\n","loss: 6.266654968261719\n","loss: 6.250866889953613\n","loss: 6.235212326049805\n","loss: 6.2222371101379395\n","loss: 6.209134578704834\n","loss: 6.199945449829102\n","loss: 6.181392192840576\n","loss: 6.1693620681762695\n","loss: 6.157234191894531\n","loss: 6.146233081817627\n","loss: 6.133818626403809\n","loss: 6.118293762207031\n","loss: 6.1081767082214355\n","loss: 6.094737529754639\n","loss: 6.083080768585205\n","loss: 6.073383808135986\n","loss: 6.06137752532959\n","loss: 6.049626350402832\n","loss: 6.037203311920166\n","loss: 6.029428482055664\n","loss: 6.018616676330566\n","loss: 6.006065845489502\n","loss: 5.997650146484375\n","loss: 5.9880452156066895\n","loss: 5.976722240447998\n","loss: 5.968349456787109\n","loss: 5.95432710647583\n","loss: 5.949341773986816\n","loss: 5.936577796936035\n","loss: 5.927967071533203\n","loss: 5.919827461242676\n","loss: 5.907027721405029\n","loss: 5.9004387855529785\n","loss: 5.892246246337891\n","loss: 5.883861541748047\n","loss: 5.8718461990356445\n","loss: 5.866439342498779\n","loss: 5.857479095458984\n","loss: 5.84963846206665\n","loss: 5.84191370010376\n","loss: 5.831862449645996\n","loss: 5.821257591247559\n","loss: 5.81594705581665\n","loss: 5.809381484985352\n","loss: 5.800168514251709\n","loss: 5.7902398109436035\n","loss: 5.785628795623779\n"]}],"source":["import torch\n","import torch.nn as nn\n","from torch.nn import functional as F\n","\n","#vocab_size = 100\n","batch_size = 16\n","block_size = 100  # seq_len\n","n_embd = 50\n","n_head = 2\n","n_layer = 2\n","dropout = 0.2\n","torch.manual_seed(1337)\n","device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","max_iters = 10000 # max no of steps for training \n","eval_interval = 100 # after how many steps the evaluation will take place\n","eval_iters = 1000  # how many sample of batches will use for evaluation\n","\n","lr = 3e-4\n","\n","\n","\n","\n","class DecoderHead(nn.Module):\n","    \"\"\" one head self-attention\"\"\"\n","\n","    def __init__(self, head_size):\n","        super().__init__()\n","        self.key = nn.Linear(n_embd, head_size, bias=False)\n","        self.query = nn.Linear(n_embd, head_size, bias=False)\n","        self.value = nn.Linear(n_embd, head_size, bias=False)\n","        self.tri = torch.tril(torch.ones(block_size, block_size))\n","        self.dropout = nn.Dropout(dropout)\n","\n","        self.register_buffer('tril', self.tri)\n","\n","\n","    def forward(self, x, y ,z, mask):\n","\n","        mask = mask.squeeze(1)\n","        B, T, C = x.shape\n","        k = self.key(x) # (B, T, h)\n","        q = self.query(x) # (B, T, h)\n","        v = self.value(x) # (B, T, h)\n","        \n","      \n","\n","        att = q @ k.transpose(-2,-1) * k.shape[-1]**-0.5 # (B,T,h)@(B,h,T) -> (B,T,T)\n","        att = att.masked_fill(mask == 0, float('-inf'))\n","        att = F.softmax(att, dim= -1) # (B,T,T)\n","        att = self.dropout(att)\n","\n","        out = att @ v # (B,T,T)@(B,T,h) -> (B,T,h)\n","        return out\n","\n","class EnocderHead(nn.Module):\n","    \"\"\" one head self-attention\"\"\"\n","\n","    def __init__(self, head_size):\n","        super().__init__()\n","        self.key = nn.Linear(n_embd, head_size, bias=False)\n","        self.query = nn.Linear(n_embd, head_size, bias=False)\n","        self.value = nn.Linear(n_embd, head_size, bias=False)\n","        self.tri = torch.tril(torch.ones(block_size, block_size))\n","        self.dropout = nn.Dropout(dropout)\n","\n","        self.register_buffer('tril', self.tri)\n","\n","\n","    def forward(self, x, mask=None):\n","\n","        mask = mask.reshape(16,1, 100)\n","        B, T, C = x.shape\n","        k = self.key(x) # (B, T, h)\n","        q = self.query(x) # (B, T, h)\n","        v = self.value(x) # (B, T, h)\n","\n","        att = q @ k.transpose(-2,-1) * k.shape[-1]**-0.5 # (B,T,h)@(B,h,T) -> (B,T,T)\n","        if mask is not None:\n","            att = att.masked_fill(mask == 0, float('-inf'))\n","        att = F.softmax(att, dim= -1) # (B,T,T)\n","        att = self.dropout(att)\n","\n","        out = att @ v # (B,T,T)@(B,T,h) -> (B,T,h)\n","\n","        return out\n","\n","class EncoderMultiHeadAtt(nn.Module):\n","\n","    def __init__(self,num_heads,  head_size):\n","        super().__init__()\n","        self.hd = nn.ModuleList([EnocderHead(head_size) for _ in range(num_heads)])\n","        self.proj = nn.Linear(head_size * num_heads, n_embd)\n","        self.dropout = nn.Dropout(dropout)\n","\n","    def forward(self,x, mask):\n","        out = torch.cat([h(x, mask) for h in self.hd], dim=-1)\n","        out = self.proj(out)\n","        out = self.dropout(out)\n","        return out\n","    \n","class DecoderMultiHeadAtt(nn.Module):\n","\n","    def __init__(self,num_heads,  head_size):\n","        super().__init__()\n","        self.hd = nn.ModuleList([DecoderHead(head_size) for _ in range(num_heads)])\n","        self.proj = nn.Linear(head_size * num_heads, n_embd)\n","        self.dropout = nn.Dropout(dropout)\n","\n","    def forward(self, x, y ,z, mask):\n","        out = torch.cat([h(x,  y ,z, mask) for h in self.hd], dim=-1)\n","        out = self.proj(out)\n","        out = self.dropout(out)\n","        return out\n","class EncoderBlock(nn.Module):\n","\n","    def __init__(self, n_embd, n_head):\n","        # n_embd: embedding dimension, n_head: the number of heads we'd like\n","        super().__init__()\n","        head_size = n_embd // n_head\n","        self.mhead = EncoderMultiHeadAtt(n_head, head_size )\n","        self.ffwd = FeedFoward(n_embd)\n","        self.ln1 = nn.LayerNorm(n_embd)\n","        self.ln2 = nn.LayerNorm(n_embd)\n","\n","    def forward(self, x, s_mask):\n","\n","        # x = x + self.mhead(x)   #skip connections\n","        # x = x + self.ffwd(x)\n","\n","        x = x + self.mhead(self.ln1(x), s_mask)\n","        x = x + self.ffwd(self.ln2(x))\n","        return x\n","class DecoderBlock(nn.Module):\n","\n","    def __init__(self, n_embd, n_head):\n","        # n_embd: embedding dimension, n_head: the number of heads we'd like\n","        super().__init__()\n","        head_size = n_embd // n_head\n","        self.mhead = DecoderMultiHeadAtt(n_head, head_size)\n","        self.croshead = DecoderMultiHeadAtt(n_head, head_size)\n","        self.ffwd = FeedFoward(n_embd)\n","        self.ln1 = nn.LayerNorm(n_embd)\n","        self.ln2 = nn.LayerNorm(n_embd)\n","\n","    def forward(self, x, e_out, s_mask, t_mask):\n","\n","        # x = x + self.mhead(x)   #skip connections\n","        # x = x + self.ffwd(x)\n","        x = self.ln1(x)\n","        x = x + self.mhead(x, e_out, e_out, s_mask)\n","        x = x + self.mhead(self.ln1(x), e_out, e_out, s_mask)\n","        x = x + self.ffwd(self.ln2(x))\n","        return x\n","\n","class FeedFoward(nn.Module):\n","\n","    def __init__(self, n_embd):\n","        super().__init__()\n","        self.net = nn.Sequential(\n","            nn.Linear(n_embd, 4 * n_embd),\n","            nn.ReLU(),\n","            nn.Linear(4 * n_embd, n_embd),\n","            nn.Dropout(dropout),\n","        )\n","\n","\n","    def forward(self, x):\n","        return self.net(x)\n","\n","class GPTFromScratch(nn.Module):\n","\n","    def __init__(self):\n","        super().__init__()\n","        self.embd_table_e = nn.Embedding(src_vocab_size, n_embd) # (vocab_size,C)\n","        self.pos_table_e = nn.Embedding(block_size, n_embd) # (T,C)\n","\n","        self.embd_table_d = nn.Embedding(tgt_vocab_size, n_embd) # (vocab_size,C)\n","        self.pos_table_d = nn.Embedding(block_size, n_embd) # (T,C)\n","\n","\n","        # self.encoderblock = nn.Sequential(\n","        #                 EncoderBlock(n_embd, n_head),\n","        #                 EncoderBlock(n_embd, n_head),\n","        #                 EncoderBlock(n_embd, n_head),\n","        #                 EncoderBlock(n_embd, n_head),\n","        #                 )\n","        #self.encoderblock = EncoderBlock(n_embd, n_head)\n","\n","        self.encoderblock = nn.ModuleList([EncoderBlock(n_embd, n_head) for _ in range(4)])\n","                     \n","        self.decoderblock = nn.ModuleList([DecoderBlock(n_embd, n_head) for _ in range(4)])\n","        # nn.Sequential(\n","        #                 DecoderBlock(n_embd, n_head),\n","        #                 DecoderBlock(n_embd, n_head),\n","        #                 DecoderBlock(n_embd, n_head),\n","        #                 DecoderBlock(n_embd, n_head),\n","        #                 )\n","\n","        #self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n","\n","\n","        self.ln_f = nn.LayerNorm(n_embd) # final layer norm\n","        #self.lm_head_e = nn.Linear(n_embd, src_vocab_size)\n","        self.lm_head_d = nn.Linear(n_embd, tgt_vocab_size)\n","\n","        self.apply(self._init_weights)\n","\n","\n","    def _init_weights(self, module):\n","        if isinstance(module, nn.Linear):\n","            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n","            if module.bias is not None:\n","                torch.nn.init.zeros_(module.bias)\n","        elif isinstance(module, nn.Embedding):\n","            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n","\n","\n","    def encoder(self, src_idx, mask):\n","        B, T = src_idx.shape  # (B -> batch, T -> block_size(seq_len))\n","\n","        # both xb, yb shape is (B,T) tensor of ints\n","        tok_emb = self.embd_table_e(src_idx) #o/p -> (B,T,C)\n","        pos_emb = self.pos_table_e(torch.arange(T, device=device))\n","        x  = tok_emb + pos_emb # (B,T,C)-> (B,T,C)+ (C,T)\n","        #x = self.head(x)\n","        # x = self.mhead(x)\n","        # x = self.ffwd(x)\n","        #x = self.encoderblock(x, mask)\n","        for block in self.encoderblock:\n","            x = block(x, mask)\n","        x = self.ln_f(x) # (B,T,C)\n","        return x\n","        #logits = self.lm_head_e(x) # (B,T,vocab_size)\n","\n","\n","    def decoder(self, tgt_idx,  e_out, s_mask, t_mask):\n","        B, T = tgt_idx.shape  # (B -> batch, T -> block_size(seq_len))\n","\n","        # both xb, yb shape is (B,T) tensor of ints\n","        tok_emb = self.embd_table_d(tgt_idx) #o/p -> (B,T,C)\n","        pos_emb = self.pos_table_d(torch.arange(T, device=device))\n","        x  = tok_emb + pos_emb # (B,T,C)-> (B,T,C)+ (C,T)\n","        #x = self.head(x)\n","        # x = self.mhead(x)\n","        # x = self.ffwd(x)\n","        #x = self.decoderblock(x, e_out, s_mask, t_mask)\n","\n","        for block in self.decoderblock:\n","            x = block(x, e_out, s_mask, t_mask)\n","        x = self.ln_f(x) # (B,T,C)\n","\n","        logits = self.lm_head_d(x) # (B,T,vocab_size)\n","\n","        return logits\n","\n","    def forward(self, idx, targets=None):\n","        B, T = idx.shape  # (B -> batch, T -> block_size(seq_len))\n","\n","        # both xb, yb shape is (B,T) tensor of ints\n","        tok_emb = self.embd_table(idx) #o/p -> (B,T,C)\n","        pos_emb = self.pos_table(torch.arange(T, device=device))\n","        x  = tok_emb + pos_emb # (B,T,C)-> (B,T,C)+ (C,T)\n","        #x = self.head(x)\n","        # x = self.mhead(x)\n","        # x = self.ffwd(x)\n","        x = self.block(x)\n","        x = self.ln_f(x) # (B,T,C)\n","\n","        logits = self.lm_head(x) # (B,T,vocab_size)\n","\n","        if targets is None:\n","            loss = None\n","        else:\n","            B, T, C = logits.shape\n","            logits = logits.view(B*T, C)\n","            targets = targets.view(B*T)\n","            loss = F.cross_entropy(logits, targets)\n","\n","        return logits, loss\n","\n","    def generate(self, idx, max_new_token):\n","        # idx is (B.T) array\n","        for _ in range(max_new_token):\n","            #crop ids to only consider last block_size tokens\n","            idx_cond =  idx[:, -block_size:] #(B,T)\n","            # predictions\n","            logits, loss = self(idx_cond)  #(B,T,C)\n","            # take only last time step\n","            logits = logits[:, -1, :]  #(B,-1, C) -> (B, T+1th, C)\n","            probs = F.softmax(logits, dim = -1)  #(B,C)\n","            # sample from the distribution\n","            idx_next = torch.multinomial(probs, num_samples=1) #(B,1)\n","            # append sample in the running sequence\n","            idx = torch.cat((idx,idx_next), dim = 1) #(B, T+1)\n","\n","        return idx  #(B, T+1)\n","\n","\n","\n","\n","\n","\n","\n","model = GPTFromScratch()\n","m = model.to(device)\n","# print the no of params in the model\n","total_params = sum(p.numel() for p in m.parameters())\n","print(f\"The total no of params in the model is {total_params}\")\n","\n","# create torch optimiser:\n","optimizer = torch.optim.AdamW(model.parameters(), lr= lr)\n","\n","\n","# training loop:\n","for iter , batch in enumerate(t_dl):\n","\n","    # # evaluation of Loss on train and val\n","    # if iter % eval_interval == 0 or iter == max_iters -1:\n","    #     losses = estimate_loss()\n","    #     print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n","\n","    # sample a batch of data\n","\n","    s_xb = batch['encoder_input'].to(device) # (b, seq_len)\n","    t_xb = batch['decoder_input'].to(device) # (B, seq_len)\n","    s_msk = batch['encoder_mask'].to(device) # (B, 1, 1, seq_len)\n","    t_msk = batch['decoder_mask'].to(device) # (B, 1, seq_len, seq_len)\n","\n","    en_out = model.encoder(s_xb,s_msk)\n","\n","    logits = model.decoder(t_xb,  en_out, s_msk, t_msk)\n","\n","    # print(en_out.shape)\n","    # print(logits.shape)\n","\n","\n","    if targets is None:\n","        loss = None\n","    else:\n","        B, T, C = logits.shape\n","        logits = logits.view(B*T, C)\n","        targets = targets.view(B*T)\n","        loss_fn = nn.CrossEntropyLoss(ignore_index=tokenizer_src.token_to_id('[PAD]'), label_smoothing=0.1).to(device)\n","        loss = loss_fn(logits, targets)\n","        #loss = F.cross_entropy(logits, targets)\n","    \n","\n","    print('loss:', loss.item())\n","    #print(logits.shape)\n","    # set grad = zero\n","\n","    optimizer.zero_grad(set_to_none=True)\n","\n","    # back propagation:\n","        # grad calculation and param update\n","    loss.backward()\n","    optimizer.step()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0h4TaRaTLnqT"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7dtIKunXLnsy"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bU6TQUDtLnvR"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nzBgVf2RLnyJ"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":34,"metadata":{"executionInfo":{"elapsed":504,"status":"ok","timestamp":1728909606615,"user":{"displayName":"Manish Negi","userId":"13518611288471091944"},"user_tz":-330},"id":"67fOVd8SA5Hh"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","from torch.utils.data import Dataset\n","\n","class BilingualDataset(Dataset):\n","\n","    def __init__(self, ds, tokenizer_src, tokenizer_tgt, src_lang, tgt_lang, seq_len):\n","        super().__init__()\n","        self.seq_len = seq_len\n","\n","        self.ds = ds\n","        self.tokenizer_src = tokenizer_src\n","        self.tokenizer_tgt = tokenizer_tgt\n","        self.src_lang = src_lang\n","        self.tgt_lang = tgt_lang\n","\n","        self.sos_token = torch.tensor([tokenizer_tgt.token_to_id(\"[SOS]\")], dtype=torch.int64)\n","        self.eos_token = torch.tensor([tokenizer_tgt.token_to_id(\"[EOS]\")], dtype=torch.int64)\n","        self.pad_token = torch.tensor([tokenizer_tgt.token_to_id(\"[PAD]\")], dtype=torch.int64)\n","\n","    def __len__(self):\n","        return len(self.ds)\n","\n","    def __getitem__(self, idx):\n","        src_target_pair = self.ds[idx]\n","        src_text = src_target_pair['translation'][self.src_lang]\n","        tgt_text = src_target_pair['translation'][self.tgt_lang]\n","\n","        # Transform the text into tokens\n","        enc_input_tokens = self.tokenizer_src.encode(src_text).ids\n","        dec_input_tokens = self.tokenizer_tgt.encode(tgt_text).ids\n","\n","        # Add sos, eos and padding to each sentence\n","        enc_num_padding_tokens = self.seq_len - len(enc_input_tokens) - 2  # We will add <s> and </s>\n","        # We will only add <s>, and </s> only on the label\n","        dec_num_padding_tokens = self.seq_len - len(dec_input_tokens) - 1\n","\n","        # Make sure the number of padding tokens is not negative. If it is, the sentence is too long\n","        if enc_num_padding_tokens < 0 or dec_num_padding_tokens < 0:\n","            raise ValueError(\"Sentence is too long\")\n","\n","        # Add <s> and </s> token\n","        encoder_input = torch.cat(\n","            [\n","                self.sos_token,\n","                torch.tensor(enc_input_tokens, dtype=torch.int64),\n","                self.eos_token,\n","                torch.tensor([self.pad_token] * enc_num_padding_tokens, dtype=torch.int64),\n","            ],\n","            dim=0,\n","        )\n","\n","        # Add only <s> token\n","        decoder_input = torch.cat(\n","            [\n","                self.sos_token,\n","                torch.tensor(dec_input_tokens, dtype=torch.int64),\n","                torch.tensor([self.pad_token] * dec_num_padding_tokens, dtype=torch.int64),\n","            ],\n","            dim=0,\n","        )\n","\n","        # Add only </s> token\n","        label = torch.cat(\n","            [\n","                torch.tensor(dec_input_tokens, dtype=torch.int64),\n","                self.eos_token,\n","                torch.tensor([self.pad_token] * dec_num_padding_tokens, dtype=torch.int64),\n","            ],\n","            dim=0,\n","        )\n","\n","        # Double check the size of the tensors to make sure they are all seq_len long\n","        assert encoder_input.size(0) == self.seq_len\n","        assert decoder_input.size(0) == self.seq_len\n","        assert label.size(0) == self.seq_len\n","\n","        return {\n","            \"encoder_input\": encoder_input,  # (seq_len)\n","            \"decoder_input\": decoder_input,  # (seq_len)\n","            \"encoder_mask\": (encoder_input != self.pad_token).unsqueeze(0).unsqueeze(0).int(), # (1, 1, seq_len)\n","            \"decoder_mask\": (decoder_input != self.pad_token).unsqueeze(0).int() & causal_mask(decoder_input.size(0)), # (1, seq_len) & (1, seq_len, seq_len),\n","            \"label\": label,  # (seq_len)\n","            \"src_text\": src_text,\n","            \"tgt_text\": tgt_text,\n","        }\n","\n","def causal_mask(size):\n","    mask = torch.triu(torch.ones((1, size, size)), diagonal=1).type(torch.int)\n","    return mask == 0"]},{"cell_type":"code","execution_count":35,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8755,"status":"ok","timestamp":1728909627635,"user":{"displayName":"Manish Negi","userId":"13518611288471091944"},"user_tz":-330},"id":"9neOfzk9Cq85","outputId":"3c5299d6-03c1-4b31-fafe-53c78ac559de"},"outputs":[{"name":"stdout","output_type":"stream","text":["Max length of source sentence: 309\n","Max length of target sentence: 274\n"]}],"source":["from datasets import load_dataset\n","from tokenizers import Tokenizer\n","from tokenizers.models import WordLevel\n","from tokenizers.trainers import WordLevelTrainer\n","from tokenizers.pre_tokenizers import Whitespace\n","def get_all_sentences(ds, lang):\n","    for item in ds:\n","        yield item['translation'][lang]\n","def get_or_build_tokenizer(config, ds, lang):\n","    tokenizer_path = Path(config['tokenizer_file'].format(lang))\n","    if not Path.exists(tokenizer_path):\n","        # Most code taken from: https://huggingface.co/docs/tokenizers/quicktour\n","        tokenizer = Tokenizer(WordLevel(unk_token=\"[UNK]\"))\n","        tokenizer.pre_tokenizer = Whitespace()\n","        trainer = WordLevelTrainer(special_tokens=[\"[UNK]\", \"[PAD]\", \"[SOS]\", \"[EOS]\"], min_frequency=2)\n","        tokenizer.train_from_iterator(get_all_sentences(ds, lang), trainer=trainer)\n","        tokenizer.save(str(tokenizer_path))\n","    else:\n","        tokenizer = Tokenizer.from_file(str(tokenizer_path))\n","    return tokenizer\n","\n","ds_raw = load_dataset(f\"{config['datasource']}\", f\"{config['lang_src']}-{config['lang_tgt']}\", split='train')\n","\n","# Build tokenizers\n","tokenizer_src = get_or_build_tokenizer(config, ds_raw, config['lang_src'])\n","tokenizer_tgt = get_or_build_tokenizer(config, ds_raw, config['lang_tgt'])\n","\n","# Keep 90% for training, 10% for validation\n","train_ds_size = int(0.9 * len(ds_raw))\n","val_ds_size = len(ds_raw) - train_ds_size\n","train_ds_raw, val_ds_raw = random_split(ds_raw, [train_ds_size, val_ds_size])\n","\n","train_ds = BilingualDataset(train_ds_raw, tokenizer_src, tokenizer_tgt, config['lang_src'], config['lang_tgt'], config['seq_len'])\n","val_ds = BilingualDataset(val_ds_raw, tokenizer_src, tokenizer_tgt, config['lang_src'], config['lang_tgt'], config['seq_len'])\n","\n","# Find the maximum length of each sentence in the source and target sentence\n","max_len_src = 0\n","max_len_tgt = 0\n","\n","for item in ds_raw:\n","    src_ids = tokenizer_src.encode(item['translation'][config['lang_src']]).ids\n","    tgt_ids = tokenizer_tgt.encode(item['translation'][config['lang_tgt']]).ids\n","    max_len_src = max(max_len_src, len(src_ids))\n","    max_len_tgt = max(max_len_tgt, len(tgt_ids))\n","\n","print(f'Max length of source sentence: {max_len_src}')\n","print(f'Max length of target sentence: {max_len_tgt}')\n","\n","\n","train_dataloader = DataLoader(train_ds, batch_size=config['batch_size'], shuffle=True)\n","val_dataloader = DataLoader(val_ds, batch_size=1, shuffle=True)"]},{"cell_type":"code","execution_count":36,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":494,"status":"ok","timestamp":1728909631268,"user":{"displayName":"Manish Negi","userId":"13518611288471091944"},"user_tz":-330},"id":"nZdnI9jMCyxS","outputId":"f45b7c4d-7042-4c60-db68-94fb84763835"},"outputs":[{"name":"stdout","output_type":"stream","text":["{'encoder_input': tensor([[    2,   196,   499,  ...,     1,     1,     1],\n","        [    2,    56,     0,  ...,     1,     1,     1],\n","        [    2,   503,    39,  ...,     1,     1,     1],\n","        ...,\n","        [    2,    51,    44,  ...,     1,     1,     1],\n","        [    2, 27130,   352,  ...,     1,     1,     1],\n","        [    2,   129,    44,  ...,     1,     1,     1]]), 'decoder_input': tensor([[    2, 12959,    14,  ...,     1,     1,     1],\n","        [    2,     0,     6,  ...,     1,     1,     1],\n","        [    2,   146,   216,  ...,     1,     1,     1],\n","        ...,\n","        [    2,    47,    44,  ...,     1,     1,     1],\n","        [    2,  2345,  5382,  ...,     1,     1,     1],\n","        [    2,   223,    12,  ...,     1,     1,     1]]), 'encoder_mask': tensor([[[[1, 1, 1,  ..., 0, 0, 0]]],\n","\n","\n","        [[[1, 1, 1,  ..., 0, 0, 0]]],\n","\n","\n","        [[[1, 1, 1,  ..., 0, 0, 0]]],\n","\n","\n","        ...,\n","\n","\n","        [[[1, 1, 1,  ..., 0, 0, 0]]],\n","\n","\n","        [[[1, 1, 1,  ..., 0, 0, 0]]],\n","\n","\n","        [[[1, 1, 1,  ..., 0, 0, 0]]]], dtype=torch.int32), 'decoder_mask': tensor([[[[1, 0, 0,  ..., 0, 0, 0],\n","          [1, 1, 0,  ..., 0, 0, 0],\n","          [1, 1, 1,  ..., 0, 0, 0],\n","          ...,\n","          [1, 1, 1,  ..., 0, 0, 0],\n","          [1, 1, 1,  ..., 0, 0, 0],\n","          [1, 1, 1,  ..., 0, 0, 0]]],\n","\n","\n","        [[[1, 0, 0,  ..., 0, 0, 0],\n","          [1, 1, 0,  ..., 0, 0, 0],\n","          [1, 1, 1,  ..., 0, 0, 0],\n","          ...,\n","          [1, 1, 1,  ..., 0, 0, 0],\n","          [1, 1, 1,  ..., 0, 0, 0],\n","          [1, 1, 1,  ..., 0, 0, 0]]],\n","\n","\n","        [[[1, 0, 0,  ..., 0, 0, 0],\n","          [1, 1, 0,  ..., 0, 0, 0],\n","          [1, 1, 1,  ..., 0, 0, 0],\n","          ...,\n","          [1, 1, 1,  ..., 0, 0, 0],\n","          [1, 1, 1,  ..., 0, 0, 0],\n","          [1, 1, 1,  ..., 0, 0, 0]]],\n","\n","\n","        ...,\n","\n","\n","        [[[1, 0, 0,  ..., 0, 0, 0],\n","          [1, 1, 0,  ..., 0, 0, 0],\n","          [1, 1, 1,  ..., 0, 0, 0],\n","          ...,\n","          [1, 1, 1,  ..., 0, 0, 0],\n","          [1, 1, 1,  ..., 0, 0, 0],\n","          [1, 1, 1,  ..., 0, 0, 0]]],\n","\n","\n","        [[[1, 0, 0,  ..., 0, 0, 0],\n","          [1, 1, 0,  ..., 0, 0, 0],\n","          [1, 1, 1,  ..., 0, 0, 0],\n","          ...,\n","          [1, 1, 1,  ..., 0, 0, 0],\n","          [1, 1, 1,  ..., 0, 0, 0],\n","          [1, 1, 1,  ..., 0, 0, 0]]],\n","\n","\n","        [[[1, 0, 0,  ..., 0, 0, 0],\n","          [1, 1, 0,  ..., 0, 0, 0],\n","          [1, 1, 1,  ..., 0, 0, 0],\n","          ...,\n","          [1, 1, 1,  ..., 0, 0, 0],\n","          [1, 1, 1,  ..., 0, 0, 0],\n","          [1, 1, 1,  ..., 0, 0, 0]]]], dtype=torch.int32), 'label': tensor([[12959,    14,   486,  ...,     1,     1,     1],\n","        [    0,     6,     0,  ...,     1,     1,     1],\n","        [  146,   216,   110,  ...,     1,     1,     1],\n","        ...,\n","        [   47,    44,    12,  ...,     1,     1,     1],\n","        [ 2345,  5382,    23,  ...,     1,     1,     1],\n","        [  223,    12,    36,  ...,     1,     1,     1]]), 'src_text': [\"To leave your son, or to continue in this degrading situation?'\", '\"Dressmaking and Millinery,\"-\\' she read.', \"She has been very ill.'\", \"I must go back and see after some executions I have ordered'; and she walked off, leaving Alice alone with the Gryphon.\", 'Stremov (who was also on the Special Committee), stung to the quick, began justifying himself, and the meeting became quite a stormy one. But Karenin triumphed and his motion was carried; three new Special Committees were formed, and the next day nothing was talked about in a certain Petersburg set but that meeting.', \"And I have done nothing wrong and don't deserve to suffer.'\", 'Matthew put his hands into the pocket of his jacket, put out his foot, and looked at his master with a slight, good-humoured smile.', \"If I don't understand, it is my fault, I am a silly or a bad boy,' thought the child, and that was the cause of his testing, questioning, and to some extent hostile expression and of the shyness and fitfulness Vronsky found so irksome.\"], 'tgt_text': ['Lasciare il figlio o continuare a vivere in questa situazione umiliante?', 'Mode e confezioni” ella leggeva.', 'È stata molto malata.', 'Io debbo tornare indietro per assistere alle esecuzioni che ho ordinate. — E andò via lasciando Alice sola col Grifone.', 'Stremov, anche lui membro della commissione e anche lui colto nel vivo, cominciò a giustificarsi, e nell’insieme ne venne fuori una seduta tempestosa; ma Aleksej Aleksandrovic trionfò, e la sua proposta fu accolta; furono nominate tre nuove commissioni e il giorno dopo, in un certo ambiente di Pietroburgo, non si fece altro che parlare di questa seduta.', 'E io non sono colpevole di nulla e non ho ragione di soffrire.', 'Matvej ficcò le mani nelle tasche del giubbetto, tirò indietro una gamba in silenzio, bonariamente, sorridendo appena, guardò il padrone.', 'Se non lo capisco, la colpa è mia che sono un ragazzo sciocco e cattivo» pensava il bambino; e da ciò derivavano la sua espressione indagatrice e quasi ostile, e quella discontinuità che tanto turbava Vronskij.']}\n"]}],"source":["for i in train_dataloader:\n","  print(i)\n","  break"]},{"cell_type":"code","execution_count":39,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":584,"status":"ok","timestamp":1728909655100,"user":{"displayName":"Manish Negi","userId":"13518611288471091944"},"user_tz":-330},"id":"Y9nMOlhcC1rj","outputId":"1b605e0c-7ca2-4612-d057-38fa66cc33b5"},"outputs":[{"data":{"text/plain":["torch.Size([8, 350])"]},"execution_count":39,"metadata":{},"output_type":"execute_result"}],"source":["i['encoder_input'].shape"]},{"cell_type":"code","execution_count":47,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":484,"status":"ok","timestamp":1728909770144,"user":{"displayName":"Manish Negi","userId":"13518611288471091944"},"user_tz":-330},"id":"uuTBnPAkC2-k","outputId":"8f591643-cdaa-4c1b-cc9f-5ff5b85f540a"},"outputs":[{"data":{"text/plain":["tensor([    2,   196,   499,    54,   522,     6,    25,     9,   872,    10,\n","           38, 22357,   988,  3561,     3,     1,     1,     1,     1,     1])"]},"execution_count":47,"metadata":{},"output_type":"execute_result"}],"source":["i['encoder_input'][0][:20] #sos eos"]},{"cell_type":"code","execution_count":48,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":468,"status":"ok","timestamp":1728909773730,"user":{"displayName":"Manish Negi","userId":"13518611288471091944"},"user_tz":-330},"id":"cgqgp1qJC-JT","outputId":"14eba222-f08a-48d6-828b-c91d0c1fe1d8"},"outputs":[{"data":{"text/plain":["tensor([    2, 12959,    14,   486,    60,  1963,    13,   455,    16,    99,\n","          520,  4107,    30,     1,     1,     1,     1,     1,     1,     1])"]},"execution_count":48,"metadata":{},"output_type":"execute_result"}],"source":["i['decoder_input'][0][:20] #sos"]},{"cell_type":"code","execution_count":49,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":486,"status":"ok","timestamp":1728909777611,"user":{"displayName":"Manish Negi","userId":"13518611288471091944"},"user_tz":-330},"id":"AnTr3lGBDOfy","outputId":"ab7c2b17-dd0f-480d-a112-e67b04ffb937"},"outputs":[{"data":{"text/plain":["tensor([12959,    14,   486,    60,  1963,    13,   455,    16,    99,   520,\n","         4107,    30,     3,     1,     1,     1,     1,     1,     1,     1])"]},"execution_count":49,"metadata":{},"output_type":"execute_result"}],"source":["i['label'][0][:20] #eos"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BwBL1GHbFGPv"},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"authorship_tag":"ABX9TyO+UNYkLSGkuA+dYDuA4GIa","gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"gpt","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.0"},"widgets":{"application/vnd.jupyter.widget-state+json":{"049b888df8334cb29c0e81af3e3f889f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"04eb6afe0c514582831b366c6fb0884e":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"064507d77e3749e0a057c92569000c5f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_133b85e54c6e499588ac2d19be61e20a","placeholder":"​","style":"IPY_MODEL_40608ab2c957447eaadb7be1085b69af","value":"Generating test split: 100%"}},"08a22eeffb8d43eabfb7127ed498f06b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"0b8586b348f84e7ba060d4dc54231412":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0bf329eae3b54cdcabd4e6a1ef3d1896":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_1c465cd3749144bd928a89acc255f814","placeholder":"​","style":"IPY_MODEL_c1b1b93e711d4edba62cbd68e70e5c0f","value":"Generating validation split: 100%"}},"0c9b92d6670c4354be0c9527bed9776e":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0e0d492bc0a740d7b9f32dc470d0793f":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"133b85e54c6e499588ac2d19be61e20a":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1482e6c738ab41059f76150278d7c202":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"15a9faf01c454b879e171deb5a2920ea":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"17b0bb669328415892cf9469bebb9bd2":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"192bb848fbd545e69fd68cb99005cef9":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1aaa6d8beb9442db81f757d918cca04b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_ca9a9f1a7267460a8447a0e6414c4d4c","IPY_MODEL_69151c9ff82c47aab36d2e78c722df04","IPY_MODEL_3b6a5d57f1624e1983975bfde0d8f67d"],"layout":"IPY_MODEL_f4c9e531571f4c2caa49f211eca19fb7"}},"1c465cd3749144bd928a89acc255f814":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1e1ae5d0671d4ceca1f28acf25d5bfac":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"20e1a5d681ce4ed5a5563902c5cb3fee":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"254f7d1dd0f04ceeafd192ec20e5170f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_f2f3e784174c44abbe8021d54e3a7cb2","max":953,"min":0,"orientation":"horizontal","style":"IPY_MODEL_2de45afb9c9244b28fa1e0228e4baf8e","value":953}},"25e3b193155a4666b248ce361a2a4e3a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"2af0dfe81fcb49b0b5ccbca7d2c04474":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"2bee743f0a0c45858604c883cf85d7a4":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"2c1f1dfd2ff04c1dbf911d42249f2cf5":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2d10355e899343ed869904450761ddeb":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_b7e4bc0932044fdfbcd9826ba7f40501","placeholder":"​","style":"IPY_MODEL_d88e3b0780fd4c058f7efafe9431c258","value":"test-00000-of-00001.parquet: 100%"}},"2d4027955e264bf1853c80df406c52ec":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2de45afb9c9244b28fa1e0228e4baf8e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"2f3a26b323454467af50c1031c588ff3":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"335e6ac2cc8a448caab3d50c51e32b03":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_064507d77e3749e0a057c92569000c5f","IPY_MODEL_e85a79cb601144e0a72eeaafd323b7ec","IPY_MODEL_53d84d6ac208426cbb62e9a586763b96"],"layout":"IPY_MODEL_e1acf2412c6a4a2f9d239ef21230a0ca"}},"368be2b49c0d4c008a7df1a0d62406c2":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_04eb6afe0c514582831b366c6fb0884e","placeholder":"​","style":"IPY_MODEL_90c0decc424a4248bafd8319f849ba17","value":"train-00000-of-00001.parquet: 100%"}},"371dc50b0947476ab85783bbf4d79de8":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3913ce669b604bba91657334471bbf77":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3a3d7345284a4c78b7be90f815b4ea55":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3b6a5d57f1624e1983975bfde0d8f67d":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_4da82dc6832c4026a4c3998966409739","placeholder":"​","style":"IPY_MODEL_de8c9f5cc02a42cb9ac52f2376236b17","value":" 1659083/1659083 [00:05&lt;00:00, 323718.49 examples/s]"}},"3cf08d908c744c5283e413ee23021275":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3d4c859cc585468ca2cb47183680ebdb":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_bd5d104ee224435e8e64bfd4a6c9e5d3","max":85707,"min":0,"orientation":"horizontal","style":"IPY_MODEL_76b03f7443464f9dac8b3a587535de6a","value":85707}},"3e15b790030f40f0b39513e805053f85":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_804e8a0566594036b515b902d013a76b","max":28064,"min":0,"orientation":"horizontal","style":"IPY_MODEL_15a9faf01c454b879e171deb5a2920ea","value":28064}},"3e747a3705ce44928b7c2b9170bf7b37":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"40608ab2c957447eaadb7be1085b69af":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"40fb3bb38ee54cc0821a0c463f434ed6":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4184efc991224c05a81cfe2e4759c4c9":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"452b68fa4cbf4e90a838eb77e0b5a7fb":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_b858139f5c3c43058e56681d0866ad5a","IPY_MODEL_254f7d1dd0f04ceeafd192ec20e5170f","IPY_MODEL_886b4ed9b0f14bc5a9fb3be50163ff2e"],"layout":"IPY_MODEL_882be15e2c5a4c67a0112c81ec8f8218"}},"4609847ffcf64e318ce73a58abcf64af":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_3cf08d908c744c5283e413ee23021275","placeholder":"​","style":"IPY_MODEL_59f052c67ae8449bb0b2fd6b697d5881","value":"train-00000-of-00001.parquet: 100%"}},"477365fd742e426f896d61959e3e69af":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_ffa3ee70d2264ca1af4b5d0d115929f5","max":3140,"min":0,"orientation":"horizontal","style":"IPY_MODEL_2bee743f0a0c45858604c883cf85d7a4","value":3140}},"47a74c44588f4f9a8c18c114690ee037":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4da82dc6832c4026a4c3998966409739":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4f3adf1ddeab4efeb196e44c45fe7ab7":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_0e0d492bc0a740d7b9f32dc470d0793f","placeholder":"​","style":"IPY_MODEL_639afb1160fc431ebb66ee2b753e12a9","value":" 3.14k/3.14k [00:00&lt;00:00, 151kB/s]"}},"5083b95e062f4de58ce449aa51a5dca7":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_0b8586b348f84e7ba060d4dc54231412","placeholder":"​","style":"IPY_MODEL_049b888df8334cb29c0e81af3e3f889f","value":"README.md: 100%"}},"53865736edcc4705a2230f6412d322d6":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"53d84d6ac208426cbb62e9a586763b96":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_1482e6c738ab41059f76150278d7c202","placeholder":"​","style":"IPY_MODEL_3e747a3705ce44928b7c2b9170bf7b37","value":" 2507/2507 [00:00&lt;00:00, 77250.03 examples/s]"}},"58afdf3b553e428fa37005752b11b0f4":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_0bf329eae3b54cdcabd4e6a1ef3d1896","IPY_MODEL_710e6651b7c447e393b6cf5866449c37","IPY_MODEL_eeb9ccdd756746a99152caf79e116c04"],"layout":"IPY_MODEL_2f3a26b323454467af50c1031c588ff3"}},"59f052c67ae8449bb0b2fd6b697d5881":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"5c4133a7b6c441dca01ce6bd91d37a73":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ca36ddab1f7a4b89b49bf969d9a942ad","placeholder":"​","style":"IPY_MODEL_6d04fedbb2f040728f7c5dcf01387246","value":"validation-00000-of-00001.parquet: 100%"}},"6375091048be41f3bfd4a03e01d98567":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"639afb1160fc431ebb66ee2b753e12a9":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"69151c9ff82c47aab36d2e78c722df04":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_0c9b92d6670c4354be0c9527bed9776e","max":1659083,"min":0,"orientation":"horizontal","style":"IPY_MODEL_fb177cbf2c994090abc687a69a97ae88","value":1659083}},"69f4df43d4d6460885f1c19fe27eef4a":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6ad01f321e4c4f9e8d8445fe6fb99fba":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_7bf57134cfa94eedb165a92502519fb1","placeholder":"​","style":"IPY_MODEL_fec0d802ce4d4204872ce779788d0b12","value":" 190M/190M [00:01&lt;00:00, 119MB/s]"}},"6bf87732f4ad4e2cb375207c31969b2a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"6d04fedbb2f040728f7c5dcf01387246":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"6e9e762985704c3b995db0c5e46f3e08":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d5380d82f7f24b93857631339a4a991b","placeholder":"​","style":"IPY_MODEL_6375091048be41f3bfd4a03e01d98567","value":" 85.7k/85.7k [00:00&lt;00:00, 2.51MB/s]"}},"6eb2db75d12740f5b13d9ebdc929f457":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_69f4df43d4d6460885f1c19fe27eef4a","max":500080,"min":0,"orientation":"horizontal","style":"IPY_MODEL_4184efc991224c05a81cfe2e4759c4c9","value":500080}},"6ef5cf0848db4bee89eba76c7f2c1697":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_91b5825739484c6ab5aac2b5e3192e86","placeholder":"​","style":"IPY_MODEL_17b0bb669328415892cf9469bebb9bd2","value":"README.md: 100%"}},"710e6651b7c447e393b6cf5866449c37":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_47a74c44588f4f9a8c18c114690ee037","max":520,"min":0,"orientation":"horizontal","style":"IPY_MODEL_d5ce10e5950d459eb00074c9b9bf27ed","value":520}},"7377b08fbc294b6da508fbad8a97f8f0":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d564962848e84bc68377231d46779849","placeholder":"​","style":"IPY_MODEL_961d920952ed414db8f1a93277a23998","value":" 500k/500k [00:00&lt;00:00, 26.3MB/s]"}},"742ef2de527e435e8477644ae5f998e6":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"76b03f7443464f9dac8b3a587535de6a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"790bc9780d6d44ecae4c5470cb4d8a15":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_5c4133a7b6c441dca01ce6bd91d37a73","IPY_MODEL_3d4c859cc585468ca2cb47183680ebdb","IPY_MODEL_6e9e762985704c3b995db0c5e46f3e08"],"layout":"IPY_MODEL_2d4027955e264bf1853c80df406c52ec"}},"7bf57134cfa94eedb165a92502519fb1":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7c5fadf60038400eb2c2d1602bd1cdd8":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_6ef5cf0848db4bee89eba76c7f2c1697","IPY_MODEL_477365fd742e426f896d61959e3e69af","IPY_MODEL_4f3adf1ddeab4efeb196e44c45fe7ab7"],"layout":"IPY_MODEL_c5ead20065ee4efa9ec1080fd3f796d2"}},"7c90c3d3766e4dbb9db5642f66238d27":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"804e8a0566594036b515b902d013a76b":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"87631464475745a3854ba0b36d8147d0":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_4609847ffcf64e318ce73a58abcf64af","IPY_MODEL_e01b8e9cd87e4aa8b1371b76fe6223d0","IPY_MODEL_6ad01f321e4c4f9e8d8445fe6fb99fba"],"layout":"IPY_MODEL_cab6ca93ce8c42c1b1eaf1fc16ccad5e"}},"882be15e2c5a4c67a0112c81ec8f8218":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"883ec651510844158dee79a950ac3380":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_368be2b49c0d4c008a7df1a0d62406c2","IPY_MODEL_b0d55e0f2c3e4517927ce0a42d3f78d8","IPY_MODEL_b3053dd2df184d02a68b922fef7194a9"],"layout":"IPY_MODEL_40fb3bb38ee54cc0821a0c463f434ed6"}},"886b4ed9b0f14bc5a9fb3be50163ff2e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_95e64bc0727c4defa854720a4f444bcf","placeholder":"​","style":"IPY_MODEL_9272eaec0bc4464497024d6a596785a6","value":" 953/953 [00:00&lt;00:00, 22.1kB/s]"}},"90c0decc424a4248bafd8319f849ba17":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"91b5825739484c6ab5aac2b5e3192e86":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9272eaec0bc4464497024d6a596785a6":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"95e64bc0727c4defa854720a4f444bcf":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"961d920952ed414db8f1a93277a23998":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"9fa93a90ecac43489967a7efec40f54c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"9fec209118ff4e75aa940453ca90c8c9":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"a71f7e94b04d439ab033916f1adf1a5c":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a824f9d6e75b473991f00c73c5e01176":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_3a3d7345284a4c78b7be90f815b4ea55","max":32332,"min":0,"orientation":"horizontal","style":"IPY_MODEL_9fec209118ff4e75aa940453ca90c8c9","value":32332}},"a912be666e714c3e906f7927d5e2cd45":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_5083b95e062f4de58ce449aa51a5dca7","IPY_MODEL_3e15b790030f40f0b39513e805053f85","IPY_MODEL_eb4980da61384370aa4c07a4ff83e537"],"layout":"IPY_MODEL_2c1f1dfd2ff04c1dbf911d42249f2cf5"}},"b0d55e0f2c3e4517927ce0a42d3f78d8":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_1e1ae5d0671d4ceca1f28acf25d5bfac","max":5726189,"min":0,"orientation":"horizontal","style":"IPY_MODEL_25e3b193155a4666b248ce361a2a4e3a","value":5726189}},"b3053dd2df184d02a68b922fef7194a9":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ec1374a0ecca43dd9dd819d3edde13b0","placeholder":"​","style":"IPY_MODEL_7c90c3d3766e4dbb9db5642f66238d27","value":" 5.73M/5.73M [00:00&lt;00:00, 21.7MB/s]"}},"b5cdbebcb6f846529c7eefa02b6e0958":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_b8958e8d6d104a46948fbd91d934388d","IPY_MODEL_a824f9d6e75b473991f00c73c5e01176","IPY_MODEL_c943132d5462423790fd6679126b7dfd"],"layout":"IPY_MODEL_192bb848fbd545e69fd68cb99005cef9"}},"b7e4bc0932044fdfbcd9826ba7f40501":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b858139f5c3c43058e56681d0866ad5a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d4bc1359b3f242b28e50c60cd5524f8e","placeholder":"​","style":"IPY_MODEL_08a22eeffb8d43eabfb7127ed498f06b","value":"dataset_infos.json: 100%"}},"b8958e8d6d104a46948fbd91d934388d":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ed8b39b1b89044eab48439d088212a08","placeholder":"​","style":"IPY_MODEL_2af0dfe81fcb49b0b5ccbca7d2c04474","value":"Generating train split: 100%"}},"bc7f71913de945a2836e3045b159e50e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"bd5d104ee224435e8e64bfd4a6c9e5d3":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"be03f60509f14d2a81d262361b98f48b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_2d10355e899343ed869904450761ddeb","IPY_MODEL_6eb2db75d12740f5b13d9ebdc929f457","IPY_MODEL_7377b08fbc294b6da508fbad8a97f8f0"],"layout":"IPY_MODEL_3913ce669b604bba91657334471bbf77"}},"c1b1b93e711d4edba62cbd68e70e5c0f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c5ead20065ee4efa9ec1080fd3f796d2":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c943132d5462423790fd6679126b7dfd":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_dc364c06b34948a89249ad4259c40ad5","placeholder":"​","style":"IPY_MODEL_6bf87732f4ad4e2cb375207c31969b2a","value":" 32332/32332 [00:00&lt;00:00, 165069.69 examples/s]"}},"ca36ddab1f7a4b89b49bf969d9a942ad":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ca9a9f1a7267460a8447a0e6414c4d4c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a71f7e94b04d439ab033916f1adf1a5c","placeholder":"​","style":"IPY_MODEL_20e1a5d681ce4ed5a5563902c5cb3fee","value":"Generating train split: 100%"}},"cab6ca93ce8c42c1b1eaf1fc16ccad5e":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d3948e2def0f4895b71a63b989a74d97":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d4bc1359b3f242b28e50c60cd5524f8e":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d5380d82f7f24b93857631339a4a991b":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d564962848e84bc68377231d46779849":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d5ce10e5950d459eb00074c9b9bf27ed":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"d86fd970330c41b7932c9cc8b4eab569":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d88e3b0780fd4c058f7efafe9431c258":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"db06d8bc38114b45ae108b0e9bcce7a6":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"dc364c06b34948a89249ad4259c40ad5":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"de8c9f5cc02a42cb9ac52f2376236b17":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e01b8e9cd87e4aa8b1371b76fe6223d0":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_371dc50b0947476ab85783bbf4d79de8","max":189602204,"min":0,"orientation":"horizontal","style":"IPY_MODEL_9fa93a90ecac43489967a7efec40f54c","value":189602204}},"e1acf2412c6a4a2f9d239ef21230a0ca":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e85a79cb601144e0a72eeaafd323b7ec":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_d86fd970330c41b7932c9cc8b4eab569","max":2507,"min":0,"orientation":"horizontal","style":"IPY_MODEL_bc7f71913de945a2836e3045b159e50e","value":2507}},"eb4980da61384370aa4c07a4ff83e537":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_742ef2de527e435e8477644ae5f998e6","placeholder":"​","style":"IPY_MODEL_53865736edcc4705a2230f6412d322d6","value":" 28.1k/28.1k [00:00&lt;00:00, 463kB/s]"}},"ec1374a0ecca43dd9dd819d3edde13b0":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ed8b39b1b89044eab48439d088212a08":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"eeb9ccdd756746a99152caf79e116c04":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d3948e2def0f4895b71a63b989a74d97","placeholder":"​","style":"IPY_MODEL_db06d8bc38114b45ae108b0e9bcce7a6","value":" 520/520 [00:00&lt;00:00, 19282.79 examples/s]"}},"f2f3e784174c44abbe8021d54e3a7cb2":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f4c9e531571f4c2caa49f211eca19fb7":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"fb177cbf2c994090abc687a69a97ae88":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"fec0d802ce4d4204872ce779788d0b12":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"ffa3ee70d2264ca1af4b5d0d115929f5":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}}}}},"nbformat":4,"nbformat_minor":0}
