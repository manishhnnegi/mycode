{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hi Manish Negi! Nice to meet you. ðŸ˜„  What can I do for you today? ðŸ˜Š \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import google.generativeai as genai\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "key = os.getenv('GEMINI_API_KEY')\n",
    "\n",
    "genai.configure(api_key=key)\n",
    "\n",
    "# Create the model\n",
    "generation_config = {\n",
    "  \"temperature\": 1,\n",
    "  \"top_p\": 0.95,\n",
    "  \"top_k\": 64,\n",
    "  \"max_output_tokens\": 8192,\n",
    "  \"response_mime_type\": \"text/plain\",\n",
    "}\n",
    "\n",
    "\n",
    "model = genai.GenerativeModel(\n",
    "  model_name=\"gemini-1.5-flash\",\n",
    "  generation_config=generation_config,\n",
    ")\n",
    "\n",
    "chat_session = model.start_chat(\n",
    "  history=[\n",
    "  ]\n",
    ")\n",
    "\n",
    "response = chat_session.send_message(\"hi my name is manish negi\")\n",
    "\n",
    "print(response.text)\n",
    "response = chat_session.send_message(\"do you remember my name\")\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yes, I remember your name! You said it's Manish Negi. ðŸ˜Š  I'm good at remembering things, especially names. \n",
      "\n",
      "How can I help you today, Manish? \n",
      "\n"
     ]
    }
   ],
   "source": [
    "response = chat_session.send_message(\"do you remember my name\")\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "response:\n",
       "GenerateContentResponse(\n",
       "    done=True,\n",
       "    iterator=None,\n",
       "    result=protos.GenerateContentResponse({\n",
       "      \"candidates\": [\n",
       "        {\n",
       "          \"content\": {\n",
       "            \"parts\": [\n",
       "              {\n",
       "                \"text\": \"Yes, I remember your name! You said it's Manish Negi. \\ud83d\\ude0a  I'm good at remembering things, especially names. \\n\\nHow can I help you today, Manish? \\n\"\n",
       "              }\n",
       "            ],\n",
       "            \"role\": \"model\"\n",
       "          },\n",
       "          \"finish_reason\": \"STOP\",\n",
       "          \"index\": 0,\n",
       "          \"safety_ratings\": [\n",
       "            {\n",
       "              \"category\": \"HARM_CATEGORY_SEXUALLY_EXPLICIT\",\n",
       "              \"probability\": \"NEGLIGIBLE\"\n",
       "            },\n",
       "            {\n",
       "              \"category\": \"HARM_CATEGORY_HATE_SPEECH\",\n",
       "              \"probability\": \"NEGLIGIBLE\"\n",
       "            },\n",
       "            {\n",
       "              \"category\": \"HARM_CATEGORY_HARASSMENT\",\n",
       "              \"probability\": \"NEGLIGIBLE\"\n",
       "            },\n",
       "            {\n",
       "              \"category\": \"HARM_CATEGORY_DANGEROUS_CONTENT\",\n",
       "              \"probability\": \"NEGLIGIBLE\"\n",
       "            }\n",
       "          ]\n",
       "        }\n",
       "      ],\n",
       "      \"usage_metadata\": {\n",
       "        \"prompt_token_count\": 40,\n",
       "        \"candidates_token_count\": 42,\n",
       "        \"total_token_count\": 82\n",
       "      }\n",
       "    }),\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import google.generativeai as genai\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "class LLM:\n",
    "    def __init__(self):\n",
    "        # Load environment variables\n",
    "        load_dotenv()\n",
    "        \n",
    "        # Set API key for the Generative AI model\n",
    "        api_key = os.getenv('GEMINI_API_KEY')\n",
    "        if not api_key:\n",
    "            raise ValueError(\"API key not found. Please set the GEMINI_API_KEY in your environment variables.\")\n",
    "        \n",
    "        # Configure API with key\n",
    "        genai.configure(api_key=api_key)\n",
    "        \n",
    "        # Model configuration settings\n",
    "        self.generation_config = {\n",
    "            \"temperature\": 1,\n",
    "            \"top_p\": 0.95,\n",
    "            \"top_k\": 64,\n",
    "            \"max_output_tokens\": 8192,\n",
    "            \"response_mime_type\": \"text/plain\",\n",
    "        }\n",
    "        \n",
    "        # Initialize model and start chat session\n",
    "        self.model = genai.GenerativeModel(\n",
    "            model_name=\"gemini-1.5-flash\",\n",
    "            generation_config=self.generation_config\n",
    "        )\n",
    "        \n",
    "        # Start a new chat session\n",
    "        self.chat_session = self.model.start_chat(history=[])\n",
    "\n",
    "    def get_response(self, query):\n",
    "        \"\"\"\n",
    "        Sends a user query to the chat session and returns the response.\n",
    "        \n",
    "        :param query: str - The user's input text.\n",
    "        :return: str - The model's response text.\n",
    "        \"\"\"\n",
    "        # Send the message to the model's chat session\n",
    "        response = self.chat_session.send_message(query)\n",
    "        \n",
    "        # Return the response text\n",
    "        return response.text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The capital of France is **Paris**. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# main.py\n",
    "#from llm import LLM  # assuming the class is saved in a file named llm.py\n",
    "\n",
    "# Instantiate the LLM\n",
    "llm = LLM()\n",
    "\n",
    "# Example query from frontend\n",
    "user_query = \"What is the capital of France?\"\n",
    "response = llm.get_response(user_query)\n",
    "\n",
    "print(response)  # Use this or send the response back to the frontend\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
